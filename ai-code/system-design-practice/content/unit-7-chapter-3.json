{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 3,
  "chapterTitle": "Horizontal vs Vertical Scaling Decisions",
  "chapterDescription": "Capacity planning trade-offs between larger nodes and more nodes across bottlenecks, reliability goals, and cost constraints.",
  "problems": [
    {
      "id": "sc-hv-001",
      "type": "multiple-choice",
      "question": "A ad ranking service reports: CPU 95% during feature extraction while memory is 48%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because requests are independent and CPU-bound.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "This prompt focuses on A ad ranking service reports: CPU 95% during feature extraction while memory is 48%. Prioritize \"Scale horizontally first because requests are independent and CPU-bound\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 95 and 48 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-002",
      "type": "multiple-choice",
      "question": "A timeline fanout worker reports: heap usage hits OOM before CPU exceeds 40%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale vertically first because the hot path needs a larger in-memory working set immediately."
      ],
      "correct": 3,
      "explanation": "For A timeline fanout worker reports: heap usage hits OOM before CPU exceeds 40%, \"Scale vertically first because the hot path needs a larger in-memory working set immediately\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Keep quantities like 40 in aligned units before deciding on an implementation approach. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-003",
      "type": "multiple-choice",
      "question": "A edge image resizer reports: network egress is maxed at 25 Gbps and CPU is 42%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because aggregate NIC bandwidth scales best by adding nodes.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "Treat A edge image resizer reports: network egress is maxed at 25 Gbps and CPU is 42% as a sequencing problem: \"Scale horizontally first because aggregate NIC bandwidth scales best by adding nodes\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 25 and 42 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-004",
      "type": "multiple-choice",
      "question": "A fraud scoring API reports: cross-node locks dominate p99 after adding more workers. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale vertically first because more nodes increased coordination overhead without reducing contention.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A fraud scoring API reports: cross-node locks dominate p99 after adding more workers is \"Scale vertically first because more nodes increased coordination overhead without reducing contention\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-005",
      "type": "multiple-choice",
      "question": "A checkout API reports: single node replacements cause visible brownouts during deploy. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because smaller nodes reduce replacement blast radius.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "This prompt focuses on A checkout API reports: single node replacements cause visible brownouts during deploy. Prioritize \"Scale horizontally first because smaller nodes reduce replacement blast radius\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "Generalize from checkout API reports: single node replacements cause visible brownouts during deploy to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-006",
      "type": "multiple-choice",
      "question": "A search aggregator reports: single-thread latency regressed after model complexity increased. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale vertically first because stronger per-core performance is the immediate bottleneck."
      ],
      "correct": 3,
      "explanation": "For A search aggregator reports: single-thread latency regressed after model complexity increased, \"Scale vertically first because stronger per-core performance is the immediate bottleneck\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-007",
      "type": "multiple-choice",
      "question": "A notification sender reports: CPU and network both moderate but one JVM pauses for 4s GC. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because more nodes absorb pause impact and smooth tail latency.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "Treat A notification sender reports: CPU and network both moderate but one JVM pauses for 4s GC as a sequencing problem: \"Scale horizontally first because more nodes absorb pause impact and smooth tail latency\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 4s should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-008",
      "type": "multiple-choice",
      "question": "A pricing engine reports: memory fragmentation causes frequent allocator stalls. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale vertically first because larger memory headroom is the fastest stabilizing step.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A pricing engine reports: memory fragmentation causes frequent allocator stalls is \"Scale vertically first because larger memory headroom is the fastest stabilizing step\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-009",
      "type": "multiple-choice",
      "question": "A video transcoding queue reports: queue depth rises with high CPU on all workers. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because parallel jobs scale linearly with worker count.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "This prompt focuses on A video transcoding queue reports: queue depth rises with high CPU on all workers. Prioritize \"Scale horizontally first because parallel jobs scale linearly with worker count\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-010",
      "type": "multiple-choice",
      "question": "A profile read API reports: cache working set no longer fits and miss penalties spike. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale vertically first because larger nodes restore cache residency without repartitioning."
      ],
      "correct": 3,
      "explanation": "Treat A profile read API reports: cache working set no longer fits and miss penalties spike as a sequencing problem: \"Scale vertically first because larger nodes restore cache residency without repartitioning\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-011",
      "type": "multiple-choice",
      "question": "A shipment planner reports: AZ-level loss removes 33% of capacity with current fleet. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because more nodes per AZ lowers failure impact per node.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A shipment planner reports: AZ-level loss removes 33% of capacity with current fleet is \"Scale horizontally first because more nodes per AZ lowers failure impact per node\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 33 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-012",
      "type": "multiple-choice",
      "question": "A metrics ingest gateway reports: packet drops appear at per-host NIC limit. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale horizontally first because capacity is constrained by host network limits.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "This prompt focuses on A metrics ingest gateway reports: packet drops appear at per-host NIC limit. Prioritize \"Scale horizontally first because capacity is constrained by host network limits\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-013",
      "type": "multiple-choice",
      "question": "A document rendering service reports: runtime is mostly single-threaded due to legacy library. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because bigger/faster cores help before rewrite is complete.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "For A document rendering service reports: runtime is mostly single-threaded due to legacy library, \"Scale vertically first because bigger/faster cores help before rewrite is complete\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-014",
      "type": "multiple-choice",
      "question": "A session enrichment API reports: latency rises as cluster metadata writes scale with node count. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale vertically first because reducing node count can cut coordination tax for now."
      ],
      "correct": 3,
      "explanation": "Treat A session enrichment API reports: latency rises as cluster metadata writes scale with node count as a sequencing problem: \"Scale vertically first because reducing node count can cut coordination tax for now\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the approach that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-015",
      "type": "multiple-choice",
      "question": "A coupon validation service reports: node memory is exhausted from large rule tables. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale vertically first because state footprint dominates and needs larger memory boxes.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A coupon validation service reports: node memory is exhausted from large rule tables is \"Scale vertically first because state footprint dominates and needs larger memory boxes\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-016",
      "type": "multiple-choice",
      "question": "A webhook delivery worker reports: traffic is bursty 6x at top of hour. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale horizontally first because elastic node count handles bursts better.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "This prompt focuses on A webhook delivery worker reports: traffic is bursty 6x at top of hour. Prioritize \"Scale horizontally first because elastic node count handles bursts better\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "Generalize from webhook delivery worker reports: traffic is bursty 6x at top of hour to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 6x appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-017",
      "type": "multiple-choice",
      "question": "A order audit pipeline reports: storage bandwidth on local NVMe is saturated per host. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because more hosts increase aggregate local I/O channels.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "For A order audit pipeline reports: storage bandwidth on local NVMe is saturated per host, \"Scale horizontally first because more hosts increase aggregate local I/O channels\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-018",
      "type": "multiple-choice",
      "question": "A chat presence service reports: one huge node is stable but failover takes 8 minutes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale horizontally first because faster recovery comes from many smaller failure domains."
      ],
      "correct": 3,
      "explanation": "Treat A chat presence service reports: one huge node is stable but failover takes 8 minutes as a sequencing problem: \"Scale horizontally first because faster recovery comes from many smaller failure domains\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 8 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-019",
      "type": "multiple-choice",
      "question": "A personalization API reports: per-request CPU cost doubled after model rollout. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because stateless compute can be parallelized safely.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A personalization API reports: per-request CPU cost doubled after model rollout is \"Scale horizontally first because stateless compute can be parallelized safely\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-020",
      "type": "multiple-choice",
      "question": "A inventory diff worker reports: RSS grows with batch size and jobs fail from OOM. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale vertically first because memory ceiling is the hard limit right now.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "For A inventory diff worker reports: RSS grows with batch size and jobs fail from OOM, \"Scale vertically first because memory ceiling is the hard limit right now\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-021",
      "type": "multiple-choice",
      "question": "A report generation service reports: autoscaling adds nodes but lock wait keeps rising. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because contention suggests scale-up before redesign.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "Treat A report generation service reports: autoscaling adds nodes but lock wait keeps rising as a sequencing problem: \"Scale vertically first because contention suggests scale-up before redesign\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-022",
      "type": "multiple-choice",
      "question": "A catalog search indexer reports: nodes are under 50% CPU but one server hit max packets/s. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale horizontally first because adding hosts increases packet processing capacity."
      ],
      "correct": 3,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A catalog search indexer reports: nodes are under 50% CPU but one server hit max packets/s is \"Scale horizontally first because adding hosts increases packet processing capacity\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 50 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-023",
      "type": "multiple-choice",
      "question": "A stream metadata API reports: request rate is steady but long-tail spikes on GC events. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because extra nodes reduce impact of individual GC stalls.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "This prompt focuses on A stream metadata API reports: request rate is steady but long-tail spikes on GC events. Prioritize \"Scale horizontally first because extra nodes reduce impact of individual GC stalls\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-024",
      "type": "multiple-choice",
      "question": "A tax calculation engine reports: same-host cache locality is crucial for 40MB hot state. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale vertically first because scale-up preserves locality with minimal refactor.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "For A tax calculation engine reports: same-host cache locality is crucial for 40MB hot state, \"Scale vertically first because scale-up preserves locality with minimal refactor\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 40MB appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-025",
      "type": "multiple-choice",
      "question": "A IoT command gateway reports: device spikes require 2-minute elasticity windows. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because rapid node count expansion matches burst profile.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "Treat A IoT command gateway reports: device spikes require 2-minute elasticity windows as a sequencing problem: \"Scale horizontally first because rapid node count expansion matches burst profile\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 2 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-026",
      "type": "multiple-choice",
      "question": "A recommendation retrieval API reports: memory use is flat but run queue length is high. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale horizontally first because CPU scheduling pressure favors more workers."
      ],
      "correct": 3,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A recommendation retrieval API reports: memory use is flat but run queue length is high is \"Scale horizontally first because CPU scheduling pressure favors more workers\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-027",
      "type": "multiple-choice",
      "question": "A promotion rules evaluator reports: largest instance type still has 25% memory headroom. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because scaling out improves resilience while headroom exists.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "This prompt focuses on A promotion rules evaluator reports: largest instance type still has 25% memory headroom. Prioritize \"Scale horizontally first because scaling out improves resilience while headroom exists\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "Generalize from promotion rules evaluator reports: largest instance type still has 25% memory headroom to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 25 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-028",
      "type": "multiple-choice",
      "question": "A ledger replay worker reports: coordination service reaches write limits at 80 nodes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale vertically first because fewer stronger workers reduce coordination load.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "For A ledger replay worker reports: coordination service reaches write limits at 80 nodes, \"Scale vertically first because fewer stronger workers reduce coordination load\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 80 should be normalized first so downstream reasoning stays consistent. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-029",
      "type": "multiple-choice",
      "question": "A email template compiler reports: build tasks are embarrassingly parallel. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because parallel execution maps directly to more nodes.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "Treat A email template compiler reports: build tasks are embarrassingly parallel as a sequencing problem: \"Scale horizontally first because parallel execution maps directly to more nodes\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-030",
      "type": "multiple-choice",
      "question": "A ML feature store API reports: memory bandwidth saturation appears before CPU maxes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale vertically first because larger machines improve memory-channel throughput."
      ],
      "correct": 3,
      "explanation": "This prompt focuses on A ML feature store API reports: memory bandwidth saturation appears before CPU maxes. Prioritize \"Scale vertically first because larger machines improve memory-channel throughput\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "Generalize from mL feature store API reports: memory bandwidth saturation appears before CPU maxes to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-031",
      "type": "multiple-choice",
      "question": "A billing statement exporter reports: nightly batch misses deadline with fixed worker pool. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale horizontally first because more workers reduce wall-clock completion time.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "For A billing statement exporter reports: nightly batch misses deadline with fixed worker pool, \"Scale horizontally first because more workers reduce wall-clock completion time\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-032",
      "type": "multiple-choice",
      "question": "A OCR processing pipeline reports: one host crash drops 20% throughput today. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Scale horizontally first because spreading load decreases per-node criticality.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 1,
      "explanation": "Treat A OCR processing pipeline reports: one host crash drops 20% throughput today as a sequencing problem: \"Scale horizontally first because spreading load decreases per-node criticality\" is correct since it targets the scenario risk with the best near-term effect.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-033",
      "type": "multiple-choice",
      "question": "A geo lookup API reports: dataset just exceeded RAM on current instance family. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because fit-in-memory requirement drives immediate scale-up.",
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 0,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, the highest-leverage move for A geo lookup API reports: dataset just exceeded RAM on current instance family is \"Scale vertically first because fit-in-memory requirement drives immediate scale-up\" because it lowers recurrence risk and improves operational control.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-034",
      "type": "multiple-choice",
      "question": "A ad click ingestion reports: CPU is fine but TLS handshake rate saturates hosts. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist.",
        "Scale horizontally first because additional frontends increase handshake capacity."
      ],
      "correct": 3,
      "explanation": "This prompt focuses on A ad click ingestion reports: CPU is fine but TLS handshake rate saturates hosts. Prioritize \"Scale horizontally first because additional frontends increase handshake capacity\" first to reduce the core failure path before secondary optimizations.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-035",
      "type": "multiple-choice",
      "question": "A compliance scan service reports: cross-shard barriers dominate compute time. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Prioritize client retry/backoff tuning first while deferring compute-path changes until the next capacity review.",
        "Reduce high-cardinality observability instrumentation first to cut overhead, then reassess whether capacity issues remain.",
        "Scale vertically first because adding nodes worsens barrier synchronization overhead.",
        "Shorten DNS TTL and rebalance traffic first, then reevaluate whether compute bottlenecks persist."
      ],
      "correct": 2,
      "explanation": "For A compliance scan service reports: cross-shard barriers dominate compute time, \"Scale vertically first because adding nodes worsens barrier synchronization overhead\" is strongest because it directly addresses the identified bottleneck in Horizontal vs Vertical Scaling Decisions.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard choices that violate required invariants during concurrent or failed states. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for feed ranking API: CPU saturation with independent requests. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "CPU-bound parallel workload",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"CPU-bound parallel workload\" is the highest-signal move for On-call for feed ranking API: CPU saturation with independent requests.",
          "detailedExplanation": "Generalize from on-call for feed ranking API: CPU saturation with independent requests to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "add nodes behind load balancer and watch tail latency",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "This stage tests control under pressure for Given that diagnosis, what is the strongest immediate action. \"add nodes behind load balancer and watch tail latency\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for feed ranking API: CPU saturation with independent requests: carry the diagnosis (the incident signal) into follow-up action (\"add nodes behind load balancer and watch tail latency\")."
    },
    {
      "id": "sc-hv-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for rules engine: heap OOM before CPU pressure. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory-bound working set"
          ],
          "correct": 3,
          "explanation": "Stage 1: for On-call for rules engine: heap OOM before CPU pressure, \"memory-bound working set\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "move to larger-memory instances as a short-term fix",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"move to larger-memory instances as a short-term fix\" is the highest-signal move for Given that bottleneck, which scaling action should you take first.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"move to larger-memory instances as a short-term fix\" for durable control."
    },
    {
      "id": "sc-hv-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for render farm: network throughput per host capped. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "network bottleneck",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "At stage 1, prioritize \"network bottleneck\" for On-call for render farm: network throughput per host capped; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "increase host count to raise aggregate egress",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "Stage 2: for Based on that root cause, what is the best next move now, \"increase host count to raise aggregate egress\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from horizontal vs Vertical Scaling Decisions to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for render farm: network throughput per host capped, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"increase host count to raise aggregate egress\"."
    },
    {
      "id": "sc-hv-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for stateful matcher: distributed lock wait rises with node count. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "coordination overhead",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "This stage tests control under pressure for On-call for stateful matcher: distributed lock wait rises with node count. \"coordination overhead\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "reduce cross-node coordination then scale carefully"
          ],
          "correct": 3,
          "explanation": "At stage 2, prioritize \"reduce cross-node coordination then scale carefully\" for With that diagnosis confirmed, what is the most effective immediate response; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for stateful matcher: distributed lock wait rises with node count: identify the core risk first, then apply \"reduce cross-node coordination then scale carefully\"."
    },
    {
      "id": "sc-hv-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for checkout workers: deploys cause significant capacity dips. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "large-node blast radius",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Stage 1: for On-call for checkout workers: deploys cause significant capacity dips, \"large-node blast radius\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "split into more smaller workers",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"split into more smaller workers\" is the highest-signal move for Given this constraint, which action best restores capacity quickly.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"split into more smaller workers\" for durable control."
    },
    {
      "id": "sc-hv-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for geo processor: single-thread hot path limits latency. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "per-core performance ceiling"
          ],
          "correct": 3,
          "explanation": "At stage 1, prioritize \"per-core performance ceiling\" for On-call for geo processor: single-thread hot path limits latency; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "use larger/faster cores while planning parallelization",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "Stage 2: for Given that diagnosis, what is the strongest immediate action, \"use larger/faster cores while planning parallelization\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from horizontal vs Vertical Scaling Decisions to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for geo processor: single-thread hot path limits latency, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"use larger/faster cores while planning parallelization\"."
    },
    {
      "id": "sc-hv-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for notification fanout: bursty arrival every 5 minutes. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "elasticity requirement",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "This stage tests control under pressure for On-call for notification fanout: bursty arrival every 5 minutes. \"elasticity requirement\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Keep quantities like 5 minutes in aligned units before deciding on an implementation approach. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "enable fast horizontal autoscaling",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "At stage 2, prioritize \"enable fast horizontal autoscaling\" for Given that bottleneck, which scaling action should you take first; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "Generalize from given that bottleneck, which scaling action should you take first to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for notification fanout: bursty arrival every 5 minutes: identify the core risk first, then apply \"enable fast horizontal autoscaling\"."
    },
    {
      "id": "sc-hv-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for anti-fraud service: metadata quorum writes dominate p99. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "control-plane bottleneck",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"control-plane bottleneck\" is the highest-signal move for On-call for anti-fraud service: metadata quorum writes dominate p99.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "decrease node churn and batch control updates"
          ],
          "correct": 3,
          "explanation": "This stage tests control under pressure for Based on that root cause, what is the best next move now. \"decrease node churn and batch control updates\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for anti-fraud service: metadata quorum writes dominate p99: carry the diagnosis (the incident signal) into follow-up action (\"decrease node churn and batch control updates\")."
    },
    {
      "id": "sc-hv-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for preview generator: jobs are independent and queue-backed. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "embarrassingly parallel workload",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Stage 1: for On-call for preview generator: jobs are independent and queue-backed, \"embarrassingly parallel workload\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "increase worker replicas",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"increase worker replicas\" is the highest-signal move for With that diagnosis confirmed, what is the most effective immediate response.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"increase worker replicas\" for durable control."
    },
    {
      "id": "sc-hv-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for document parser: RSS climbs with document complexity. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory pressure"
          ],
          "correct": 3,
          "explanation": "At stage 1, prioritize \"memory pressure\" for On-call for document parser: RSS climbs with document complexity; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "scale up RAM and cap concurrent docs",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "Stage 2: for Given this constraint, which action best restores capacity quickly, \"scale up RAM and cap concurrent docs\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for document parser: RSS climbs with document complexity, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"scale up RAM and cap concurrent docs\"."
    },
    {
      "id": "sc-hv-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for metrics rollup: packet-per-second cap reached. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "host networking limit",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "This stage tests control under pressure for On-call for metrics rollup: packet-per-second cap reached. \"host networking limit\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "add ingest nodes and rebalance producers",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "At stage 2, prioritize \"add ingest nodes and rebalance producers\" for Given that diagnosis, what is the strongest immediate action; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for metrics rollup: packet-per-second cap reached: identify the core risk first, then apply \"add ingest nodes and rebalance producers\"."
    },
    {
      "id": "sc-hv-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for promotion evaluator: cross-shard fanout grew 4x. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "high coordination fanout",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"high coordination fanout\" is the highest-signal move for On-call for promotion evaluator: cross-shard fanout grew 4x.",
          "detailedExplanation": "Generalize from on-call for promotion evaluator: cross-shard fanout grew 4x to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 4x should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "co-locate related keys before adding nodes"
          ],
          "correct": 3,
          "explanation": "This stage tests control under pressure for Given that bottleneck, which scaling action should you take first. \"co-locate related keys before adding nodes\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for promotion evaluator: cross-shard fanout grew 4x: carry the diagnosis (the incident signal) into follow-up action (\"co-locate related keys before adding nodes\")."
    },
    {
      "id": "sc-hv-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for invoice calculator: tail spikes tied to stop-the-world pauses. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "single-node pause sensitivity",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Stage 1: for On-call for invoice calculator: tail spikes tied to stop-the-world pauses, \"single-node pause sensitivity\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "add more nodes to dilute pause impact",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"add more nodes to dilute pause impact\" is the highest-signal move for Based on that root cause, what is the best next move now.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"add more nodes to dilute pause impact\" for durable control."
    },
    {
      "id": "sc-hv-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for chat delivery: single node failure removes 18% capacity. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "failure-domain problem"
          ],
          "correct": 3,
          "explanation": "At stage 1, prioritize \"failure-domain problem\" for On-call for chat delivery: single node failure removes 18% capacity; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Numbers such as 18 should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "increase node count to cut per-node share",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "Stage 2: for With that diagnosis confirmed, what is the most effective immediate response, \"increase node count to cut per-node share\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from horizontal vs Vertical Scaling Decisions to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for chat delivery: single node failure removes 18% capacity, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"increase node count to cut per-node share\"."
    },
    {
      "id": "sc-hv-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for search suggest: cache no longer fits on current hosts. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "insufficient memory footprint",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"insufficient memory footprint\" is the highest-signal move for On-call for search suggest: cache no longer fits on current hosts.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "upgrade node memory class",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "This stage tests control under pressure for Given this constraint, which action best restores capacity quickly. \"upgrade node memory class\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for search suggest: cache no longer fits on current hosts: carry the diagnosis (the incident signal) into follow-up action (\"upgrade node memory class\")."
    },
    {
      "id": "sc-hv-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for stream enrich: queue age rises while CPU on all nodes > 90%. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "compute saturation",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Stage 1: for On-call for stream enrich: queue age rises while CPU on all nodes > 90%, \"compute saturation\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 90 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "add workers and enforce backpressure"
          ],
          "correct": 3,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"add workers and enforce backpressure\" is the highest-signal move for Given that diagnosis, what is the strongest immediate action.",
          "detailedExplanation": "Generalize from given that diagnosis, what is the strongest immediate action to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"add workers and enforce backpressure\" for durable control."
    },
    {
      "id": "sc-hv-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for catalog merge: barrier synchronization dominates runtime. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "synchronization overhead",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "At stage 1, prioritize \"synchronization overhead\" for On-call for catalog merge: barrier synchronization dominates runtime; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "reduce barrier frequency before scaling out",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "Stage 2: for Given that bottleneck, which scaling action should you take first, \"reduce barrier frequency before scaling out\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from horizontal vs Vertical Scaling Decisions to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for catalog merge: barrier synchronization dominates runtime, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"reduce barrier frequency before scaling out\"."
    },
    {
      "id": "sc-hv-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for sensor gateway: connection table maxes out per instance. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "per-host state limit"
          ],
          "correct": 3,
          "explanation": "This stage tests control under pressure for On-call for sensor gateway: connection table maxes out per instance. \"per-host state limit\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "increase node count and shard connections",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "At stage 2, prioritize \"increase node count and shard connections\" for Based on that root cause, what is the best next move now; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for sensor gateway: connection table maxes out per instance: identify the core risk first, then apply \"increase node count and shard connections\"."
    },
    {
      "id": "sc-hv-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for billing export: batch misses SLA by 30%. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "insufficient parallel capacity",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"insufficient parallel capacity\" is the highest-signal move for On-call for billing export: batch misses SLA by 30%.",
          "detailedExplanation": "Generalize from on-call for billing export: batch misses SLA by 30% to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 30 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "add temporary worker pool for batch window",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "This stage tests control under pressure for With that diagnosis confirmed, what is the most effective immediate response. \"add temporary worker pool for batch window\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for billing export: batch misses SLA by 30%: carry the diagnosis (the incident signal) into follow-up action (\"add temporary worker pool for batch window\")."
    },
    {
      "id": "sc-hv-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for report precompute: larger instance type available with 2x memory. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "quick vertical relief option",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Stage 1: for On-call for report precompute: larger instance type available with 2x memory, \"quick vertical relief option\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Strong answers connect quorum/coordination settings to concrete correctness goals. If values like 2x appear, convert them into one unit basis before comparison. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "scale up now and revisit partitioning"
          ],
          "correct": 3,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"scale up now and revisit partitioning\" is the highest-signal move for Given this constraint, which action best restores capacity quickly.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"scale up now and revisit partitioning\" for durable control."
    },
    {
      "id": "sc-hv-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for live scoreboard: frequent deployment with tight SLOs. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "operational resilience pressure",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "At stage 1, prioritize \"operational resilience pressure\" for On-call for live scoreboard: frequent deployment with tight SLOs; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "prefer more small nodes for safer rollouts",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "Stage 2: for Given that diagnosis, what is the strongest immediate action, \"prefer more small nodes for safer rollouts\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "For On-call for live scoreboard: frequent deployment with tight SLOs, scoring is sequential: stage 1 isolates the governing risk and stage 2 prioritizes \"prefer more small nodes for safer rollouts\"."
    },
    {
      "id": "sc-hv-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for OCR API: CPU fine but memory bandwidth saturated. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory-channel bottleneck"
          ],
          "correct": 3,
          "explanation": "This stage tests control under pressure for On-call for OCR API: CPU fine but memory bandwidth saturated. \"memory-channel bottleneck\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "move to instances with higher memory bandwidth",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "At stage 2, prioritize \"move to instances with higher memory bandwidth\" for Given that bottleneck, which scaling action should you take first; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for OCR API: CPU fine but memory bandwidth saturated: identify the core risk first, then apply \"move to instances with higher memory bandwidth\"."
    },
    {
      "id": "sc-hv-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for vector search: coordinator CPU pegs while workers idle. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "central coordination bottleneck",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "For stage 1 in Horizontal vs Vertical Scaling Decisions, \"central coordination bottleneck\" is the highest-signal move for On-call for vector search: coordinator CPU pegs while workers idle.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "split coordinator role then scale",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "This stage tests control under pressure for Based on that root cause, what is the best next move now. \"split coordinator role then scale\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This item rewards stage continuity for On-call for vector search: coordinator CPU pegs while workers idle: carry the diagnosis (the incident signal) into follow-up action (\"split coordinator role then scale\")."
    },
    {
      "id": "sc-hv-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for ad delivery: TLS termination maxes per host. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "frontend host bottleneck",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Stage 1: for On-call for ad delivery: TLS termination maxes per host, \"frontend host bottleneck\" is correct because it addresses the incident signal in Horizontal vs Vertical Scaling Decisions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "scale out edge terminators"
          ],
          "correct": 3,
          "explanation": "For stage 2 in Horizontal vs Vertical Scaling Decisions, \"scale out edge terminators\" is the highest-signal move for With that diagnosis confirmed, what is the most effective immediate response.",
          "detailedExplanation": "Generalize from with that diagnosis confirmed, what is the most effective immediate response to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "Treat this as a linked decision chain in Horizontal vs Vertical Scaling Decisions. Diagnose the main signal, then execute \"scale out edge terminators\" for durable control."
    },
    {
      "id": "sc-hv-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for compliance scanner: state transfer during failover is too large. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "heavy per-node state size",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "This stage tests control under pressure for On-call for compliance scanner: state transfer during failover is too large. \"heavy per-node state size\" is strongest because it targets the scenario risk directly.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "first reduce state size, then re-evaluate strategy",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "At stage 2, prioritize \"first reduce state size, then re-evaluate strategy\" for Given this constraint, which action best restores capacity quickly; it best reduces the named risk before follow-on changes.",
          "detailedExplanation": "Generalize from given this constraint, which action best restores capacity quickly to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior",
      "explanation": "This two-stage prompt in Horizontal vs Vertical Scaling Decisions evaluates diagnosis then mitigation for On-call for compliance scanner: state transfer during failover is too large: identify the core risk first, then apply \"first reduce state size, then re-evaluate strategy\"."
    },
    {
      "id": "sc-hv-061",
      "type": "multi-select",
      "question": "Which signals justify choosing horizontal scaling first?",
      "options": [
        "Independent request processing and high CPU across nodes",
        "Need lower blast radius per failure",
        "Working set only fits on one very large machine",
        "Bursty demand needing elastic capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The right answer for Which signals justify choosing horizontal scaling first is a combined strategy: Independent request processing and high CPU across nodes, Need lower blast radius per failure, Bursty demand needing elastic capacity. Single-choice responses leave material risk unaddressed.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-062",
      "type": "multi-select",
      "question": "Which conditions make vertical scaling a reasonable first move?",
      "options": [
        "Memory ceiling is the immediate blocker",
        "Minimal time for architectural change",
        "Hard requirement to reduce single-node risk",
        "Strong single-thread performance is required"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Horizontal vs Vertical Scaling Decisions, Which conditions make vertical scaling a reasonable first move is best handled with coordinated controls. Select Memory ceiling is the immediate blocker, Minimal time for architectural change, Strong single-thread performance is required to close the most important gaps.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: propagating an early bad assumption through all steps.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-063",
      "type": "multi-select",
      "question": "Which are common costs of horizontal scaling?",
      "options": [
        "Higher coordination/metadata overhead",
        "More complex partitioning and balancing",
        "Guaranteed lower cloud bill at any scale",
        "Increased operational surface area"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This scenario for Which are common costs of horizontal scaling is multi-dimensional. The strongest combination is Higher coordination/metadata overhead, More complex partitioning and balancing, Increased operational surface area, covering prevention and containment together.",
      "detailedExplanation": "Generalize from common costs of horizontal scaling to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-064",
      "type": "multi-select",
      "question": "Which are key risks of vertical-only strategy over time?",
      "options": [
        "Hitting max instance-size limits",
        "Larger per-node blast radius",
        "Automatic elimination of coordination bugs",
        "Potentially slower recovery/replacement"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Which are key risks of vertical-only strategy over time in Horizontal vs Vertical Scaling Decisions, this requires a control bundle, not one tactic. The correct set is Hitting max instance-size limits, Larger per-node blast radius, Potentially slower recovery/replacement.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-065",
      "type": "multi-select",
      "question": "Which metrics are most useful before deciding between scale up vs out?",
      "options": [
        "Per-resource utilization (CPU/memory/network)",
        "Queue depth and queue age trends",
        "Only daily average request count",
        "Tail latency with concurrency context"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The right answer for Which metrics are most useful before deciding between scale up vs out is a combined strategy: Per-resource utilization (CPU/memory/network), Queue depth and queue age trends, Tail latency with concurrency context. Single-choice responses leave material risk unaddressed.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-066",
      "type": "multi-select",
      "question": "For cost modeling of scaling paths, which inputs matter?",
      "options": [
        "Per-node cost by instance class",
        "Coordination overhead as node count rises",
        "Failure and recovery cost assumptions",
        "Only peak QPS without utilization targets"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Horizontal vs Vertical Scaling Decisions, cost modeling of scaling paths is best handled with coordinated controls. Select Per-node cost by instance class, Coordination overhead as node count rises, Failure and recovery cost assumptions to close the most important gaps.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-067",
      "type": "multi-select",
      "question": "Which engineering steps improve horizontal efficiency before adding nodes?",
      "options": [
        "Reduce lock contention hot spots",
        "Improve partition-key distribution",
        "Increase cross-service synchronous fanout",
        "Minimize shared mutable state"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This scenario for Which engineering steps improve horizontal efficiency before adding nodes is multi-dimensional. The strongest combination is Reduce lock contention hot spots, Improve partition-key distribution, Minimize shared mutable state, covering prevention and containment together.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-068",
      "type": "multi-select",
      "question": "Which statements about blast radius are accurate?",
      "options": [
        "More smaller nodes usually reduce per-node impact",
        "One huge node can make failures more severe",
        "Blast radius is unrelated to node size mix",
        "Deployment safety often improves with smaller increments"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Which statements about blast radius are accurate in Horizontal vs Vertical Scaling Decisions, this requires a control bundle, not one tactic. The correct set is More smaller nodes usually reduce per-node impact, One huge node can make failures more severe, Deployment safety often improves with smaller increments.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-069",
      "type": "multi-select",
      "question": "Which workloads typically benefit from horizontal scaling?",
      "options": [
        "Queue-backed independent jobs",
        "Stateless APIs with uniform requests",
        "Monolithic single-threaded code with shared mutable state",
        "Batch pipelines that can shard by key"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The right answer for Which workloads typically benefit from horizontal scaling is a combined strategy: Queue-backed independent jobs, Stateless APIs with uniform requests, Batch pipelines that can shard by key. Single-choice responses leave material risk unaddressed.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-070",
      "type": "multi-select",
      "question": "Which workloads often require some vertical scaling component?",
      "options": [
        "Large in-memory models requiring high RAM per process",
        "Latency-sensitive single-thread hot loops",
        "Simple stateless request routers only",
        "Workloads constrained by per-process memory bandwidth"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This scenario for Which workloads often require some vertical scaling component is multi-dimensional. The strongest combination is Large in-memory models requiring high RAM per process, Latency-sensitive single-thread hot loops, Workloads constrained by per-process memory bandwidth, covering prevention and containment together.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-071",
      "type": "multi-select",
      "question": "Which anti-patterns often hide true scaling bottlenecks?",
      "options": [
        "Scaling blindly on CPU average only",
        "Ignoring queue age and tail latency",
        "Correlating metrics with deployment events",
        "Skipping bottleneck decomposition by resource"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Which anti-patterns often hide true scaling bottlenecks in Horizontal vs Vertical Scaling Decisions, this requires a control bundle, not one tactic. The correct set is Scaling blindly on CPU average only, Ignoring queue age and tail latency, Skipping bottleneck decomposition by resource.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-072",
      "type": "multi-select",
      "question": "When scale-out appears ineffective, which checks are high-value?",
      "options": [
        "Measure cross-node coordination cost",
        "Inspect partition skew and hot keys",
        "Assume DNS is root cause by default",
        "Review shared datastore contention"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The right answer for When scale-out appears ineffective, which checks are high-value is a combined strategy: Measure cross-node coordination cost, Inspect partition skew and hot keys, Review shared datastore contention. Single-choice responses leave material risk unaddressed.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-073",
      "type": "multi-select",
      "question": "Which are practical short-term actions during memory-bound incidents?",
      "options": [
        "Move to larger-memory nodes temporarily",
        "Reduce per-request memory footprint",
        "Increase retries to smooth memory spikes",
        "Cap concurrency until headroom returns"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Horizontal vs Vertical Scaling Decisions, Which are practical short-term actions during memory-bound incidents is best handled with coordinated controls. Select Move to larger-memory nodes temporarily, Reduce per-request memory footprint, Cap concurrency until headroom returns to close the most important gaps.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-074",
      "type": "multi-select",
      "question": "Which statements about coordination overhead are true?",
      "options": [
        "It can rise nonlinearly with node count",
        "Global locks can erase scale-out gains",
        "It always decreases when adding nodes",
        "Partition-local work usually scales better"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This scenario for Which statements about coordination overhead are true is multi-dimensional. The strongest combination is It can rise nonlinearly with node count, Global locks can erase scale-out gains, Partition-local work usually scales better, covering prevention and containment together.",
      "detailedExplanation": "Generalize from statements about coordination overhead are true to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-075",
      "type": "multi-select",
      "question": "Which are valid reasons to mix vertical and horizontal strategies?",
      "options": [
        "Scale up for immediate relief, then scale out for resilience",
        "Use larger nodes for coordinators and more small workers",
        "Pick one strategy forever regardless of workload changes",
        "Adapt strategy by bottleneck phase and growth stage"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Which are valid reasons to mix vertical and horizontal strategies in Horizontal vs Vertical Scaling Decisions, this requires a control bundle, not one tactic. The correct set is Scale up for immediate relief, then scale out for resilience, Use larger nodes for coordinators and more small workers, Adapt strategy by bottleneck phase and growth stage.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-076",
      "type": "multi-select",
      "question": "Which signals suggest network-bound scaling limits?",
      "options": [
        "Per-host egress plateaus at NIC limit",
        "CPU remains moderate during saturation",
        "OOM kills dominate incidents",
        "Adding hosts raises aggregate throughput"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The right answer for Which signals suggest network-bound scaling limits is a combined strategy: Per-host egress plateaus at NIC limit, CPU remains moderate during saturation, Adding hosts raises aggregate throughput. Single-choice responses leave material risk unaddressed.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-077",
      "type": "multi-select",
      "question": "Which guardrails reduce risk while applying scaling changes?",
      "options": [
        "Canary rollout with rollback thresholds",
        "Predefined max node/size limits",
        "Disable alerts during rollout to reduce noise",
        "Post-change metric comparison against baseline"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Horizontal vs Vertical Scaling Decisions, Which guardrails reduce risk while applying scaling changes is best handled with coordinated controls. Select Canary rollout with rollback thresholds, Predefined max node/size limits, Post-change metric comparison against baseline to close the most important gaps.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-078",
      "type": "numeric-input",
      "question": "Each node handles 1600 QPS at 80% target utilization. Peak target is 14000 QPS. Nodes needed?",
      "answer": 11,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "The sizing math for Each node handles 1600 QPS at 80% target utilization yields 11 nodes. Use within +/-10% as the grading bound.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 1600 QPS and 80 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-079",
      "type": "numeric-input",
      "question": "Current fleet: 10 nodes at 2200 QPS each. Forecast +40% traffic. Keep 70% utilization. New node count?",
      "answer": 20,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "For Current fleet: 10 nodes at 2200 QPS each in Horizontal vs Vertical Scaling Decisions, the expected result is 20 nodes. Answers within +/-10% are acceptable.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 10 and 2200 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-080",
      "type": "numeric-input",
      "question": "Option A: 12 small nodes at $0.45/hr. Option B: 5 large nodes at $1.25/hr. Percent increase B vs A?",
      "answer": 15.74,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "This estimate anchors the decision for Option A: 12 small nodes at $0: 15.74 %. Responses within +/-20% show correct reasoning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 12 and 0.45 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-081",
      "type": "numeric-input",
      "question": "Tier has 24 equal nodes. One node fails and traffic redistributes perfectly. Percent capacity loss?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "The sizing math for Tier has 24 equal nodes yields 4.17 %. Use within +/-20% as the grading bound.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 24 and 1 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-082",
      "type": "numeric-input",
      "question": "A vertical move doubles per-node memory from 64GB to 128GB. Fleet shrinks from 14 to 8 nodes. Total memory change (%)?",
      "answer": 14.29,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "For A vertical move doubles per-node memory from 64GB to 128GB in Horizontal vs Vertical Scaling Decisions, the expected result is 14.29 %. Answers within +/-20% are acceptable.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 64GB and 128GB in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-083",
      "type": "numeric-input",
      "question": "Autoscaling target is 65% CPU. Each node sustains 3000 QPS at 100% CPU linear assumption. For 39000 QPS peak, nodes needed?",
      "answer": 20,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, Autoscaling target is 65% CPU evaluates to 20 nodes. Any result within +/-10% demonstrates sound capacity judgment.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 65 and 3000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-084",
      "type": "numeric-input",
      "question": "A workload needs 9 TB/day egress. Each node safely delivers 350 MB/s average. Minimum nodes required?",
      "answer": 1,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "This estimate anchors the decision for A workload needs 9 TB/day egress: 1 nodes. Responses within +/-10% show correct reasoning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Numbers such as 9 TB and 350 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-085",
      "type": "numeric-input",
      "question": "Scale-out plan adds 6 nodes to current 18. By what percent does raw node count increase?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "The sizing math for Scale-out plan adds 6 nodes to current 18 yields 33.33 %. Use within +/-20% as the grading bound.",
      "detailedExplanation": "Generalize from scale-out plan adds 6 nodes to current 18 to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 6 and 18 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-086",
      "type": "numeric-input",
      "question": "Single large node costs $2.40/hr. Equivalent capacity uses 7 small nodes at $0.38/hr. Percent cheaper option?",
      "answer": 9.77,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "For Single large node costs $2 in Horizontal vs Vertical Scaling Decisions, the expected result is 9.77 %. Answers within +/-20% are acceptable.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 2.40 and 7 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-087",
      "type": "numeric-input",
      "question": "At 30 nodes, coordination overhead consumes 18% CPU. At 45 nodes, 27% CPU. Relative overhead increase (%)?",
      "answer": 50,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "In Horizontal vs Vertical Scaling Decisions, At 30 nodes, coordination overhead consumes 18% CPU evaluates to 50 %. Any result within +/-20% demonstrates sound capacity judgment.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Strong answers connect quorum/coordination settings to concrete correctness goals. Numbers such as 30 and 18 should be normalized first so downstream reasoning stays consistent. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-088",
      "type": "numeric-input",
      "question": "Batch job takes 90 min on 12 workers. Assume ideal scaling. Time on 18 workers (minutes)?",
      "answer": 60,
      "unit": "minutes",
      "tolerance": 0.1,
      "explanation": "This estimate anchors the decision for Batch job takes 90 min on 12 workers: 60 minutes. Responses within +/-10% show correct reasoning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 90 min and 12 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-089",
      "type": "numeric-input",
      "question": "A service currently has 16 nodes with 25% spare headroom. Traffic grows 20%. Additional nodes needed (same size)?",
      "answer": 4,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "The sizing math for A service currently has 16 nodes with 25% spare headroom yields 4 nodes. Use within +/-10% as the grading bound.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 16 and 25 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-090",
      "type": "ordering",
      "question": "Rank the scaling workflow from first to last.",
      "items": [
        "Identify bottleneck by resource and contention",
        "Choose scale up/out hypothesis and expected impact",
        "Roll out gradually with guardrails",
        "Re-measure SLO and saturation after rollout"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Correct ordering for Horizontal vs Vertical Scaling Decisions starts at Identify bottleneck by resource and contention and ends at Re-measure SLO and saturation after rollout, preserving operational safety.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-091",
      "type": "ordering",
      "question": "Rank from lowest to highest coordination overhead.",
      "items": [
        "Independent stateless workers",
        "Partitioned workers with rare cross-shard calls",
        "Workers using frequent distributed locks",
        "Workers requiring global consensus updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "In Rank from lowest to highest coordination overhead, this ordering minimizes rework and blast radius: Independent stateless workers comes before Workers requiring global consensus updates.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Build the rank from biggest differences first, then refine with adjacent checks. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-092",
      "type": "ordering",
      "question": "Rank from smallest to largest per-node failure blast radius.",
      "items": [
        "24 small nodes",
        "12 medium nodes",
        "6 large nodes",
        "1 very large node"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The safe sequence in Horizontal vs Vertical Scaling Decisions is dependency-first: begin with 24 small nodes and finish with 1 very large node.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-093",
      "type": "ordering",
      "question": "Rank from fastest short-term relief to slowest.",
      "items": [
        "Resize to larger instances in same family",
        "Add more replicas with existing topology",
        "Refactor to reduce shared-state contention",
        "Redesign architecture boundaries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Rank from fastest short-term relief to slowest, order steps from Resize to larger instances in same family to Redesign architecture boundaries; Horizontal vs Vertical Scaling Decisions requires stabilization before optimization.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-094",
      "type": "ordering",
      "question": "Rank autoscaling signals from least to most direct for compute saturation.",
      "items": [
        "Daily average traffic",
        "Five-minute average CPU",
        "Queue age and tail latency",
        "Per-resource saturation plus queue age under load"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Correct ordering for Horizontal vs Vertical Scaling Decisions starts at Daily average traffic and ends at Per-resource saturation plus queue age under load, preserving operational safety.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-095",
      "type": "ordering",
      "question": "Rank interventions from most to least useful when memory is the hard bottleneck.",
      "items": [
        "Increase per-node memory",
        "Reduce per-request memory footprint",
        "Improve partitioning to lower duplication",
        "Only increase retries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "In Rank interventions from most to least useful when memory is the hard bottleneck, this ordering minimizes rework and blast radius: Increase per-node memory comes before Only increase retries.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-096",
      "type": "ordering",
      "question": "Rank decision quality from weakest to strongest.",
      "items": [
        "Choose strategy from team preference only",
        "Choose from average CPU alone",
        "Choose from multi-metric bottleneck analysis",
        "Choose from bottleneck analysis plus canary validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The safe sequence in Horizontal vs Vertical Scaling Decisions is dependency-first: begin with Choose strategy from team preference only and finish with Choose from bottleneck analysis plus canary validation.",
      "detailedExplanation": "Generalize from rank decision quality from weakest to strongest to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-097",
      "type": "ordering",
      "question": "Rank from least to most resilient deployment pattern.",
      "items": [
        "Single large node replacement",
        "Few large nodes with long drains",
        "Moderate node pool with canary",
        "Many small nodes with progressive rollout"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Rank from least to most resilient deployment pattern, order steps from Single large node replacement to Many small nodes with progressive rollout; Horizontal vs Vertical Scaling Decisions requires stabilization before optimization.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-098",
      "type": "ordering",
      "question": "Rank from least to most scalable for burst handling.",
      "items": [
        "Fixed-size vertical-only fleet",
        "Vertical scale plus manual adds",
        "Horizontal autoscaling by CPU only",
        "Horizontal autoscaling by CPU and queue age"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Correct ordering for Horizontal vs Vertical Scaling Decisions starts at Fixed-size vertical-only fleet and ends at Horizontal autoscaling by CPU and queue age, preserving operational safety.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-099",
      "type": "ordering",
      "question": "Rank from lowest to highest operational complexity.",
      "items": [
        "Single pool of homogeneous nodes",
        "Two pools split by workload class",
        "Multi-pool with custom schedulers",
        "Global multi-region compute with per-region policies"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "In Rank from lowest to highest operational complexity, this ordering minimizes rework and blast radius: Single pool of homogeneous nodes comes before Global multi-region compute with per-region policies.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hv-100",
      "type": "ordering",
      "question": "Rank from worst to best incident response sequence.",
      "items": [
        "Scale randomly and hope",
        "Scale first then inspect metrics",
        "Diagnose bottleneck then scale",
        "Diagnose, scale with guardrails, verify outcome"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The safe sequence in Horizontal vs Vertical Scaling Decisions is dependency-first: begin with Scale randomly and hope and finish with Diagnose, scale with guardrails, verify outcome.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "horizontal-vs-vertical-scaling-decisions"],
      "difficulty": "senior"
    }
  ]
}
