{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 3,
  "chapterTitle": "Horizontal vs Vertical Scaling Decisions",
  "chapterDescription": "Capacity planning trade-offs between larger nodes and more nodes across bottlenecks, reliability goals, and cost constraints.",
  "problems": [
    {
      "id": "sc-hv-001",
      "type": "multiple-choice",
      "question": "A ad ranking service reports: CPU 95% during feature extraction while memory is 48%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because requests are independent and CPU-bound.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The key clue in this question is \"ad ranking service reports: CPU 95% during feature extraction while memory is 48%\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 95 and 48 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-002",
      "type": "multiple-choice",
      "question": "A timeline fanout worker reports: heap usage hits OOM before CPU exceeds 40%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale vertically first because the hot path needs a larger in-memory working set immediately."
      ],
      "correct": 3,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Read this as a scenario about \"timeline fanout worker reports: heap usage hits OOM before CPU exceeds 40%\". Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Keep quantities like 40 in aligned units before selecting an answer. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-003",
      "type": "multiple-choice",
      "question": "A edge image resizer reports: network egress is maxed at 25 Gbps and CPU is 42%. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because aggregate NIC bandwidth scales best by adding nodes.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The decision turns on \"edge image resizer reports: network egress is maxed at 25 Gbps and CPU is 42%\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 25 and 42 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-004",
      "type": "multiple-choice",
      "question": "A fraud scoring API reports: cross-node locks dominate p99 after adding more workers. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale vertically first because more nodes increased coordination overhead without reducing contention.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "This prompt is really about \"fraud scoring API reports: cross-node locks dominate p99 after adding more workers\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-005",
      "type": "multiple-choice",
      "question": "A checkout API reports: single node replacements cause visible brownouts during deploy. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because smaller nodes reduce replacement blast radius.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Use \"checkout API reports: single node replacements cause visible brownouts during deploy\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-006",
      "type": "multiple-choice",
      "question": "A search aggregator reports: single-thread latency regressed after model complexity increased. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale vertically first because stronger per-core performance is the immediate bottleneck."
      ],
      "correct": 3,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The core signal here is \"search aggregator reports: single-thread latency regressed after model complexity\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-007",
      "type": "multiple-choice",
      "question": "A notification sender reports: CPU and network both moderate but one JVM pauses for 4s GC. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because more nodes absorb pause impact and smooth tail latency.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "If you keep \"notification sender reports: CPU and network both moderate but one JVM pauses for 4s GC\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 4s should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-008",
      "type": "multiple-choice",
      "question": "A pricing engine reports: memory fragmentation causes frequent allocator stalls. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale vertically first because larger memory headroom is the fastest stabilizing step.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Start from \"pricing engine reports: memory fragmentation causes frequent allocator stalls\", then pressure-test the result against the options. Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-009",
      "type": "multiple-choice",
      "question": "A video transcoding queue reports: queue depth rises with high CPU on all workers. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because parallel jobs scale linearly with worker count.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The key clue in this question is \"video transcoding queue reports: queue depth rises with high CPU on all workers\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-010",
      "type": "multiple-choice",
      "question": "A profile read API reports: cache working set no longer fits and miss penalties spike. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale vertically first because larger nodes restore cache residency without repartitioning."
      ],
      "correct": 3,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The decision turns on \"profile read API reports: cache working set no longer fits and miss penalties spike\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "sc-hv-011",
      "type": "multiple-choice",
      "question": "A shipment planner reports: AZ-level loss removes 33% of capacity with current fleet. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because more nodes per AZ lowers failure impact per node.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Read this as a scenario about \"shipment planner reports: AZ-level loss removes 33% of capacity with current fleet\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 33 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-012",
      "type": "multiple-choice",
      "question": "A metrics ingest gateway reports: packet drops appear at per-host NIC limit. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale horizontally first because capacity is constrained by host network limits.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The key clue in this question is \"metrics ingest gateway reports: packet drops appear at per-host NIC limit\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-013",
      "type": "multiple-choice",
      "question": "A document rendering service reports: runtime is mostly single-threaded due to legacy library. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because bigger/faster cores help before rewrite is complete.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Start from \"document rendering service reports: runtime is mostly single-threaded due to legacy\", then pressure-test the result against the options. Prefer the option that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-014",
      "type": "multiple-choice",
      "question": "A session enrichment API reports: latency rises as cluster metadata writes scale with node count. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale vertically first because reducing node count can cut coordination tax for now."
      ],
      "correct": 3,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "If you keep \"session enrichment API reports: latency rises as cluster metadata writes scale with\" in view, the correct answer separates faster. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-015",
      "type": "multiple-choice",
      "question": "A coupon validation service reports: node memory is exhausted from large rule tables. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale vertically first because state footprint dominates and needs larger memory boxes.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The core signal here is \"coupon validation service reports: node memory is exhausted from large rule tables\". Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-016",
      "type": "multiple-choice",
      "question": "A webhook delivery worker reports: traffic is bursty 6x at top of hour. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale horizontally first because elastic node count handles bursts better.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Use \"webhook delivery worker reports: traffic is bursty 6x at top of hour\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 6x appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-017",
      "type": "multiple-choice",
      "question": "A order audit pipeline reports: storage bandwidth on local NVMe is saturated per host. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because more hosts increase aggregate local I/O channels.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "This prompt is really about \"order audit pipeline reports: storage bandwidth on local NVMe is saturated per host\". Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-018",
      "type": "multiple-choice",
      "question": "A chat presence service reports: one huge node is stable but failover takes 8 minutes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale horizontally first because faster recovery comes from many smaller failure domains."
      ],
      "correct": 3,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The decision turns on \"chat presence service reports: one huge node is stable but failover takes 8 minutes\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 8 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-019",
      "type": "multiple-choice",
      "question": "A personalization API reports: per-request CPU cost doubled after model rollout. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because stateless compute can be parallelized safely.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Read this as a scenario about \"personalization API reports: per-request CPU cost doubled after model rollout\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-020",
      "type": "multiple-choice",
      "question": "A inventory diff worker reports: RSS grows with batch size and jobs fail from OOM. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale vertically first because memory ceiling is the hard limit right now.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Read this as a scenario about \"inventory diff worker reports: RSS grows with batch size and jobs fail from OOM\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-021",
      "type": "multiple-choice",
      "question": "A report generation service reports: autoscaling adds nodes but lock wait keeps rising. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because contention suggests scale-up before redesign.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The decision turns on \"report generation service reports: autoscaling adds nodes but lock wait keeps rising\". Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-022",
      "type": "multiple-choice",
      "question": "A catalog search indexer reports: nodes are under 50% CPU but one server hit max packets/s. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale horizontally first because adding hosts increases packet processing capacity."
      ],
      "correct": 3,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Start from \"catalog search indexer reports: nodes are under 50% CPU but one server hit max packets/s\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 50 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-023",
      "type": "multiple-choice",
      "question": "A stream metadata API reports: request rate is steady but long-tail spikes on GC events. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because extra nodes reduce impact of individual GC stalls.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The key clue in this question is \"stream metadata API reports: request rate is steady but long-tail spikes on GC events\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-024",
      "type": "multiple-choice",
      "question": "A tax calculation engine reports: same-host cache locality is crucial for 40MB hot state. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale vertically first because scale-up preserves locality with minimal refactor.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The core signal here is \"tax calculation engine reports: same-host cache locality is crucial for 40MB hot state\". Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 40MB appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-025",
      "type": "multiple-choice",
      "question": "A IoT command gateway reports: device spikes require 2-minute elasticity windows. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because rapid node count expansion matches burst profile.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "If you keep \"ioT command gateway reports: device spikes require 2-minute elasticity windows\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 2 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-026",
      "type": "multiple-choice",
      "question": "A recommendation retrieval API reports: memory use is flat but run queue length is high. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale horizontally first because CPU scheduling pressure favors more workers."
      ],
      "correct": 3,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "This prompt is really about \"recommendation retrieval API reports: memory use is flat but run queue length is high\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-hv-027",
      "type": "multiple-choice",
      "question": "A promotion rules evaluator reports: largest instance type still has 25% memory headroom. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because scaling out improves resilience while headroom exists.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "Use \"promotion rules evaluator reports: largest instance type still has 25% memory headroom\" as your starting point, then verify tradeoffs carefully. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 25 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-028",
      "type": "multiple-choice",
      "question": "A ledger replay worker reports: coordination service reaches write limits at 80 nodes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale vertically first because fewer stronger workers reduce coordination load.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Read this as a scenario about \"ledger replay worker reports: coordination service reaches write limits at 80 nodes\". Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 80 should be normalized first so downstream reasoning stays consistent. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-029",
      "type": "multiple-choice",
      "question": "A email template compiler reports: build tasks are embarrassingly parallel. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale horizontally first because parallel execution maps directly to more nodes.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The decision turns on \"email template compiler reports: build tasks are embarrassingly parallel\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-030",
      "type": "multiple-choice",
      "question": "A ML feature store API reports: memory bandwidth saturation appears before CPU maxes. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale vertically first because larger machines improve memory-channel throughput."
      ],
      "correct": 3,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Use \"mL feature store API reports: memory bandwidth saturation appears before CPU maxes\" as your starting point, then verify tradeoffs carefully. Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-031",
      "type": "multiple-choice",
      "question": "A billing statement exporter reports: nightly batch misses deadline with fixed worker pool. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale horizontally first because more workers reduce wall-clock completion time.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "This prompt is really about \"billing statement exporter reports: nightly batch misses deadline with fixed worker pool\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-032",
      "type": "multiple-choice",
      "question": "A OCR processing pipeline reports: one host crash drops 20% throughput today. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Scale horizontally first because spreading load decreases per-node criticality.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 1,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "If you keep \"oCR processing pipeline reports: one host crash drops 20% throughput today\" in view, the correct answer separates faster. Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-033",
      "type": "multiple-choice",
      "question": "A geo lookup API reports: dataset just exceeded RAM on current instance family. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Scale vertically first because fit-in-memory requirement drives immediate scale-up.",
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 0,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "The core signal here is \"geo lookup API reports: dataset just exceeded RAM on current instance family\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-034",
      "type": "multiple-choice",
      "question": "A ad click ingestion reports: CPU is fine but TLS handshake rate saturates hosts. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Change DNS TTL and assume that resolves capacity bottlenecks.",
        "Scale horizontally first because additional frontends increase handshake capacity."
      ],
      "correct": 3,
      "explanation": "Horizontal scaling is preferred when workload parallelizes well, bandwidth is host-limited, or resilience/blast-radius dominates. It raises aggregate capacity while improving failure isolation.",
      "detailedExplanation": "The key clue in this question is \"ad click ingestion reports: CPU is fine but TLS handshake rate saturates hosts\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-hv-035",
      "type": "multiple-choice",
      "question": "A compliance scan service reports: cross-shard barriers dominate compute time. For the next quarter, what is the strongest first scaling move?",
      "options": [
        "Do nothing on compute and only increase client retry count.",
        "Disable observability agents so dashboards look healthier.",
        "Scale vertically first because adding nodes worsens barrier synchronization overhead.",
        "Change DNS TTL and assume that resolves capacity bottlenecks."
      ],
      "correct": 2,
      "explanation": "Vertical scaling is preferred when per-node memory/per-core constraints or coordination costs dominate. It buys immediate headroom with minimal architecture change.",
      "detailedExplanation": "Start from \"compliance scan service reports: cross-shard barriers dominate compute time\", then pressure-test the result against the options. Discard choices that violate required invariants during concurrent or failed states. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for feed ranking API: CPU saturation with independent requests. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "CPU-bound parallel workload",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Use \"on-call for feed ranking API: CPU saturation with independent requests\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "add nodes behind load balancer and watch tail latency",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The core signal here is \"given that diagnosis, what is the strongest immediate action\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The decision turns on \"horizontal vs Vertical Scaling Decisions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for rules engine: heap OOM before CPU pressure. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory-bound working set"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Read this as a scenario about \"on-call for rules engine: heap OOM before CPU pressure\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "move to larger-memory instances as a short-term fix",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The key clue in this question is \"given that bottleneck, which scaling action should you take first\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"horizontal vs Vertical Scaling Decisions\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for render farm: network throughput per host capped. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "network bottleneck",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The decision turns on \"on-call for render farm: network throughput per host capped\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "increase host count to raise aggregate egress",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Start from \"based on that root cause, what is the best next move now\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"horizontal vs Vertical Scaling Decisions\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for stateful matcher: distributed lock wait rises with node count. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "coordination overhead",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Start from \"on-call for stateful matcher: distributed lock wait rises with node count\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "reduce cross-node coordination then scale carefully"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The decision turns on \"with that diagnosis confirmed, what is the most effective immediate response\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"horizontal vs Vertical Scaling Decisions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for checkout workers: deploys cause significant capacity dips. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "large-node blast radius",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Start from \"on-call for checkout workers: deploys cause significant capacity dips\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "split into more smaller workers",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The decision turns on \"given this constraint, which action best restores capacity quickly\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for geo processor: single-thread hot path limits latency. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "per-core performance ceiling"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The decision turns on \"on-call for geo processor: single-thread hot path limits latency\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "use larger/faster cores while planning parallelization",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Start from \"given that diagnosis, what is the strongest immediate action\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"horizontal vs Vertical Scaling Decisions\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for notification fanout: bursty arrival every 5 minutes. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "elasticity requirement",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The core signal here is \"on-call for notification fanout: bursty arrival every 5 minutes\". Solve this as chained reasoning where stage two must respect stage one assumptions. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Keep quantities like 5 minutes in aligned units before selecting an answer. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "enable fast horizontal autoscaling",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Use \"given that bottleneck, which scaling action should you take first\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for anti-fraud service: metadata quorum writes dominate p99. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "control-plane bottleneck",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The key clue in this question is \"on-call for anti-fraud service: metadata quorum writes dominate p99\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "decrease node churn and batch control updates"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Read this as a scenario about \"based on that root cause, what is the best next move now\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"horizontal vs Vertical Scaling Decisions\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for preview generator: jobs are independent and queue-backed. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "embarrassingly parallel workload",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "This prompt is really about \"on-call for preview generator: jobs are independent and queue-backed\". Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "increase worker replicas",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "If you keep \"with that diagnosis confirmed, what is the most effective immediate response\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"horizontal vs Vertical Scaling Decisions\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for document parser: RSS climbs with document complexity. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory pressure"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "If you keep \"on-call for document parser: RSS climbs with document complexity\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "scale up RAM and cap concurrent docs",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "This prompt is really about \"given this constraint, which action best restores capacity quickly\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"horizontal vs Vertical Scaling Decisions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for metrics rollup: packet-per-second cap reached. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "host networking limit",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Read this as a scenario about \"on-call for metrics rollup: packet-per-second cap reached\". Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "add ingest nodes and rebalance producers",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The key clue in this question is \"given that diagnosis, what is the strongest immediate action\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"horizontal vs Vertical Scaling Decisions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for promotion evaluator: cross-shard fanout grew 4x. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "high coordination fanout",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Use \"on-call for promotion evaluator: cross-shard fanout grew 4x\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 4x should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "co-locate related keys before adding nodes"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The core signal here is \"given that bottleneck, which scaling action should you take first\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"horizontal vs Vertical Scaling Decisions\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for invoice calculator: tail spikes tied to stop-the-world pauses. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "single-node pause sensitivity",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Start from \"on-call for invoice calculator: tail spikes tied to stop-the-world pauses\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "add more nodes to dilute pause impact",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The decision turns on \"based on that root cause, what is the best next move now\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"horizontal vs Vertical Scaling Decisions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for chat delivery: single node failure removes 18% capacity. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "failure-domain problem"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The decision turns on \"on-call for chat delivery: single node failure removes 18% capacity\". Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Numbers such as 18 should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "increase node count to cut per-node share",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Start from \"with that diagnosis confirmed, what is the most effective immediate response\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"horizontal vs Vertical Scaling Decisions\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for search suggest: cache no longer fits on current hosts. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "insufficient memory footprint",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The key clue in this question is \"on-call for search suggest: cache no longer fits on current hosts\". Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "upgrade node memory class",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Read this as a scenario about \"given this constraint, which action best restores capacity quickly\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"horizontal vs Vertical Scaling Decisions\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for stream enrich: queue age rises while CPU on all nodes > 90%. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "compute saturation",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The core signal here is \"on-call for stream enrich: queue age rises while CPU on all nodes > 90%\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 90 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "add workers and enforce backpressure"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Use \"given that diagnosis, what is the strongest immediate action\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for catalog merge: barrier synchronization dominates runtime. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "synchronization overhead",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The decision turns on \"on-call for catalog merge: barrier synchronization dominates runtime\". Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "reduce barrier frequency before scaling out",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Start from \"given that bottleneck, which scaling action should you take first\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"horizontal vs Vertical Scaling Decisions\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for sensor gateway: connection table maxes out per instance. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "per-host state limit"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Start from \"on-call for sensor gateway: connection table maxes out per instance\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "increase node count and shard connections",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The decision turns on \"based on that root cause, what is the best next move now\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"horizontal vs Vertical Scaling Decisions\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for billing export: batch misses SLA by 30%. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "insufficient parallel capacity",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Use \"on-call for billing export: batch misses SLA by 30%\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 30 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "add temporary worker pool for batch window",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The core signal here is \"with that diagnosis confirmed, what is the most effective immediate response\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The decision turns on \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for report precompute: larger instance type available with 2x memory. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "quick vertical relief option",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "Read this as a scenario about \"on-call for report precompute: larger instance type available with 2x memory\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Strong answers connect quorum/coordination settings to concrete correctness goals. If values like 2x appear, convert them into one unit basis before comparison. Common pitfall: ignoring conflict resolution behavior."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "scale up now and revisit partitioning"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "The key clue in this question is \"given this constraint, which action best restores capacity quickly\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for live scoreboard: frequent deployment with tight SLOs. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "operational resilience pressure",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "If you keep \"on-call for live scoreboard: frequent deployment with tight SLOs\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "Given that diagnosis, what is the strongest immediate action?",
          "options": [
            "prefer more small nodes for safer rollouts",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "This prompt is really about \"given that diagnosis, what is the strongest immediate action\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"horizontal vs Vertical Scaling Decisions\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for OCR API: CPU fine but memory bandwidth saturated. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter",
            "memory-channel bottleneck"
          ],
          "correct": 3,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "This prompt is really about \"on-call for OCR API: CPU fine but memory bandwidth saturated\". Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "Given that bottleneck, which scaling action should you take first?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "move to instances with higher memory bandwidth",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 1,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "If you keep \"given that bottleneck, which scaling action should you take first\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"horizontal vs Vertical Scaling Decisions\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for vector search: coordinator CPU pegs while workers idle. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "central coordination bottleneck",
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 0,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The key clue in this question is \"on-call for vector search: coordinator CPU pegs while workers idle\". Do not reset assumptions between stages; carry forward prior constraints directly. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures."
        },
        {
          "question": "Based on that root cause, what is the best next move now?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "split coordinator role then scale",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 2,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Read this as a scenario about \"based on that root cause, what is the best next move now\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"horizontal vs Vertical Scaling Decisions\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for ad delivery: TLS termination maxes per host. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "frontend host bottleneck",
            "UI thread rendering bottleneck",
            "Certificate renewal jitter"
          ],
          "correct": 1,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The core signal here is \"on-call for ad delivery: TLS termination maxes per host\". Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants."
        },
        {
          "question": "With that diagnosis confirmed, what is the most effective immediate response?",
          "options": [
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work",
            "scale out edge terminators"
          ],
          "correct": 3,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Use \"with that diagnosis confirmed, what is the most effective immediate response\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"horizontal vs Vertical Scaling Decisions\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "On-call for compliance scanner: state transfer during failover is too large. What is the best diagnosis to anchor scaling decisions?",
          "options": [
            "DNS cache warmup issue",
            "UI thread rendering bottleneck",
            "heavy per-node state size",
            "Certificate renewal jitter"
          ],
          "correct": 2,
          "explanation": "Scaling decisions should start from the actual limiting resource or coordination pattern, not generic tactics.",
          "detailedExplanation": "The core signal here is \"on-call for compliance scanner: state transfer during failover is too large\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given this constraint, which action best restores capacity quickly?",
          "options": [
            "first reduce state size, then re-evaluate strategy",
            "Disable retries globally and accept higher failure rate",
            "Freeze deploys for one month without changing capacity",
            "Only tune log sampling and postpone scaling work"
          ],
          "correct": 0,
          "explanation": "The next action should directly reduce the identified bottleneck while preserving service reliability.",
          "detailedExplanation": "Use \"given this constraint, which action best restores capacity quickly\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"horizontal vs Vertical Scaling Decisions\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-061",
      "type": "multi-select",
      "question": "Which signals justify choosing horizontal scaling first?",
      "options": [
        "Independent request processing and high CPU across nodes",
        "Need lower blast radius per failure",
        "Working set only fits on one very large machine",
        "Bursty demand needing elastic capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "If you keep \"signals justify choosing horizontal scaling first\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-062",
      "type": "multi-select",
      "question": "Which conditions make vertical scaling a reasonable first move?",
      "options": [
        "Memory ceiling is the immediate blocker",
        "Minimal time for architectural change",
        "Hard requirement to reduce single-node risk",
        "Strong single-thread performance is required"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "This prompt is really about \"conditions make vertical scaling a reasonable first move\". Validate each option independently; do not select statements that are only partially true. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: propagating an early bad assumption through all steps.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-063",
      "type": "multi-select",
      "question": "Which are common costs of horizontal scaling?",
      "options": [
        "Higher coordination/metadata overhead",
        "More complex partitioning and balancing",
        "Guaranteed lower cloud bill at any scale",
        "Increased operational surface area"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Use \"common costs of horizontal scaling\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-064",
      "type": "multi-select",
      "question": "Which are key risks of vertical-only strategy over time?",
      "options": [
        "Hitting max instance-size limits",
        "Larger per-node blast radius",
        "Automatic elimination of coordination bugs",
        "Potentially slower recovery/replacement"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Read this as a scenario about \"key risks of vertical-only strategy over time\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-065",
      "type": "multi-select",
      "question": "Which metrics are most useful before deciding between scale up vs out?",
      "options": [
        "Per-resource utilization (CPU/memory/network)",
        "Queue depth and queue age trends",
        "Only daily average request count",
        "Tail latency with concurrency context"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The decision turns on \"metrics are most useful before deciding between scale up vs out\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-066",
      "type": "multi-select",
      "question": "For cost modeling of scaling paths, which inputs matter?",
      "options": [
        "Per-node cost by instance class",
        "Coordination overhead as node count rises",
        "Failure and recovery cost assumptions",
        "Only peak QPS without utilization targets"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Start from \"for cost modeling of scaling paths, which inputs matter\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-067",
      "type": "multi-select",
      "question": "Which engineering steps improve horizontal efficiency before adding nodes?",
      "options": [
        "Reduce lock contention hot spots",
        "Improve partition-key distribution",
        "Increase cross-service synchronous fanout",
        "Minimize shared mutable state"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The key clue in this question is \"engineering steps improve horizontal efficiency before adding nodes\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-068",
      "type": "multi-select",
      "question": "Which statements about blast radius are accurate?",
      "options": [
        "More smaller nodes usually reduce per-node impact",
        "One huge node can make failures more severe",
        "Blast radius is unrelated to node size mix",
        "Deployment safety often improves with smaller increments"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The core signal here is \"statements about blast radius are accurate\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-069",
      "type": "multi-select",
      "question": "Which workloads typically benefit from horizontal scaling?",
      "options": [
        "Queue-backed independent jobs",
        "Stateless APIs with uniform requests",
        "Monolithic single-threaded code with shared mutable state",
        "Batch pipelines that can shard by key"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "If you keep \"workloads typically benefit from horizontal scaling\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-070",
      "type": "multi-select",
      "question": "Which workloads often require some vertical scaling component?",
      "options": [
        "Large in-memory models requiring high RAM per process",
        "Latency-sensitive single-thread hot loops",
        "Simple stateless request routers only",
        "Workloads constrained by per-process memory bandwidth"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The key clue in this question is \"workloads often require some vertical scaling component\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-071",
      "type": "multi-select",
      "question": "Which anti-patterns often hide true scaling bottlenecks?",
      "options": [
        "Scaling blindly on CPU average only",
        "Ignoring queue age and tail latency",
        "Correlating metrics with deployment events",
        "Skipping bottleneck decomposition by resource"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Start from \"anti-patterns often hide true scaling bottlenecks\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-072",
      "type": "multi-select",
      "question": "When scale-out appears ineffective, which checks are high-value?",
      "options": [
        "Measure cross-node coordination cost",
        "Inspect partition skew and hot keys",
        "Assume DNS is root cause by default",
        "Review shared datastore contention"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The decision turns on \"scale-out appears ineffective, which checks are high-value\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-073",
      "type": "multi-select",
      "question": "Which are practical short-term actions during memory-bound incidents?",
      "options": [
        "Move to larger-memory nodes temporarily",
        "Reduce per-request memory footprint",
        "Increase retries to smooth memory spikes",
        "Cap concurrency until headroom returns"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Read this as a scenario about \"practical short-term actions during memory-bound incidents\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-074",
      "type": "multi-select",
      "question": "Which statements about coordination overhead are true?",
      "options": [
        "It can rise nonlinearly with node count",
        "Global locks can erase scale-out gains",
        "It always decreases when adding nodes",
        "Partition-local work usually scales better"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "Use \"statements about coordination overhead are true\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-075",
      "type": "multi-select",
      "question": "Which are valid reasons to mix vertical and horizontal strategies?",
      "options": [
        "Scale up for immediate relief, then scale out for resilience",
        "Use larger nodes for coordinators and more small workers",
        "Pick one strategy forever regardless of workload changes",
        "Adapt strategy by bottleneck phase and growth stage"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "This prompt is really about \"valid reasons to mix vertical and horizontal strategies\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-076",
      "type": "multi-select",
      "question": "Which signals suggest network-bound scaling limits?",
      "options": [
        "Per-host egress plateaus at NIC limit",
        "CPU remains moderate during saturation",
        "OOM kills dominate incidents",
        "Adding hosts raises aggregate throughput"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "If you keep \"signals suggest network-bound scaling limits\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-077",
      "type": "multi-select",
      "question": "Which guardrails reduce risk while applying scaling changes?",
      "options": [
        "Canary rollout with rollback thresholds",
        "Predefined max node/size limits",
        "Disable alerts during rollout to reduce noise",
        "Post-change metric comparison against baseline"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices align with bottleneck-driven scaling and operational trade-offs; excluded options are incomplete or misleading.",
      "detailedExplanation": "The core signal here is \"guardrails reduce risk while applying scaling changes\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-078",
      "type": "numeric-input",
      "question": "Each node handles 1600 QPS at 80% target utilization. Peak target is 14000 QPS. Nodes needed?",
      "answer": 11,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "Effective per-node capacity is 1280 QPS. 14000/1280 = 10.94, so 11 nodes.",
      "detailedExplanation": "The key clue in this question is \"each node handles 1600 QPS at 80% target utilization\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 1600 QPS and 80 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-079",
      "type": "numeric-input",
      "question": "Current fleet: 10 nodes at 2200 QPS each. Forecast +40% traffic. Keep 70% utilization. New node count?",
      "answer": 20,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "Future peak is 30799.999999999996 QPS. At target utilization each node provides 1540 QPS, so need 20 nodes.",
      "detailedExplanation": "Start from \"current fleet: 10 nodes at 2200 QPS each\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 10 and 2200 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "sc-hv-080",
      "type": "numeric-input",
      "question": "Option A: 12 small nodes at $0.45/hr. Option B: 5 large nodes at $1.25/hr. Percent increase B vs A?",
      "answer": 15.74,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "A costs 5.40/hr and B costs 6.25/hr. Increase is 15.74%.",
      "detailedExplanation": "Start from \"option A: 12 small nodes at $0\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 12 and 0.45 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-081",
      "type": "numeric-input",
      "question": "Tier has 24 equal nodes. One node fails and traffic redistributes perfectly. Percent capacity loss?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "Losing 1 of 24 equal nodes reduces raw capacity by 1/24 = 4.17%.",
      "detailedExplanation": "The key clue in this question is \"tier has 24 equal nodes\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 24 and 1 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-082",
      "type": "numeric-input",
      "question": "A vertical move doubles per-node memory from 64GB to 128GB. Fleet shrinks from 14 to 8 nodes. Total memory change (%)?",
      "answer": 14.29,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "Old total memory 896GB, new 1024GB. Change is 14.29%.",
      "detailedExplanation": "Read this as a scenario about \"vertical move doubles per-node memory from 64GB to 128GB\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 64GB and 128GB in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-083",
      "type": "numeric-input",
      "question": "Autoscaling target is 65% CPU. Each node sustains 3000 QPS at 100% CPU linear assumption. For 39000 QPS peak, nodes needed?",
      "answer": 20,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "Per-node safe throughput 1950 QPS. 39000/1950 = 20.00 so 20 nodes.",
      "detailedExplanation": "The decision turns on \"autoscaling target is 65% CPU\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 65 and 3000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-084",
      "type": "numeric-input",
      "question": "A workload needs 9 TB/day egress. Each node safely delivers 350 MB/s average. Minimum nodes required?",
      "answer": 1,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "Each node handles about 30.24 TB/day. Need 1 nodes for 9 TB/day.",
      "detailedExplanation": "This prompt is really about \"workload needs 9 TB/day egress\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Numbers such as 9 TB and 350 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-085",
      "type": "numeric-input",
      "question": "Scale-out plan adds 6 nodes to current 18. By what percent does raw node count increase?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "Adding 6 to 18 is a 6/18 = 33.33% increase.",
      "detailedExplanation": "Use \"scale-out plan adds 6 nodes to current 18\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 6 and 18 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-086",
      "type": "numeric-input",
      "question": "Single large node costs $2.40/hr. Equivalent capacity uses 7 small nodes at $0.38/hr. Percent cheaper option?",
      "answer": 9.77,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "Small-node option costs 2.66/hr. Large node costs 2.40/hr, which is 9.77% cheaper.",
      "detailedExplanation": "The core signal here is \"single large node costs $2\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 2.40 and 7 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-087",
      "type": "numeric-input",
      "question": "At 30 nodes, coordination overhead consumes 18% CPU. At 45 nodes, 27% CPU. Relative overhead increase (%)?",
      "answer": 50,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "Coordination overhead rose from 18% to 27%, a relative increase of 50.00%.",
      "detailedExplanation": "If you keep \"at 30 nodes, coordination overhead consumes 18% CPU\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Strong answers connect quorum/coordination settings to concrete correctness goals. Numbers such as 30 and 18 should be normalized first so downstream reasoning stays consistent. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hv-088",
      "type": "numeric-input",
      "question": "Batch job takes 90 min on 12 workers. Assume ideal scaling. Time on 18 workers (minutes)?",
      "answer": 60,
      "unit": "minutes",
      "tolerance": 0.1,
      "explanation": "Under ideal scaling, time is inversely proportional to workers: 90*12/18 = 60 minutes.",
      "detailedExplanation": "Start from \"batch job takes 90 min on 12 workers\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 90 min and 12 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hv-089",
      "type": "numeric-input",
      "question": "A service currently has 16 nodes with 25% spare headroom. Traffic grows 20%. Additional nodes needed (same size)?",
      "answer": 4,
      "unit": "nodes",
      "tolerance": 0.1,
      "explanation": "With 25% headroom, effective capacity equals 20 node-equivalents. After 20% growth you need 19.2 node-equivalents, so 16 nodes remain sufficient and additional nodes needed is 4.",
      "detailedExplanation": "The key clue in this question is \"service currently has 16 nodes with 25% spare headroom\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 16 and 25 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "sc-hv-090",
      "type": "ordering",
      "question": "Rank the scaling workflow from first to last.",
      "items": [
        "Identify bottleneck by resource and contention",
        "Choose scale up/out hypothesis and expected impact",
        "Roll out gradually with guardrails",
        "Re-measure SLO and saturation after rollout"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "The decision turns on \"rank the scaling workflow from first to last\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-091",
      "type": "ordering",
      "question": "Rank from lowest to highest coordination overhead.",
      "items": [
        "Independent stateless workers",
        "Partitioned workers with rare cross-shard calls",
        "Workers using frequent distributed locks",
        "Workers requiring global consensus updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "Read this as a scenario about \"rank from lowest to highest coordination overhead\". Build the rank from biggest differences first, then refine with adjacent checks. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "sc-hv-092",
      "type": "ordering",
      "question": "Rank from smallest to largest per-node failure blast radius.",
      "items": [
        "24 small nodes",
        "12 medium nodes",
        "6 large nodes",
        "1 very large node"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "The key clue in this question is \"rank from smallest to largest per-node failure blast radius\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-093",
      "type": "ordering",
      "question": "Rank from fastest short-term relief to slowest.",
      "items": [
        "Resize to larger instances in same family",
        "Add more replicas with existing topology",
        "Refactor to reduce shared-state contention",
        "Redesign architecture boundaries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "Start from \"rank from fastest short-term relief to slowest\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-094",
      "type": "ordering",
      "question": "Rank autoscaling signals from least to most direct for compute saturation.",
      "items": [
        "Daily average traffic",
        "Five-minute average CPU",
        "Queue age and tail latency",
        "Per-resource saturation plus queue age under load"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "If you keep \"rank autoscaling signals from least to most direct for compute saturation\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-095",
      "type": "ordering",
      "question": "Rank interventions from most to least useful when memory is the hard bottleneck.",
      "items": [
        "Increase per-node memory",
        "Reduce per-request memory footprint",
        "Improve partitioning to lower duplication",
        "Only increase retries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "The core signal here is \"rank interventions from most to least useful when memory is the hard bottleneck\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-096",
      "type": "ordering",
      "question": "Rank decision quality from weakest to strongest.",
      "items": [
        "Choose strategy from team preference only",
        "Choose from average CPU alone",
        "Choose from multi-metric bottleneck analysis",
        "Choose from bottleneck analysis plus canary validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "Use \"rank decision quality from weakest to strongest\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-097",
      "type": "ordering",
      "question": "Rank from least to most resilient deployment pattern.",
      "items": [
        "Single large node replacement",
        "Few large nodes with long drains",
        "Moderate node pool with canary",
        "Many small nodes with progressive rollout"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "This prompt is really about \"rank from least to most resilient deployment pattern\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-098",
      "type": "ordering",
      "question": "Rank from least to most scalable for burst handling.",
      "items": [
        "Fixed-size vertical-only fleet",
        "Vertical scale plus manual adds",
        "Horizontal autoscaling by CPU only",
        "Horizontal autoscaling by CPU and queue age"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "The decision turns on \"rank from least to most scalable for burst handling\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-099",
      "type": "ordering",
      "question": "Rank from lowest to highest operational complexity.",
      "items": [
        "Single pool of homogeneous nodes",
        "Two pools split by workload class",
        "Multi-pool with custom schedulers",
        "Global multi-region compute with per-region policies"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "Read this as a scenario about \"rank from lowest to highest operational complexity\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hv-100",
      "type": "ordering",
      "question": "Rank from worst to best incident response sequence.",
      "items": [
        "Scale randomly and hope",
        "Scale first then inspect metrics",
        "Diagnose bottleneck then scale",
        "Diagnose, scale with guardrails, verify outcome"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The ordering reflects a bottleneck-first approach and reliability-aware scaling practice.",
      "detailedExplanation": "The key clue in this question is \"rank from worst to best incident response sequence\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
