{
  "unit": 6,
  "unitTitle": "Messaging & Async",
  "chapter": 6,
  "chapterTitle": "Stream Processing",
  "chapterDescription": "Processing continuous data streams: partitions, consumer groups, offsets, windowing strategies, stream-table duality, watermarks, and late data handling.",
  "problems": [
    {
      "id": "msg-str-001",
      "type": "multiple-choice",
      "question": "A Kafka topic has 12 partitions. A consumer group has 4 consumers. How are partitions assigned?",
      "options": [
        "One consumer reads all 12 partitions while the others stand by",
        "All consumers read all 12 partitions simultaneously",
        "Partitions are assigned randomly on each message",
        "Each consumer is assigned 3 partitions, processing them independently"
      ],
      "correct": 3,
      "explanation": "Kafka distributes partitions evenly across consumer group members: 12 partitions / 4 consumers = 3 partitions each. Each partition is assigned to exactly one consumer in the group, ensuring each message is processed once. If a consumer crashes, its partitions are reassigned to the remaining consumers (rebalancing).",
      "detailedExplanation": "The key clue in this question is \"kafka topic has 12 partitions\". Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 12 and 4 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-002",
      "type": "multiple-choice",
      "question": "A Kafka topic has 6 partitions. A consumer group has 8 consumers. What happens to the 2 extra consumers?",
      "options": [
        "They share partitions with other consumers for parallel processing",
        "They read from a different topic automatically",
        "They sit idle — Kafka assigns at most one consumer per partition within a group, so 2 consumers have no partitions",
        "Kafka creates 2 additional partitions to match"
      ],
      "correct": 2,
      "explanation": "With more consumers than partitions, some consumers are idle. Kafka assigns each partition to exactly one group member — it never splits a partition across consumers. The idle consumers serve as hot standbys: if an active consumer fails, an idle one gets its partitions instantly. This is why consumer count should generally not exceed partition count.",
      "detailedExplanation": "Read this as a scenario about \"kafka topic has 6 partitions\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 6 and 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-003",
      "type": "multiple-choice",
      "question": "A team processes clickstream data with a Kafka consumer group. They need exactly 3x throughput. Currently they have 4 partitions and 4 consumers. What must they change?",
      "options": [
        "Increase partitions to 12 and add 8 more consumers (12 total) — throughput scales with partition count, not just consumer count",
        "Increase the batch size by 3x",
        "Add 8 more consumers to the existing group",
        "Create 3 separate consumer groups reading the same topic"
      ],
      "correct": 0,
      "explanation": "Adding consumers beyond partition count doesn't help — extra consumers sit idle. To scale 3x, increase partitions to 12 (3 × 4) and add consumers to match. Each consumer processes one partition, so 12 partitions with 12 consumers gives 3x throughput. Creating 3 consumer groups would process each message 3 times (fan-out), not 3x throughput on unique messages.",
      "detailedExplanation": "The decision turns on \"team processes clickstream data with a Kafka consumer group\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 3x and 4 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-004",
      "type": "multiple-choice",
      "question": "A Kafka consumer processes a batch of messages, then crashes before committing its offset. When it restarts, what happens?",
      "options": [
        "The messages are lost — Kafka deletes them after delivery",
        "The consumer resumes from where it crashed, skipping the uncommitted batch",
        "Kafka resends messages from the last committed offset, causing the already-processed messages to be redelivered",
        "Kafka notifies the producer to resend the messages"
      ],
      "correct": 2,
      "explanation": "Kafka tracks consumer position via committed offsets. If the consumer processed messages at offsets 100-150 but only committed up to 99, restart resumes from offset 100 — redelivering 100-150. This is at-least-once delivery: messages may be processed more than once. To avoid duplicate effects, the consumer must be idempotent or use exactly-once semantics (transactions).",
      "detailedExplanation": "This prompt is really about \"kafka consumer processes a batch of messages, then crashes before committing its offset\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 100 and 150 should be normalized first so downstream reasoning stays consistent. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-005",
      "type": "multiple-choice",
      "question": "A stream processing job computes a running count of page views per URL. It receives events continuously: {url, timestamp, userId}. Rather than a batch job that runs hourly, it updates the count in real-time as each event arrives. What type of processing is this?",
      "options": [
        "Stream processing — compute incrementally as each event arrives, maintaining a running state",
        "MapReduce with periodic shuffles",
        "Request-response processing with synchronous calls",
        "Batch processing — aggregate all events, then compute the result"
      ],
      "correct": 0,
      "explanation": "Stream processing computes results incrementally on unbounded data. Unlike batch (which processes finite datasets), stream processing updates state with each new event: count[url] += 1. The result is always current (modulo processing lag). This requires maintaining state across events — the running count must persist between messages. Frameworks like Kafka Streams, Flink, and Spark Streaming support this pattern.",
      "detailedExplanation": "Use \"stream processing job computes a running count of page views per URL\" as your starting point, then verify tradeoffs carefully. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-006",
      "type": "multiple-choice",
      "question": "A stream processor counts events per 5-minute window. An event with timestamp 10:03:22 arrives. Which window does it belong to?",
      "options": [
        "A new window created specifically for this event",
        "The 10:00-10:05 window — tumbling windows are fixed, non-overlapping intervals aligned to the clock",
        "The 10:05-10:10 window (next window)",
        "Both the 10:00-10:05 and 10:05-10:10 windows"
      ],
      "correct": 1,
      "explanation": "Tumbling windows divide time into fixed, non-overlapping intervals. With 5-minute tumbling windows: [10:00, 10:05), [10:05, 10:10), etc. An event at 10:03:22 falls in [10:00, 10:05). Each event belongs to exactly one window. Tumbling windows are the simplest windowing strategy — no overlap, no gaps, deterministic assignment.",
      "detailedExplanation": "The core signal here is \"stream processor counts events per 5-minute window\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 5 and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-007",
      "type": "multiple-choice",
      "question": "A stream processor uses sliding windows of 10 minutes with a 2-minute slide. An event at 10:07 belongs to how many windows?",
      "options": [
        "2 windows",
        "Exactly 1 window",
        "10 windows",
        "5 windows — the event falls in every window whose range includes 10:07 (windows starting at 9:58, 10:00, 10:02, 10:04, 10:06)"
      ],
      "correct": 3,
      "explanation": "Sliding (or hopping) windows with 10-minute size and 2-minute slide: windows start at 9:58, 10:00, 10:02, 10:04, 10:06, etc. The event at 10:07 is within the range of windows starting at 9:58 (9:58-10:08), 10:00 (10:00-10:10), 10:02 (10:02-10:12), 10:04 (10:04-10:14), and 10:06 (10:06-10:16). That's 5 windows. Window count = ceiling(size / slide) = ceiling(10/2) = 5.",
      "detailedExplanation": "If you keep \"stream processor uses sliding windows of 10 minutes with a 2-minute slide\" in view, the correct answer separates faster. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 10 minutes and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-008",
      "type": "multiple-choice",
      "question": "A stream processor counts login attempts per user. It should reset the count if no login attempt occurs for 30 minutes. If a new attempt arrives within 30 minutes, the window extends. What windowing strategy is this?",
      "options": [
        "Session windows — the window starts with the first event and extends with each subsequent event, closing after 30 minutes of inactivity (the gap timeout)",
        "Tumbling windows with a 30-minute size",
        "Sliding windows with a 30-minute slide",
        "Fixed windows with a 30-minute expiry"
      ],
      "correct": 0,
      "explanation": "Session windows are activity-based, not clock-based. A session starts with the first event and stays open as long as events keep arriving within the gap timeout (30 minutes). If no event arrives for 30 minutes, the session closes. Each user gets independent sessions. This is ideal for tracking user sessions, login burst detection, and activity patterns where behavior-based grouping matters more than clock-based intervals.",
      "detailedExplanation": "Start from \"stream processor counts login attempts per user\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 30 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-009",
      "type": "multiple-choice",
      "question": "A Kafka consumer commits offsets after every message (synchronous commit). This ensures no message is reprocessed after a crash but severely reduces throughput. What is the standard trade-off?",
      "options": [
        "Commit offsets periodically (e.g., every 5 seconds or every 1,000 messages), accepting that a crash may cause a small window of redelivery",
        "Commit offsets before processing each message",
        "Never commit offsets — rely on message expiry",
        "Let the broker commit offsets automatically after delivery"
      ],
      "correct": 0,
      "explanation": "Periodic commits balance throughput and redelivery risk. Committing every 5 seconds means a crash loses at most 5 seconds of progress — those messages are redelivered. At 10,000 msg/s, that's up to 50,000 redelivered messages. The consumer must be idempotent to handle this. This is Kafka's default (auto-commit every 5 seconds). Manual periodic commits give more control over exactly when offsets advance.",
      "detailedExplanation": "The key clue in this question is \"kafka consumer commits offsets after every message (synchronous commit)\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 5 seconds and 10,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-010",
      "type": "multiple-choice",
      "question": "A team uses Kafka to process order events. They partition the 'orders' topic by customer_id. Why is this partitioning key important for their stream processing job that maintains per-customer state?",
      "options": [
        "It distributes data randomly for load balancing",
        "It makes the topic easier to browse",
        "It enables compression of similar events",
        "All events for the same customer go to the same partition, ensuring a single consumer processes them in order — required for consistent per-customer state updates"
      ],
      "correct": 3,
      "explanation": "Partitioning by customer_id ensures all events for customer X land on the same partition. Since Kafka guarantees ordering within a partition, the consumer sees customer X's events in the correct sequence. For per-customer state (order count, running total), processing events out of order would produce incorrect results. The partition key is the unit of ordering and parallelism in Kafka.",
      "detailedExplanation": "The decision turns on \"team uses Kafka to process order events\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-011",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processing job reads from a Kafka topic and writes results to a database. The job processes an event, writes to the database, then commits the Kafka offset. The database write succeeds but the offset commit fails (network blip). What happens on retry?",
          "options": [
            "The database rejects the second write automatically",
            "The event is skipped — the database already has the result",
            "The event is reprocessed: Kafka redelivers it (offset wasn't committed) and the database receives a duplicate write",
            "Kafka detects the duplicate and suppresses it"
          ],
          "correct": 2,
          "explanation": "Without exactly-once guarantees, the gap between 'write to DB' and 'commit offset' creates a duplicate risk. The DB write succeeded, but Kafka doesn't know that. On restart, Kafka replays from the last committed offset, causing the event to be processed again. The consumer must be idempotent (e.g., upsert with event ID) to handle this, or use Kafka transactions for exactly-once semantics.",
          "detailedExplanation": "Read this as a scenario about \"stream processing job reads from a Kafka topic and writes results to a database\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "To achieve exactly-once semantics for this Kafka-to-database pipeline, the team considers three approaches: (A) idempotent writes using the event's unique ID, (B) Kafka transactions that atomically commit the offset and produce output, (C) storing the Kafka offset in the database in the same transaction as the result. Which approach works when the sink is an external database (not a Kafka topic)?",
          "options": [
            "Only approach A works — Kafka transactions can't span external systems",
            "Only approach B (Kafka transactions) works with external databases",
            "Approaches A and C both work — idempotent writes tolerate duplicates, and storing offsets in the DB makes the write and offset commit atomic",
            "None of them — exactly-once is impossible with external databases"
          ],
          "correct": 2,
          "explanation": "Kafka transactions (B) only span Kafka-to-Kafka operations — they can't include external database writes. For external sinks: (A) Idempotent writes (upsert by event ID) make duplicates harmless. (C) Storing Kafka offsets in the same DB transaction as the result makes the two operations atomic — if the DB write rolls back, the offset isn't advanced. On restart, both are retried together. Many production pipelines combine A and C for defense-in-depth.",
          "detailedExplanation": "The key clue in this question is \"to achieve exactly-once semantics for this Kafka-to-database pipeline, the team\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-012",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processing job computes hourly revenue totals from an order stream. Events arrive with event_time (when the order was placed) and processing_time (when the processor receives it). Network delays cause some events to arrive 10 minutes late. If the job uses processing_time for windowing, what's the consequence?",
          "options": [
            "No consequence — processing time is always accurate",
            "The processor buffers all events until the hour ends",
            "Late events are dropped automatically",
            "Late events are assigned to the wrong hour window. An order placed at 9:55 but arriving at 10:05 is counted in the 10:00-11:00 window instead of 9:00-10:00"
          ],
          "correct": 3,
          "explanation": "Processing-time windows assign events based on when they arrive, not when they occurred. With network delays, late events end up in the wrong window. For revenue reporting, this means the 9:00-10:00 window is permanently undercounted (missing late orders) and the 10:00-11:00 window is overcounted. Event-time processing avoids this by using the event's timestamp for window assignment.",
          "detailedExplanation": "If you keep \"stream processing job computes hourly revenue totals from an order stream\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 10 minutes and 9 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "The team switches to event-time windowing. But they need to know when a window's result is 'complete enough' to emit. At 10:05, can they be sure all events for the 9:00-10:00 window have arrived? What mechanism tracks this?",
          "options": [
            "A fixed delay timer — wait 15 minutes after the window closes, then emit",
            "The producer sends a 'window complete' signal",
            "Watermarks — a timestamp that advances as events are processed, asserting 'no events with timestamps before this value are expected to arrive'",
            "The broker tracks the oldest unprocessed event per partition"
          ],
          "correct": 2,
          "explanation": "Watermarks are the stream processing mechanism for tracking event-time progress. A watermark at 10:05 says: 'I believe all events with event_time < 10:05 have arrived.' When the watermark passes the window's end (10:00), the window result can be emitted. Watermarks are imperfect — they're heuristics based on observed event timestamps. Events arriving after the watermark passes are 'late data' and require special handling.",
          "detailedExplanation": "This prompt is really about \"team switches to event-time windowing\". Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 10 and 05 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"stream Processing\". Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-013",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processor using event-time windowing has a watermark at 10:15. An event arrives with event_time 9:58 — it's 17 minutes late and the 9:00-10:00 window has already been emitted. What is this event called?",
          "options": [
            "A late event — it arrived after the watermark passed the window boundary, so the window result was already emitted",
            "A duplicate event",
            "A future event from a clock-skewed producer",
            "An out-of-order event within the window"
          ],
          "correct": 0,
          "explanation": "Late data: events that arrive after the watermark has advanced past their window. The 9:00-10:00 window was emitted when the watermark passed 10:00. An event at 9:58 arriving when the watermark is at 10:15 missed its window. Handling options: drop the late event (accept inaccuracy), update the already-emitted window result (requires downstream consumers to handle updates), or buffer the result and allow late arrivals within a grace period.",
          "detailedExplanation": "This prompt is really about \"stream processor using event-time windowing has a watermark at 10:15\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 10 and 15 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "The team decides late events within 30 minutes of window close should be included (an 'allowed lateness' or grace period). Events more than 30 minutes late are dropped. What trade-off does a longer grace period introduce?",
          "options": [
            "Higher accuracy (fewer dropped late events) but higher memory/state usage — the processor must keep window state open longer to accept late arrivals, and downstream consumers see more result updates",
            "Higher throughput because batches are larger",
            "No trade-off — longer is always better",
            "Lower accuracy because more events are included"
          ],
          "correct": 0,
          "explanation": "Grace period trade-offs: longer periods catch more late data (higher accuracy) but consume more memory (window state stays open longer) and complicate downstream systems (they must handle updated results). A 30-minute grace means every window's state is retained for 30 extra minutes. With thousands of windows active simultaneously, this memory overhead adds up. The right grace period depends on the SLA: financial reconciliation needs high accuracy (long grace); real-time dashboards can tolerate small inaccuracies (short grace).",
          "detailedExplanation": "If you keep \"team decides late events within 30 minutes of window close should be included (an\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 30 minutes and 30 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"stream Processing\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-014",
      "type": "two-stage",
      "stages": [
        {
          "question": "A Kafka Streams application maintains a local state store (RocksDB) for aggregation. The application instance crashes and restarts on a different machine. The local state store is lost. How does Kafka Streams restore it?",
          "options": [
            "Request the state from other application instances",
            "Replay the changelog topic — Kafka Streams backs up every state change to a Kafka topic, and replays it on recovery to rebuild the local store",
            "Restore from a database backup",
            "The state is permanently lost — restart the aggregation from scratch"
          ],
          "correct": 1,
          "explanation": "Kafka Streams' fault tolerance: every write to the local state store is also published to a changelog topic (a compacted Kafka topic). On crash recovery, the new instance replays the changelog to rebuild the exact state. The changelog is the source of truth — the local store is a materialized cache. This gives local-speed reads with Kafka-backed durability. The replay time depends on the state size (mitigated by standby replicas).",
          "detailedExplanation": "The key clue in this question is \"kafka Streams application maintains a local state store (RocksDB) for aggregation\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Replaying the changelog for a large state store takes 15 minutes. During this time, the partitions assigned to the crashed instance aren't being processed. What feature reduces this recovery time?",
          "options": [
            "Increase the number of partitions",
            "Use a faster disk for the local state store",
            "Standby replicas — other instances maintain warm copies of the state store by continuously replaying the changelog, enabling near-instant failover",
            "Store state in an external database instead of locally"
          ],
          "correct": 2,
          "explanation": "Standby replicas: Kafka Streams can be configured to maintain standby copies of state stores on other instances. These standbys continuously consume the changelog, keeping their copy nearly up-to-date. On failover, the standby has the state already — recovery requires replaying only the handful of events since the last standby sync, not the entire changelog. This reduces recovery from minutes to seconds.",
          "detailedExplanation": "Read this as a scenario about \"replaying the changelog for a large state store takes 15 minutes\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 15 minutes appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"stream Processing\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-015",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team wants to join a stream of orders with a table of customer profiles. The orders arrive continuously; customer profiles change infrequently. In Kafka Streams terminology, what is this join called?",
          "options": [
            "A stream-to-table (KStream-KTable) join — each order event is enriched with the latest customer profile from the table",
            "A batch join where all orders are collected before joining",
            "A table-to-table join performed periodically",
            "A stream-to-stream join requiring a time window"
          ],
          "correct": 0,
          "explanation": "Stream-table joins: the stream (orders) is unbounded and processed event-by-event. The table (customer profiles) is a materialized view of the latest state per key. When an order arrives for customer_id=X, the processor looks up X's current profile in the table and enriches the order. The table is updated when profile change events arrive. This is the most common join pattern in stream processing — enriching fast-moving events with slow-changing reference data.",
          "detailedExplanation": "The core signal here is \"team wants to join a stream of orders with a table of customer profiles\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "The customer profile table is built from a compacted Kafka topic. A customer updates their address. What does the compacted topic retain?",
          "options": [
            "Only the latest value per key — compaction deletes older records for the same key, keeping only the most recent version of each customer's profile",
            "Only the first value ever published for that customer",
            "A diff between the old and new values",
            "All historical address values for that customer"
          ],
          "correct": 0,
          "explanation": "Log compaction: Kafka periodically removes older records for the same key, retaining only the latest value. For customer_id=X, if 10 profile updates were published, compaction keeps only the most recent one. This makes compacted topics ideal for materializing tables — they represent current state per key with bounded storage. The trade-off: you lose the history of changes (unlike a regular topic that retains all events within the retention period).",
          "detailedExplanation": "Use \"customer profile table is built from a compacted Kafka topic\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 10 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "The core signal here is \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-016",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team processes a stream of sensor readings with Kafka. Each reading has a sensor_id and a value. They partition the topic by sensor_id. A single sensor generates 50% of all traffic (a 'hot' sensor). What problem does this cause?",
          "options": [
            "The hot sensor's events are dropped",
            "The partition containing the hot sensor receives 50% of all traffic, while other partitions are nearly idle — creating a data skew that limits parallelism",
            "Kafka redistributes events across partitions automatically",
            "No problem — Kafka handles uneven partitions automatically"
          ],
          "correct": 1,
          "explanation": "Partition key skew: if one sensor produces 50% of events, the partition for that sensor_id handles 50% of the load. One consumer processes half the traffic while others are underutilized. Scaling from 4 to 40 consumers doesn't help — the bottleneck is the single hot partition. Solutions: use a composite key (sensor_id + random suffix) to spread the hot sensor across partitions (at the cost of per-sensor ordering), or process the hot sensor on dedicated infrastructure.",
          "detailedExplanation": "The decision turns on \"team processes a stream of sensor readings with Kafka\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 50 and 4 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "The team decides to spread the hot sensor across 4 partitions using a composite key (sensor_id + random(0-3)). But their downstream aggregation needs all readings for a sensor in one place. How do they reconcile this?",
          "options": [
            "Aggregate within each partition and accept incomplete results",
            "They can't — partitioning and aggregation are mutually exclusive",
            "Use a two-stage pipeline: first stage uses composite keys for parallel processing, then a repartition step groups by sensor_id for aggregation",
            "Store all results in a shared database that both partitions write to"
          ],
          "correct": 2,
          "explanation": "Two-stage processing: (1) Spread the hot sensor across partitions with composite keys for parallel pre-processing (filtering, normalization). (2) Repartition (re-key) by sensor_id to bring all readings together for aggregation. The repartition step writes to an intermediate Kafka topic keyed by sensor_id. This adds latency (an extra topic hop) but enables both parallelism and correct aggregation. Kafka Streams handles this automatically with through() or repartition().",
          "detailedExplanation": "Start from \"team decides to spread the hot sensor across 4 partitions using a composite key\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 4 and 0 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Use \"stream Processing\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-017",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processing framework offers two APIs: a low-level 'process each event' API and a high-level declarative API (like SQL over streams). The team wants to compute 'average order value per region per hour' from an order event stream. Which API is more appropriate?",
          "options": [
            "Neither — this requires batch processing",
            "The low-level API — they need fine-grained control over each event",
            "The low-level API wrapped in a custom framework",
            "The declarative API — 'SELECT region, AVG(total) FROM orders GROUP BY region, WINDOW(TUMBLE, 1 HOUR)' expresses the intent directly without manual state management"
          ],
          "correct": 3,
          "explanation": "Declarative stream SQL (like Flink SQL, KSQL, or Spark Structured Streaming) expresses windowed aggregations naturally. The framework handles state management, windowing, watermarks, and late data — the developer just describes the query. Low-level APIs give more control (custom state, complex event processing) but require manual window management, state cleanup, and watermark handling. For standard aggregations, declarative APIs are simpler and less error-prone.",
          "detailedExplanation": "Start from \"stream processing framework offers two APIs: a low-level 'process each event' API and a\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "The team uses the declarative API. Their query works perfectly, but they also need custom logic: 'if an order's total exceeds $10,000, emit an alert immediately without waiting for the window to close.' Can the declarative API handle this?",
          "options": [
            "It depends — declarative APIs excel at aggregations but struggle with event-level side effects that must fire immediately. This alert logic is better suited to the low-level process API or a separate stream branch",
            "No — declarative APIs can't filter events",
            "Yes — add a WHERE clause filtering totals > $10,000",
            "Yes — use a 0-second window"
          ],
          "correct": 0,
          "explanation": "Declarative APIs handle aggregations, joins, and filtering well, but immediate event-level side effects (alerting, API calls) often require the low-level API. The best approach: split the stream. One branch uses declarative SQL for hourly aggregations. A separate branch uses the low-level API to inspect each event and emit alerts for high-value orders. Many stream processing jobs combine both APIs — declarative for standard analytics, low-level for custom logic.",
          "detailedExplanation": "The decision turns on \"team uses the declarative API\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. If values like 10,000 appear, convert them into one unit basis before comparison. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-018",
      "type": "multi-select",
      "question": "Which are valid reasons to increase the number of partitions for a Kafka topic? (Select all that apply)",
      "options": [
        "To increase write throughput — producers can write to more partitions in parallel",
        "To enable finer-grained consumer scaling (e.g., scaling from 4 to 12 consumers requires at least 12 partitions)",
        "To improve ordering guarantees — more partitions mean stronger ordering",
        "To increase consumer parallelism — more partitions allow more consumers to process in parallel"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "More partitions enable more parallelism (both producer writes and consumer reads) and finer scaling granularity. However, more partitions weaken ordering — Kafka guarantees order only within a partition, not across partitions. Going from 1 to 12 partitions means events with different keys may arrive out of order relative to each other. More partitions also increase metadata overhead on the Kafka cluster and rebalance time.",
      "detailedExplanation": "The decision turns on \"valid reasons to increase the number of partitions for a Kafka topic? (Select all that\". Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 1 and 12 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-019",
      "type": "multi-select",
      "question": "Which are trade-offs of using event-time (vs. processing-time) for stream windowing? (Select all that apply)",
      "options": [
        "Simpler implementation — no need to handle out-of-order events or late data",
        "Must handle late-arriving events that miss their window (allowed lateness, side outputs)",
        "Requires watermarks to track event-time progress and determine when windows are complete",
        "More accurate results — events are assigned to windows based on when they actually occurred"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Event-time processing is more accurate (events go to the correct window) but more complex. It requires watermarks (to track progress), late data handling (grace periods, side outputs for dropped events), and state management (keeping windows open for late arrivals). Processing-time is simpler (no watermarks, no late data concept) but less accurate when events arrive out of order. The 'simpler implementation' option describes processing-time, not event-time.",
      "detailedExplanation": "Read this as a scenario about \"trade-offs of using event-time (vs\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-020",
      "type": "multi-select",
      "question": "A Kafka Streams application performs a stateful aggregation. Which components are involved in making this state fault-tolerant? (Select all that apply)",
      "options": [
        "A changelog topic in Kafka that receives every state mutation for durable backup",
        "A local state store (e.g., RocksDB) for fast reads and writes during processing",
        "An external distributed database that all instances share",
        "Standby replicas on other instances that pre-load the changelog for fast failover"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Kafka Streams' state fault tolerance: (1) Local state store for fast access during processing. (2) Changelog topic for durable backup — every write to the local store is published to a compacted Kafka topic. (3) Optional standby replicas that continuously replay the changelog on other instances for fast failover. No external database is needed — the Kafka changelog is the durable backing store. This 'local + changelog' pattern gives both performance (local reads) and durability (Kafka replication).",
      "detailedExplanation": "Read this as a scenario about \"kafka Streams application performs a stateful aggregation\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-021",
      "type": "multi-select",
      "question": "Which scenarios benefit from session windows instead of tumbling or sliding windows? (Select all that apply)",
      "options": [
        "Detecting login attempt bursts where activity gaps indicate separate attack sessions",
        "Computing fixed 5-minute aggregations for a monitoring dashboard",
        "Grouping IoT device events into activity periods separated by sleep intervals",
        "Tracking user browsing sessions that have variable duration based on activity"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Session windows group events by activity, not by fixed time intervals. They're ideal when the meaningful boundary is a gap in activity: user browsing sessions (session ends when user stops clicking), login bursts (gap between bursts defines separate sessions), and IoT activity periods (device sleep intervals separate sessions). Fixed 5-minute aggregations use tumbling windows — the boundary is clock-based, not activity-based.",
      "detailedExplanation": "The decision turns on \"scenarios benefit from session windows instead of tumbling or sliding windows? (Select\". Treat every option as a separate true/false test under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-022",
      "type": "numeric-input",
      "question": "A Kafka topic has 24 partitions. A consumer group has 6 consumers. Each consumer processes 5,000 messages/second from its assigned partitions. What is the total consumer group throughput in messages/second?",
      "answer": 30000,
      "unit": "msg/s",
      "tolerance": "exact",
      "explanation": "24 partitions / 6 consumers = 4 partitions each. Each consumer processes 5,000 msg/s. Total: 6 × 5,000 = 30,000 msg/s. If the incoming rate exceeds 30,000 msg/s, consumer lag grows. Options: add consumers (up to 24, matching partition count), optimize processing logic, or increase partitions for more parallelism.",
      "detailedExplanation": "Start from \"kafka topic has 24 partitions\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 24 and 6 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-023",
      "type": "numeric-input",
      "question": "A stream processor uses tumbling windows of 1 hour. Events arrive at 50,000/second. Each event is 200 bytes. How much state must the processor maintain for the current window (assume no compression)?",
      "answer": 36,
      "unit": "GB",
      "tolerance": 0.05,
      "explanation": "Events per hour: 50,000/s × 3,600s = 180,000,000 events. State size: 180,000,000 × 200 bytes = 36,000,000,000 bytes = 36 GB. This is the raw state for one window. With overlapping windows (sliding), multiply by the number of active windows. Stream processors need sufficient memory/disk for window state — this is why state management and windowing configuration directly affect infrastructure costs.",
      "detailedExplanation": "The key clue in this question is \"stream processor uses tumbling windows of 1 hour\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1 hour and 50,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-024",
      "type": "ordering",
      "question": "Rank these Kafka consumer offset commit strategies from most likely to reprocess duplicates to least likely:",
      "items": [
        "Auto-commit every 30 seconds (large commit interval)",
        "Auto-commit every 5 seconds (default)",
        "Manual commit after every 100 messages",
        "Manual commit after every single message (synchronous)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Larger commit intervals mean larger windows of uncommitted work. A crash during a 30-second interval could reprocess up to 30 seconds of messages. At 5 seconds, max reprocessing is 5 seconds. Committing every 100 messages (at 1000 msg/s, about every 100ms) reduces the window further. Per-message synchronous commits minimize reprocessing but have the highest overhead — each commit is a round-trip to the broker.",
      "detailedExplanation": "The core signal here is \"rank these Kafka consumer offset commit strategies from most likely to reprocess\". Place obvious extremes first, then sort the middle by pairwise comparison. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 30 and 30 seconds in aligned units before selecting an answer. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-025",
      "type": "ordering",
      "question": "Rank these windowing strategies from simplest to implement to most complex:",
      "items": [
        "Tumbling windows (fixed size, no overlap)",
        "Sliding/hopping windows (fixed size, overlapping by slide interval)",
        "Session windows (variable size, gap-based)",
        "Custom windows with allowed lateness, watermarks, and side outputs for late data"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Tumbling windows: each event belongs to exactly one window, deterministic. Sliding windows: events belong to multiple overlapping windows, more state. Session windows: variable-length, require merging when events bridge a gap, dynamic state management. Custom windows with late data handling add watermark tracking, grace periods, state retention, and side outputs for discarded events — the most complex configuration.",
      "detailedExplanation": "If you keep \"rank these windowing strategies from simplest to implement to most complex:\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-026",
      "type": "multiple-choice",
      "question": "A Kafka topic retains messages for 7 days. A new consumer group starts reading the topic. The team wants it to process all existing messages, not just new ones. What consumer configuration controls this?",
      "options": [
        "Set auto.offset.reset to 'latest' to catch up quickly",
        "Configure the broker to resend all messages",
        "Set max.poll.records to the total message count",
        "Set auto.offset.reset to 'earliest' — when no committed offset exists for the group, start from the beginning of the topic"
      ],
      "correct": 3,
      "explanation": "auto.offset.reset controls where a new consumer group starts: 'earliest' reads from the oldest available message (beginning of retention window), 'latest' reads only new messages from the point of joining. For backfill or replay scenarios, 'earliest' is required. For real-time-only consumers that don't need history, 'latest' avoids processing a potentially huge backlog.",
      "detailedExplanation": "This prompt is really about \"kafka topic retains messages for 7 days\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 7 days appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-027",
      "type": "multiple-choice",
      "question": "A stream processing job maintains a running count per key. If the processor crashes and restarts from the last committed offset, it will reprocess some events, incrementing the count for those keys a second time. What property must the state update logic have to produce correct results despite reprocessing?",
      "options": [
        "Reversibility — each operation can be undone",
        "Idempotency — applying the same event twice produces the same result as applying it once",
        "Associativity — grouping of operations doesn't affect the result",
        "Commutativity — the order of operations doesn't matter"
      ],
      "correct": 1,
      "explanation": "Idempotent state updates ensure reprocessing is safe: processing event X twice has the same effect as processing it once. For a simple counter, 'count += 1' is NOT idempotent (reprocessing adds 1 again). An idempotent alternative: track processed event IDs and skip duplicates, or use 'set count to N' instead of 'increment by 1'. Kafka's exactly-once semantics (transactions + idempotent producers) provide framework-level idempotency for Kafka-to-Kafka pipelines.",
      "detailedExplanation": "Use \"stream processing job maintains a running count per key\" as your starting point, then verify tradeoffs carefully. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-028",
      "type": "multiple-choice",
      "question": "A team needs to join two event streams in real-time: 'ad impressions' and 'ad clicks.' A click should be matched with its impression (same ad_id) to calculate click-through rate. Impressions and clicks may arrive minutes apart. What type of stream join is this?",
      "options": [
        "A cross join of all impressions with all clicks",
        "A batch join that collects both streams hourly",
        "A windowed stream-stream join — both streams are unbounded, so a time window limits how far apart events can be matched",
        "A stream-table join using a materialized impression table"
      ],
      "correct": 2,
      "explanation": "Stream-stream joins require a time window because both sides are unbounded — you can't hold all impressions forever waiting for clicks. A 30-minute window means: 'match a click with an impression if they share an ad_id and occurred within 30 minutes of each other.' Events outside the window are unmatched. The window determines how much state to maintain and how long to wait for the matching event.",
      "detailedExplanation": "Read this as a scenario about \"team needs to join two event streams in real-time: 'ad impressions' and 'ad clicks\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 30 and 30 minutes appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-029",
      "type": "multiple-choice",
      "question": "A team partitions their Kafka topic by user_id. Their stream processing job needs to compute a global top-10 leaderboard across all users. A single partition contains only a subset of users. How does the processor compute the global result?",
      "options": [
        "Each partition independently computes a global top-10",
        "A two-phase approach: each partition computes a local top-10, then a repartition step merges all local top-10s into a single partition for the global result",
        "All events are routed to a single partition for global computation",
        "The broker aggregates results across partitions automatically"
      ],
      "correct": 1,
      "explanation": "Global aggregations across partitioned data require a two-phase approach: (1) Local aggregation — each partition computes its top-10 (parallel, scalable). (2) Global merge — local results are repartitioned to a single partition where they're merged into the final top-10. This pattern minimizes the data flowing to the single merge step (10 results per partition instead of all events). It's the stream equivalent of MapReduce's combiner pattern.",
      "detailedExplanation": "The decision turns on \"team partitions their Kafka topic by user_id\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 10 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-030",
      "type": "multiple-choice",
      "question": "A stream processor computes moving averages over a 1-hour sliding window with a 1-minute slide. At any given time, how many active windows does each event potentially contribute to?",
      "options": [
        "It depends on the event's timestamp",
        "60 windows — the event is included in every window whose time range covers its timestamp",
        "1 window",
        "2 windows (current and next)"
      ],
      "correct": 1,
      "explanation": "With a 1-hour window and 1-minute slide, windows start every minute: [10:00-11:00), [10:01-11:01), [10:02-11:02), etc. An event at 10:30 is covered by windows starting from 9:31 through 10:30 — that's 60 windows. Each event contributes to window_size / slide_interval = 60/1 = 60 active windows. This is why sliding windows with small slides are expensive: each event updates many windows' state.",
      "detailedExplanation": "Use \"stream processor computes moving averages over a 1-hour sliding window with a 1-minute\" as your starting point, then verify tradeoffs carefully. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 1 and 10 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-031",
      "type": "multiple-choice",
      "question": "A team's Kafka Streams application checkpoints state to a local RocksDB store. The store grows to 50 GB. The application is restarted for a code update. How long does the restart take if the state can be loaded from the local disk?",
      "options": [
        "50 GB / network throughput — the state must be downloaded from Kafka",
        "Instantaneous — state is kept in memory",
        "50 GB / disk throughput — loading from local disk is fast because the RocksDB files are persisted and can be reused without replaying the changelog",
        "The state is lost and must be rebuilt from scratch"
      ],
      "correct": 2,
      "explanation": "If the Kafka Streams app restarts on the same machine, the local RocksDB state store files are already on disk. The application loads them directly — no changelog replay needed. Only events that arrived after the last checkpoint need replaying. This makes restarts fast (seconds, not minutes). If restarting on a different machine, the local files aren't available and the full changelog must be replayed (or loaded from a standby replica).",
      "detailedExplanation": "This prompt is really about \"team's Kafka Streams application checkpoints state to a local RocksDB store\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 50 GB in aligned units before selecting an answer. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-032",
      "type": "multiple-choice",
      "question": "A team uses Kafka consumer groups for stream processing. They occasionally need to reprocess all events from the past 24 hours (e.g., after deploying a bugfix). What is the procedure?",
      "options": [
        "Create a new topic and copy the events",
        "Ask producers to republish the events",
        "Delete and recreate the topic",
        "Reset the consumer group's offsets to the desired timestamp using kafka-consumer-groups --reset-offsets, then restart the consumers"
      ],
      "correct": 3,
      "explanation": "Kafka's offset management supports seeking to a timestamp: kafka-consumer-groups --reset-offsets --to-datetime can set offsets for a consumer group to a specific point. On restart, consumers read from the reset position, reprocessing events. This requires: (1) topic retention covers the 24-hour window, (2) consumers are idempotent or state is cleared before reprocessing, (3) downstream systems can handle re-emitted results.",
      "detailedExplanation": "If you keep \"team uses Kafka consumer groups for stream processing\" in view, the correct answer separates faster. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. If values like 24 hours and 1 appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-033",
      "type": "multiple-choice",
      "question": "A stream processing job reads from a high-throughput topic (100,000 msg/s). The job performs a costly enrichment (10ms per message) that calls an external API. At 10ms per message, a single worker processes only 100 msg/s. The topic has 16 partitions. What is the minimum number of worker instances needed to keep up?",
      "options": [
        "16 workers (one per partition)",
        "1,000 workers",
        "10 workers with 10x batch size",
        "1,000 workers — but Kafka limits to 16 (one per partition), so the team needs to repartition to at least 1,000 partitions or process asynchronously within each consumer"
      ],
      "correct": 3,
      "explanation": "Required throughput: 100,000 msg/s. Per-worker throughput: 100 msg/s (limited by the 10ms API call). Workers needed: 100,000 / 100 = 1,000. But Kafka limits consumers to partition count (16). Solutions: (1) Increase partitions to 1,000+. (2) Use async processing within each consumer (thread pool, non-blocking I/O) to achieve 1,000/16 ≈ 63x throughput per consumer. (3) Batch API calls. Option 2 is most practical — creating 1,000 partitions is excessive for this scenario.",
      "detailedExplanation": "The core signal here is \"stream processing job reads from a high-throughput topic (100,000 msg/s)\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 100,000 and 10ms in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-034",
      "type": "multiple-choice",
      "question": "A team streams financial transactions and needs to detect fraud patterns: 'more than 5 transactions exceeding $1,000 from the same account within 10 minutes.' This requires stateful pattern matching across multiple events. What stream processing capability is this?",
      "options": [
        "Complex event processing (CEP) — detecting multi-event patterns with temporal and conditional constraints over a stream",
        "Batch anomaly detection",
        "Simple filtering (stateless)",
        "Database trigger-based alerting"
      ],
      "correct": 0,
      "explanation": "Complex event processing (CEP) detects patterns across sequences of events. Unlike simple filtering (single event), CEP maintains state across events: tracking per-account transaction counts within a sliding 10-minute window, checking both the count threshold (>5) and the amount threshold (>$1,000). Frameworks like Flink CEP, Esper, and Siddhi provide pattern-matching DSLs for expressing these rules declaratively over event streams.",
      "detailedExplanation": "The key clue in this question is \"team streams financial transactions and needs to detect fraud patterns: 'more than 5\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 5 and 1,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-035",
      "type": "multiple-choice",
      "question": "A stream processor emits results to a Kafka output topic. Downstream consumers read this topic. The processor uses at-least-once processing, so it may emit duplicate results during failure recovery. What must downstream consumers handle?",
      "options": [
        "Missing results — at-least-once sometimes loses messages",
        "Out-of-order results — at-least-once reorders messages",
        "Duplicate results — consumers must be idempotent or deduplicate to avoid counting results twice",
        "Nothing — duplicates don't affect downstream systems"
      ],
      "correct": 2,
      "explanation": "At-least-once means the processor may emit the same result more than once (after a crash-recovery reprocess). Downstream consumers must handle this: (1) Idempotent writes (upsert by result key — writing the same result twice is harmless). (2) Deduplication (track result IDs, skip already-seen results). Without this, duplicates compound: if the processor duplicates a +$100 revenue event, the downstream revenue total is inflated by $100.",
      "detailedExplanation": "Start from \"stream processor emits results to a Kafka output topic\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "A Kafka consumer group with 6 consumers is processing smoothly. One consumer crashes. What happens during the rebalance?",
          "options": [
            "The crashed consumer's partitions are abandoned until it recovers",
            "All consumption pauses briefly while Kafka reassigns the crashed consumer's partitions to the remaining 5 consumers. This 'stop-the-world' rebalance affects all group members",
            "A new consumer is spawned automatically",
            "The remaining 5 consumers continue with their existing partitions unchanged"
          ],
          "correct": 1,
          "explanation": "Kafka's default rebalance protocol (eager rebalance): all consumers stop processing, release their partition assignments, and receive new assignments. The crashed consumer's partitions are distributed among the 5 survivors. During rebalance (typically a few seconds to a minute), no messages are processed by any consumer in the group. This is the 'rebalance storm' problem — frequent crashes or deployments cause repeated pauses.",
          "detailedExplanation": "Use \"kafka consumer group with 6 consumers is processing smoothly\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 6 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "The team's rolling deployments trigger a rebalance for every instance restart (6 restarts = 6 rebalances). Each rebalance pauses all consumers for 10 seconds. Total disruption: 60 seconds. What Kafka feature reduces this impact?",
          "options": [
            "Cooperative (incremental) rebalancing — instead of revoking all partitions, only the partitions that need to move are reassigned, and other consumers continue processing uninterrupted",
            "Use a single consumer instead of a group",
            "Increase the session timeout to avoid detecting the restart as a crash",
            "Disable rebalancing entirely during deployments"
          ],
          "correct": 0,
          "explanation": "Cooperative rebalancing (introduced in Kafka 2.4): instead of stop-the-world, only the partitions that actually need to move are revoked and reassigned. Other consumers keep processing their existing partitions. For a rolling deployment, only the restarting instance's partitions are redistributed — the other 5 consumers are barely affected. This reduces total disruption from 60 seconds to roughly 10 seconds.",
          "detailedExplanation": "The core signal here is \"team's rolling deployments trigger a rebalance for every instance restart (6 restarts =\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 6 and 10 seconds in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The decision turns on \"stream Processing\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team wants to build a real-time materialized view of 'total revenue per product category' from an order event stream. The view should always reflect the latest state. In stream processing terms, what is this view?",
          "options": [
            "A table — an ever-updating materialized view derived from the stream, where each category's total is continuously updated as new order events arrive",
            "A snapshot taken every 5 minutes",
            "A batch query scheduled hourly",
            "A windowed aggregation with tumbling windows"
          ],
          "correct": 0,
          "explanation": "The stream-table duality: a stream of events can be materialized into a table (current state per key). The 'total revenue per category' table is continuously updated — each order event increments the relevant category's total. This is a KTable in Kafka Streams: the stream is the changelog, the table is the materialized view. Unlike windowed aggregations (which produce a result per time window), tables produce a continuously evolving current state.",
          "detailedExplanation": "Read this as a scenario about \"team wants to build a real-time materialized view of 'total revenue per product\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        },
        {
          "question": "The team's materialized table can also be thought of as a stream. If they publish every change to the revenue-per-category table as events, they create a changelog stream. This bidirectional relationship — streams can become tables, tables can become streams — is called what?",
          "options": [
            "The stream-table duality — a table is the current state derived from a stream (by aggregation), and a stream is the changelog of a table (by capturing updates)",
            "Materialized view maintenance",
            "Event sourcing",
            "Log-structured storage"
          ],
          "correct": 0,
          "explanation": "Stream-table duality: every stream can be materialized into a table (apply events to get current state), and every table's changes can be captured as a stream (changelog). In Kafka: a KStream is the event stream, a KTable is the materialized state, and a compacted topic is the stored changelog. This duality is foundational to stream processing — it connects event-driven and state-driven views of the same data.",
          "detailedExplanation": "The key clue in this question is \"team's materialized table can also be thought of as a stream\". Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"stream Processing\". Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processor computes fraud scores in real-time. During a deployment, the old version must stop and the new version must start without losing any events or processing them twice. What deployment strategy achieves this?",
          "options": [
            "Run both versions permanently",
            "Stop the old version, then start the new version (gap in processing)",
            "Blue-green with Kafka consumer groups: start the new version in a separate group, verify it works, then stop the old group. Use a shared output topic with idempotent writes to handle the brief overlap",
            "Pause the topic during deployment"
          ],
          "correct": 2,
          "explanation": "Blue-green stream deployment: (1) Start the new version in a new consumer group, reading from the same input topic. (2) Both versions process events simultaneously during the overlap. (3) Output uses idempotent writes (keyed by event ID) so duplicates from both versions are harmless. (4) Once the new version is verified, stop the old group. The brief double-processing costs compute but ensures zero data loss and enables instant rollback.",
          "detailedExplanation": "The decision turns on \"stream processor computes fraud scores in real-time\". Solve this as chained reasoning where stage two must respect stage one assumptions. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "During the blue-green overlap, both the old and new consumer groups process the same events. The new version writes to the output topic with a different schema (added a 'fraud_category' field). How should downstream consumers handle receiving both old-schema and new-schema results during the overlap?",
          "options": [
            "Use a schema registry to block incompatible schemas",
            "The old version should be stopped before the new schema appears",
            "Write old and new results to separate topics",
            "Downstream consumers must support both schemas simultaneously, ignoring unknown fields from the new version and handling missing fields from the old version"
          ],
          "correct": 3,
          "explanation": "During blue-green overlap, both schemas appear on the output topic. Downstream consumers must be tolerant: ignore unknown fields (forward compatibility) and use defaults for missing fields (backward compatibility). This is why additive-only schema evolution is critical — adding optional fields is safe, but removing or renaming fields would break consumers reading old-format messages. The overlap is brief, but consumers must handle the mixed-schema window.",
          "detailedExplanation": "Start from \"during the blue-green overlap, both the old and new consumer groups process the same\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "Use \"stream Processing\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-039",
      "type": "multi-select",
      "question": "Which are characteristics of log compaction in Kafka? (Select all that apply)",
      "options": [
        "For non-deleted keys, compaction ensures the latest value remains available in the topic",
        "Retains only the latest record per key, deleting older records with the same key",
        "Preserves the complete event history for all keys, ordered by timestamp",
        "Supports tombstone records (value=null) to indicate key deletion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Log compaction: (1) Keeps the latest record per key — older records with the same key are eventually removed. (2) Guarantees the latest value per key survives indefinitely (no time-based expiry). (3) Does NOT preserve complete history — that's what regular time-based retention does. (4) Tombstones (null value) mark a key as deleted; after compaction, both the tombstone and all prior records for that key are removed. Compacted topics are ideal for materializing tables/state stores.",
      "detailedExplanation": "This prompt is really about \"characteristics of log compaction in Kafka? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-040",
      "type": "multi-select",
      "question": "A stream processing job needs to enrich order events with customer data. Which approaches work? (Select all that apply)",
      "options": [
        "Preload: load all customer data into the processor's memory at startup, refresh periodically",
        "Stream-table join: maintain a KTable of customer data from a compacted topic, join each order event with the table",
        "Partition orders by order_id and customer data by customer_id, then do a local partition-wise join without repartitioning",
        "Broadcast join: publish the customer dataset to all processor instances via a broadcast channel"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Valid approaches include: preloading small/slow-changing reference data, stream-table joins via a compacted customer topic, and broadcast joins in frameworks that support them. The invalid option is joining two differently partitioned streams locally without repartitioning/co-partitioning on the join key; records for the same customer will land on different partitions and miss matches.",
      "detailedExplanation": "This prompt is really about \"stream processing job needs to enrich order events with customer data\". Validate each option independently; do not select statements that are only partially true. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-041",
      "type": "multi-select",
      "question": "Which are challenges of stream-stream joins compared to stream-table joins? (Select all that apply)",
      "options": [
        "Stream-stream joins are always faster than stream-table joins",
        "Out-of-order events may cause a match to be missed if the matching event arrives after the window expires",
        "The join must buffer events from both streams, consuming more memory than a table lookup",
        "Both sides are unbounded, requiring a time window to limit how far back to look for matches"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Stream-stream joins face unique challenges: (1) No natural boundary — without a window, you'd buffer events forever. (2) State is proportional to window size × both streams' throughput. (3) Late events may miss their window — an impression and click 35 minutes apart won't match in a 30-minute window. Stream-table joins are simpler because the table side is bounded (one entry per key) and always available for lookup. Stream-stream joins are NOT faster — they require more state and computation.",
      "detailedExplanation": "Use \"challenges of stream-stream joins compared to stream-table joins? (Select all that\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-042",
      "type": "numeric-input",
      "question": "A Kafka topic has 32 partitions, each receiving events at 1,000 msg/s. The consumer group has 8 consumers. Each consumer is assigned 4 partitions. What is the per-consumer throughput requirement?",
      "answer": 4000,
      "unit": "msg/s",
      "tolerance": "exact",
      "explanation": "32 partitions / 8 consumers = 4 partitions per consumer. Each partition produces 1,000 msg/s, so each consumer handles 4 × 1,000 = 4,000 msg/s. If a consumer can only process 3,000 msg/s, it falls behind at 1,000 msg/s — accumulating 60,000 unprocessed messages per minute. Capacity planning must account for per-partition throughput × partitions-per-consumer.",
      "detailedExplanation": "The core signal here is \"kafka topic has 32 partitions, each receiving events at 1,000 msg/s\". Keep every transformation in one unit system and check order of magnitude at the end. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 32 and 1,000 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-043",
      "type": "numeric-input",
      "question": "A stream processor uses a 30-minute session window with a gap timeout of 10 minutes. User A has events at 10:00, 10:05, 10:08, 10:12, and 10:30. How many sessions does User A have?",
      "answer": 2,
      "unit": "sessions",
      "tolerance": "exact",
      "explanation": "Session windows merge events that are within the gap timeout (10 minutes) of each other. Events: 10:00 → 10:05 (gap 5min, merged) → 10:08 (gap 3min, merged) → 10:12 (gap 4min, merged). Gap from 10:12 to 10:30 is 18 minutes — exceeds the 10-minute timeout. Session 1: [10:00-10:12], Session 2: [10:30-10:30]. Result: 2 sessions. The 30-minute window size is a maximum — the session can be shorter if the gap timeout triggers first.",
      "detailedExplanation": "If you keep \"stream processor uses a 30-minute session window with a gap timeout of 10 minutes\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 30 and 10 minutes appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-044",
      "type": "numeric-input",
      "question": "A stream processing job uses event-time windowing with a watermark that's 5 minutes behind the latest event time. The latest event received has event_time 14:23. A window for 14:00-14:15 is waiting to be emitted. Has the watermark advanced past the window's end (14:15)?",
      "answer": 1,
      "unit": "(1=yes, 0=no)",
      "tolerance": "exact",
      "explanation": "Watermark = latest event_time - allowed lateness = 14:23 - 5 minutes = 14:18. The window end is 14:15. Since watermark (14:18) > window end (14:15), the watermark has passed the window. The processor can emit the window's result. Events with event_time before 14:18 that haven't arrived yet are considered late. The 5-minute watermark delay allows events up to 5 minutes late to be included in their correct window.",
      "detailedExplanation": "Start from \"stream processing job uses event-time windowing with a watermark that's 5 minutes\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 5 minutes and 14 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-045",
      "type": "numeric-input",
      "question": "A Kafka cluster has a topic with 3 partitions and a replication factor of 3. Each message is 1 KB. Producers write 10,000 messages/second. What is the total write throughput across all broker replicas in KB/s?",
      "answer": 30000,
      "unit": "KB/s",
      "tolerance": "exact",
      "explanation": "Each message is written to 3 replicas (replication factor 3). Total writes: 10,000 msg/s × 1 KB × 3 replicas = 30,000 KB/s (≈30 MB/s). The replication factor multiplies write I/O across the cluster. This is why high replication factors with high throughput require significant disk and network capacity. The producer sees 10,000 writes/s, but the cluster handles 30,000 writes/s internally.",
      "detailedExplanation": "The key clue in this question is \"kafka cluster has a topic with 3 partitions and a replication factor of 3\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 3 and 1 KB should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-046",
      "type": "ordering",
      "question": "Rank these stream processing guarantees from weakest to strongest:",
      "items": [
        "At-most-once — process each event zero or one time (may lose events on failure)",
        "At-least-once — process each event one or more times (may duplicate on failure)",
        "Exactly-once — process each event exactly one time (no loss, no duplicates)"
      ],
      "correctOrder": [0, 1, 2],
      "explanation": "At-most-once: commit offset before processing. If processing fails, the event is skipped. At-least-once: process first, commit after. If commit fails, the event is reprocessed. Exactly-once: atomic processing + commit (Kafka transactions). Each level adds complexity and overhead: exactly-once requires transaction coordination, which reduces throughput by 10-20% compared to at-least-once.",
      "detailedExplanation": "Read this as a scenario about \"rank these stream processing guarantees from weakest to strongest:\". Order by relative scale and bottleneck effect, then validate neighboring items. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 10 and 20 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-047",
      "type": "ordering",
      "question": "Rank these Kafka consumer lag scenarios from least concerning to most concerning:",
      "items": [
        "Lag of 100 messages — consumer is slightly behind, typical during brief processing spikes",
        "Lag of 50,000 messages — consumer is minutes behind, usually recoverable by processing faster than production rate",
        "Lag growing steadily at 1,000 msg/s — consumer can't keep up, will never catch up without intervention",
        "Lag approaching topic retention limit — oldest unconsumed messages are about to be deleted, causing data loss"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Small, stable lag is normal (processing isn't instant). Large but stable lag means the consumer fell behind but can catch up. Growing lag means the consumer is permanently slower than the producer — it will never catch up and lag will grow indefinitely. Lag approaching retention limit is critical: once messages age past the retention window, they're deleted. If the consumer hasn't processed them, they're permanently lost.",
      "detailedExplanation": "The decision turns on \"rank these Kafka consumer lag scenarios from least concerning to most concerning:\". Order by relative scale and bottleneck effect, then validate neighboring items. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-048",
      "type": "ordering",
      "question": "Rank these steps in the correct order for deploying a new stream processing job that reads from an existing Kafka topic:",
      "items": [
        "Configure the consumer group with auto.offset.reset=earliest (if backfill needed) or latest (real-time only)",
        "Deploy the stream processing job and verify it processes events correctly",
        "Monitor consumer lag and processing throughput to ensure the job keeps up",
        "Set up alerting on consumer lag thresholds and error rates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Deployment sequence: (1) Configure where to start reading — this determines whether you backfill history or start fresh. (2) Deploy and verify correctness — check output for expected results on known inputs. (3) Monitor performance — consumer lag and throughput reveal whether the job can sustain production load. (4) Alerting — set thresholds so you're notified before lag becomes critical. Skipping step 3 means you won't know the job is falling behind until users report stale data.",
      "detailedExplanation": "This prompt is really about \"rank these steps in the correct order for deploying a new stream processing job that\". Build the rank from biggest differences first, then refine with adjacent checks. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-049",
      "type": "ordering",
      "question": "Rank these approaches for handling a hot partition (one partition receiving disproportionate traffic) from simplest to most involved:",
      "items": [
        "Accept the imbalance — if the consumer handling the hot partition can keep up, no action needed",
        "Add more partitions to distribute the hot key's events (requires changing the partitioning logic)",
        "Use a composite partition key (original key + random suffix) to spread the hot key across multiple partitions",
        "Implement a two-stage pipeline: spread with composite key first, then repartition by original key for aggregation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Simplest first: accept it (if perf is acceptable). Adding partitions redistributes load if the hot key hashes to different partitions (not guaranteed). Composite keys spread the hot key deterministically across N partitions. Two-stage processing handles the aggregation requirement that composite keys break (need all events for a key in one place). Each approach adds complexity but handles increasingly severe skew.",
      "detailedExplanation": "Use \"rank these approaches for handling a hot partition (one partition receiving\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-050",
      "type": "multiple-choice",
      "question": "A stream processor reads events from Kafka, enriches them by calling an external HTTP API (average 50ms latency), and writes results back to Kafka. The single-threaded consumer processes 20 events/second. The target is 500 events/second. What architectural change achieves this without adding partitions?",
      "options": [
        "Compress the events to reduce processing time",
        "Use async I/O: issue multiple HTTP calls concurrently (e.g., 25 in-flight requests), decoupling Kafka consumption from the blocking API call",
        "Increase the Kafka batch size",
        "Use a faster network connection to the API"
      ],
      "correct": 1,
      "explanation": "The bottleneck is the 50ms synchronous HTTP call — the consumer blocks while waiting for each response. At 50ms/event, max throughput = 1/0.05 = 20 events/s. With async I/O (25 concurrent requests): 25 × 20 = 500 events/s. The consumer issues 25 HTTP calls in parallel, collects responses, and writes results in batches. This is the standard pattern for I/O-bound stream processing: decouple consumption from blocking external calls.",
      "detailedExplanation": "If you keep \"stream processor reads events from Kafka, enriches them by calling an external HTTP API\" in view, the correct answer separates faster. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. If values like 50ms and 20 appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-051",
      "type": "multiple-choice",
      "question": "A team's stream processing pipeline has three stages: (1) parse JSON, (2) enrich with user data, (3) write to database. Stage 2 is the bottleneck (slow API call). They want to scale stage 2 independently without scaling stages 1 and 3. How can they decouple the stages?",
      "options": [
        "Separate each stage into its own consumer group with intermediate Kafka topics between stages — stage 1 writes parsed events to topic A, stage 2 reads from A and writes enriched events to topic B, stage 3 reads from B",
        "Run all three stages in a single process and scale the whole pipeline",
        "Cache all user data to eliminate the API call",
        "Use a larger Kafka batch size for stage 2"
      ],
      "correct": 0,
      "explanation": "Intermediate topics between stages enable independent scaling. Stage 2 (the bottleneck) can be scaled to 20 consumers while stages 1 and 3 remain at 4 consumers each. Each stage reads from one topic and writes to the next. The intermediate topics serve as buffers — stage 1 can produce faster than stage 2 consumes, with the topic absorbing the difference. This is the microservices approach to stream processing pipelines.",
      "detailedExplanation": "The core signal here is \"team's stream processing pipeline has three stages: (1) parse JSON, (2) enrich with\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "A Kafka producer sends events to a topic with 8 partitions. The producer doesn't specify a key for some events. How does Kafka assign these keyless events to partitions?",
          "options": [
            "Round-robin (or sticky partitioning) — keyless events are distributed across partitions for even load, but without ordering guarantees relative to other keyless events",
            "Kafka rejects events without keys",
            "The consumer chooses which partition to read from",
            "All keyless events go to partition 0"
          ],
          "correct": 0,
          "explanation": "Keyless events: Kafka's default partitioner distributes them across partitions. In older clients, this was round-robin (each event to a different partition). In newer clients (Kafka 2.4+), 'sticky partitioning' batches events to the same partition until a batch is full, then switches — better throughput with similar distribution. Since there's no key, there's no ordering guarantee across partitions. For keyed events, hash(key) % num_partitions ensures key-based partitioning.",
          "detailedExplanation": "The decision turns on \"kafka producer sends events to a topic with 8 partitions\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 8 and 2.4 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic."
        },
        {
          "question": "The producer starts specifying user_id as the partition key. An event with user_id='alice' is assigned to partition 5 via hash('alice') % 8 = 5. The team later increases partitions from 8 to 16. Where does the next event for user_id='alice' go?",
          "options": [
            "Still partition 5 — Kafka maintains key-to-partition mappings",
            "The new partition 13 (8 + 5)",
            "Kafka assigns it randomly after a partition change",
            "hash('alice') % 16, which may be a different partition — adding partitions changes the hash-to-partition mapping, potentially breaking per-key ordering during the transition"
          ],
          "correct": 3,
          "explanation": "Kafka uses hash(key) % num_partitions for key-based routing. Changing num_partitions changes the modulo result, potentially moving keys to different partitions. alice's events that were on partition 5 may now route to partition 13 (hash('alice') % 16). During the transition, some events for the same key end up on different partitions, breaking per-key ordering. This is why partition count changes are disruptive — they're best avoided for keyed topics with ordering requirements.",
          "detailedExplanation": "Start from \"producer starts specifying user_id as the partition key\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 5 and 8 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "Use \"stream Processing\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team processes IoT sensor data from 10,000 devices. Each device sends 1 reading/second (10,000 events/s total). They partition the topic by device_id. How many partitions should they create?",
          "options": [
            "Based on target consumer parallelism and per-consumer throughput — if each consumer handles 2,500 events/s, they need 4 partitions (10,000 / 2,500). More partitions enable finer scaling",
            "1 partition for simplicity",
            "Use the Kafka default (1 partition)",
            "10,000 partitions — one per device"
          ],
          "correct": 0,
          "explanation": "Partition count should be driven by throughput needs and desired parallelism, not entity count. 10,000 partitions (one per device) wastes resources — each partition has metadata overhead. Instead: target throughput / per-consumer throughput = minimum partitions. At 10,000 events/s with 2,500/s per consumer, 4 partitions suffice. Over-provisioning slightly (e.g., 8 or 16) allows headroom for growth. Kafka's default of 1 partition severely limits parallelism.",
          "detailedExplanation": "Start from \"team processes IoT sensor data from 10,000 devices\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 10,000 and 1 in aligned units before selecting an answer. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "The team starts with 8 partitions. Six months later, device count grows to 100,000 (100,000 events/s). They need to scale from 8 to 32 partitions. But they use device_id as the partition key, and some consumers maintain per-device state. What complication does the partition increase cause?",
          "options": [
            "No complication — Kafka handles everything automatically",
            "The topic must be deleted and recreated",
            "Events for the same device_id may now route to a different partition (hash(device_id) % 32 ≠ hash(device_id) % 8). Consumers' local state for those devices becomes stranded on the wrong partition",
            "Producers must be reconfigured manually for each device"
          ],
          "correct": 2,
          "explanation": "Changing partition count remaps keys. A device on partition 3 (hash(device_id) % 8 = 3) may move to partition 19 (hash(device_id) % 32 = 19). The consumer that had partition 3's state for that device no longer receives its events. Solutions: transfer state during rebalance (complex), rebuild state from the changelog, or use a lookup layer that routes state queries by device_id regardless of partition assignment.",
          "detailedExplanation": "The decision turns on \"team starts with 8 partitions\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 8 and 100,000 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-054",
      "type": "multi-select",
      "question": "Which are consequences of setting Kafka's message retention period too short? (Select all that apply)",
      "options": [
        "Increased storage costs from keeping too many messages",
        "Consumers that fall behind may lose messages — events are deleted before they're consumed",
        "Stream processing replay for bug fixes is limited to the retention window",
        "New consumer groups that need historical data (auto.offset.reset=earliest) won't find old events"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Short retention means: (1) Slow consumers risk data loss — if they fall behind further than the retention window, oldest messages are deleted. (2) New consumers can't backfill beyond the retention window. (3) Replay for reprocessing is limited to what's retained. Increased storage is a consequence of too LONG retention, not too short. Setting retention involves balancing storage costs against replay needs, consumer lag tolerance, and new-consumer backfill requirements.",
      "detailedExplanation": "The decision turns on \"consequences of setting Kafka's message retention period too short? (Select all that\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-055",
      "type": "multi-select",
      "question": "Which are benefits of using intermediate Kafka topics between stream processing stages (vs. a monolithic single-process pipeline)? (Select all that apply)",
      "options": [
        "Each stage can be scaled independently based on its throughput needs",
        "Stages can be written in different languages or frameworks",
        "Lower end-to-end latency because events skip the broker between stages",
        "The intermediate topic acts as a buffer, absorbing temporary throughput mismatches between stages"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Intermediate topics provide: (1) Independent scaling — scale the bottleneck stage without touching others. (2) Buffering — if stage 2 slows down, the intermediate topic absorbs the backlog from stage 1. (3) Technology flexibility — each stage is a separate consumer/producer and can use any language. End-to-end latency is actually HIGHER with intermediate topics (each hop adds broker write/read latency), not lower. The benefits are operational flexibility, not raw performance.",
      "detailedExplanation": "Read this as a scenario about \"benefits of using intermediate Kafka topics between stream processing stages (vs\". Validate each option independently; do not select statements that are only partially true. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-056",
      "type": "multiple-choice",
      "question": "A stream processor computes 'unique visitors per hour' using a tumbling window. A user visits 10 times during the 9:00-10:00 window. The processor must count them once, not 10 times. What data structure efficiently tracks uniqueness within the window?",
      "options": [
        "A hash map of URL to visit count",
        "A HyperLogLog or set that deduplicates user IDs within the window, counting only distinct visitors",
        "A simple counter that increments for each event",
        "A sorted list of all events"
      ],
      "correct": 1,
      "explanation": "Exact uniqueness: maintain a set of user IDs per window. Each event adds the user_id to the set; the count is the set's cardinality. For high-cardinality data (millions of users), exact sets consume too much memory. HyperLogLog provides approximate cardinality with ~2% error using only ~12 KB of memory regardless of cardinality. The choice depends on accuracy requirements vs. memory budget.",
      "detailedExplanation": "The key clue in this question is \"stream processor computes 'unique visitors per hour' using a tumbling window\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 10 and 9 appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-057",
      "type": "multiple-choice",
      "question": "A team processes a Kafka topic with 16 partitions using Flink with parallelism=16. Each Flink task reads one partition. The team wants to add a second Flink job that reads the same topic independently (for a different computation). What happens?",
      "options": [
        "The 16 partitions are split between the two jobs (8 each)",
        "The two jobs conflict — only one can read the topic",
        "Each job uses a different consumer group, so both read all 16 partitions independently. Kafka delivers every event to both jobs",
        "Kafka queues events alternately between the two jobs"
      ],
      "correct": 2,
      "explanation": "Different consumer groups are independent. Kafka delivers every event to every consumer group. Job A (group=job-a) reads all 16 partitions; Job B (group=job-b) also reads all 16 partitions. This is Kafka's pub/sub model: topics support multiple independent consumers via groups. Within a group, partitions are divided among members. Across groups, every group gets all messages.",
      "detailedExplanation": "Start from \"team processes a Kafka topic with 16 partitions using Flink with parallelism=16\", then pressure-test the result against the options. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 16 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-058",
      "type": "multiple-choice",
      "question": "A Kafka producer sends events with acks=all and min.insync.replicas=2 to a topic with replication factor 3. One of the 3 brokers hosting a partition goes down. Can the producer still write to that partition?",
      "options": [
        "Yes, but without any durability guarantee",
        "Yes — 2 of 3 replicas are still in-sync, meeting the min.insync.replicas=2 requirement. The producer can write and receive acknowledgment",
        "No — all 3 replicas must be available for acks=all",
        "The producer switches to a different partition automatically"
      ],
      "correct": 1,
      "explanation": "acks=all means the producer waits for all in-sync replicas (ISR) to acknowledge. With min.insync.replicas=2, at least 2 replicas must be in the ISR for the partition to accept writes. With 1 of 3 brokers down, 2 replicas are in the ISR — the write succeeds. If a second broker goes down, ISR drops to 1 (below the minimum of 2), and the partition becomes read-only until a replica catches up.",
      "detailedExplanation": "If you keep \"kafka producer sends events with acks=all and min\" in view, the correct answer separates faster. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 2 and 3 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-059",
      "type": "multiple-choice",
      "question": "A stream processing job has a checkpoint interval of 60 seconds. It uses exactly-once processing with Flink's checkpointing. If the job fails and restarts, it rolls back to the last checkpoint. What is the maximum amount of work that must be reprocessed?",
      "options": [
        "Zero events — exactly-once means nothing is ever reprocessed",
        "Only the single event that caused the failure",
        "All events since the job started",
        "Up to 60 seconds of events — the work since the last checkpoint, which is then replayed with exactly-once guarantees"
      ],
      "correct": 3,
      "explanation": "Checkpoints capture consistent snapshots of the job's state and input positions. On failure, the job restores from the last checkpoint and replays events from that point. With a 60-second interval, up to 60 seconds of events are replayed. Exactly-once semantics ensure the replay produces the same output as the original processing (no duplicates externally). Shorter checkpoint intervals reduce replay volume but increase checkpointing overhead.",
      "detailedExplanation": "The core signal here is \"stream processing job has a checkpoint interval of 60 seconds\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 60 seconds and 60 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-060",
      "type": "multiple-choice",
      "question": "A team's Kafka consumer reads messages in batches of 500 (max.poll.records=500). Processing each batch takes 45 seconds. max.poll.interval.ms is set to 30 seconds. What happens?",
      "options": [
        "The broker waits for the consumer to finish",
        "The batch is automatically split into smaller pieces",
        "The consumer is kicked from the group — processing exceeded max.poll.interval.ms, so Kafka treats the consumer as failed and triggers a rebalance",
        "The batch is processed successfully with no side effects"
      ],
      "correct": 2,
      "explanation": "If the app does not call poll() within max.poll.interval.ms, Kafka considers it stuck and removes it from the group, triggering a rebalance. Here, processing takes 45s but max.poll.interval.ms is 30s, so the consumer is revoked before finishing. Typical fixes: increase max.poll.interval.ms, reduce batch size/processing time, or move long-running work off the poll loop.",
      "detailedExplanation": "The core signal here is \"team's Kafka consumer reads messages in batches of 500 (max\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 500 and 45 seconds appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-061",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team runs a Flink job that processes 500,000 events/second with exactly-once checkpointing. Checkpoints take 5 seconds to complete (snapshot state, write to S3). They want to reduce the checkpoint interval from 60 seconds to 10 seconds. What risk does this introduce?",
          "options": [
            "Shorter intervals improve throughput",
            "Exactly-once semantics break with shorter intervals",
            "Checkpoint overhead may consume significant processing capacity — with 5-second checkpoints every 10 seconds, the job spends 50% of its time checkpointing, reducing effective throughput",
            "Checkpoints don't affect processing"
          ],
          "correct": 2,
          "explanation": "Checkpointing isn't free — it consumes I/O, CPU, and network resources. With 5-second checkpoints every 10 seconds: the job is checkpointing 50% of the time. Even with asynchronous checkpointing (Flink's default), the state snapshot phase involves copying state. At high throughput (500K events/s), this is substantial. The trade-off: shorter intervals reduce recovery replay (max 10s vs 60s of reprocessing) but increase steady-state overhead.",
          "detailedExplanation": "The key clue in this question is \"team runs a Flink job that processes 500,000 events/second with exactly-once\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 500,000 and 5 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "The team keeps the 60-second checkpoint interval. After a failure, the job replays 55 seconds of events. Some of these events triggered external HTTP API calls (sending notifications). During replay, those notifications would be sent again. How should the job handle this?",
          "options": [
            "Use a side-effect buffer: during replay, suppress external calls. After replay catches up to the failure point, resume normal processing including external calls. Alternatively, make the external API calls idempotent",
            "Disable exactly-once for stages with external calls",
            "Accept duplicate notifications — they're harmless",
            "Skip all external calls during replay"
          ],
          "correct": 0,
          "explanation": "Exactly-once semantics in Flink apply to state and Kafka outputs, NOT to external system calls. During replay, the job re-executes all logic including HTTP calls. Options: (1) Make external calls idempotent (include a unique ID so the notification service deduplicates). (2) Use a sink that buffers outputs and flushes atomically with checkpoints. (3) Track which calls were already made in the checkpoint state. External side effects are the hardest part of exactly-once processing.",
          "detailedExplanation": "Read this as a scenario about \"team keeps the 60-second checkpoint interval\". Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Numbers such as 60 and 55 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation."
        }
      ],
      "detailedExplanation": "If you keep \"stream Processing\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-062",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team processes a stream of e-commerce events. They need to detect 'abandoned carts': users who add items to a cart but don't purchase within 30 minutes. This requires tracking the absence of an event (no purchase) within a time window after a positive event (cart addition). What stream processing capability handles this?",
          "options": [
            "Temporal pattern matching with a timeout — detect 'event A occurred but event B did NOT occur within N minutes'",
            "A stream-table join with a purchase table",
            "A simple filter on cart events",
            "A tumbling window aggregation"
          ],
          "correct": 0,
          "explanation": "Detecting non-events requires temporal pattern matching: 'cart_add event for user X, followed by NOT purchase event for user X within 30 minutes.' When the 30-minute timer fires without a matching purchase, emit an 'abandoned cart' event. Frameworks like Flink CEP support this with 'followed by not within' patterns. This is more complex than simple filtering or aggregation because the trigger is the absence of an event, not its presence.",
          "detailedExplanation": "Start from \"team processes a stream of e-commerce events\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 30 minutes and 30 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic."
        },
        {
          "question": "The team implements the abandoned cart detector. A user adds items at 10:00 and purchases at 10:28 (within 30 minutes), but the purchase event is delayed by the network and arrives at 10:35 (event_time=10:28, processing_time=10:35). Using processing time, the detector fires the 30-minute timeout at 10:30 and emits a false 'abandoned cart.' How does event-time processing prevent this?",
          "options": [
            "Event-time processing uses the event timestamp (10:28) for the timer, recognizing the purchase arrived within the 30-minute window regardless of when the processor received it",
            "Event-time processing doesn't help with non-event detection",
            "Event-time processing always waits an extra hour for late events",
            "Event-time processing ignores late events entirely"
          ],
          "correct": 0,
          "explanation": "With event-time processing, the timer is based on event timestamps, not wall-clock time. The purchase event has event_time=10:28, which is within the 30-minute window starting at 10:00. Even though the event physically arrived at 10:35, the processor evaluates it against event time. The watermark mechanism ensures the processor waits for late events before firing the timeout. This eliminates the false positive caused by network delay.",
          "detailedExplanation": "The decision turns on \"team implements the abandoned cart detector\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 10 and 00 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-063",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team's Flink job processes events from 3 Kafka topics in parallel. Each topic has different event-time characteristics: Topic A events are near-real-time (< 1s delay), Topic B events arrive 5 minutes late on average, and Topic C events arrive up to 30 minutes late. How should watermark generation differ across sources?",
          "options": [
            "Ignore watermarks and use processing time",
            "Each source generates its own watermark based on its lateness characteristics. Flink takes the minimum watermark across all sources for operators that combine them",
            "Use the fastest source's watermark (1 second) for all",
            "Use a single global watermark that accommodates the slowest source (30 minutes)"
          ],
          "correct": 1,
          "explanation": "Per-source watermarks allow each source to advance at its own pace. Topic A's watermark advances rapidly (near-real-time), while Topic C's advances slowly (30-minute lag). When sources are combined (e.g., a join or union), Flink uses the minimum watermark across all inputs — this ensures no source's late events are prematurely dropped. A single 30-minute global watermark would unnecessarily delay Topic A's window results.",
          "detailedExplanation": "The decision turns on \"team's Flink job processes events from 3 Kafka topics in parallel\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 3 and 1s should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "Because Flink uses the minimum watermark across sources, Topic C (30 minutes late) holds back the combined watermark for any operator that reads all three topics. This means windows that only need Topic A and B data still wait 30 minutes. How can the team isolate the slow-watermark impact?",
          "options": [
            "Process Topic C in a separate Flink job or a separate subgraph that doesn't share operators with Topics A and B, so their watermarks don't affect each other",
            "Force Topic C's watermark to advance faster by dropping late events",
            "Remove Topic C from the pipeline",
            "Use processing time for Topic C only"
          ],
          "correct": 0,
          "explanation": "Watermark isolation: operators that don't need Topic C should not be connected to it in the Flink DAG. Process Topics A+B in one subgraph (fast watermarks) and Topic C separately (slow watermarks). Only combine results where necessary, downstream of the time-sensitive windows. This prevents a slow source from holding back the entire pipeline. Alternatively, side outputs can separate late data from on-time data within the same job.",
          "detailedExplanation": "Start from \"because Flink uses the minimum watermark across sources, Topic C (30 minutes late)\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 30 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Use \"stream Processing\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-064",
      "type": "multi-select",
      "question": "Which are valid uses of Kafka topic compaction? (Select all that apply)",
      "options": [
        "Materializing a database table as a Kafka topic (each row's latest state is retained by key)",
        "Retaining the complete, ordered event history for audit purposes",
        "Publishing configuration updates where only the latest config per key matters",
        "Backing up Kafka Streams state stores (the changelog topic uses compaction to retain only latest state per key)"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Compaction retains the latest value per key, which is useful for materializing current table-like state, distributing latest configuration values, and backing Kafka Streams state-store changelogs. It is not suitable for complete audit history because older records for the same key are eventually removed.",
      "detailedExplanation": "Read this as a scenario about \"valid uses of Kafka topic compaction? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-065",
      "type": "multi-select",
      "question": "A team is troubleshooting a Kafka Streams application that's processing slowly. Which metrics should they check first? (Select all that apply)",
      "options": [
        "State store size and checkpoint duration — large state can slow processing and checkpointing",
        "The producer's compression ratio",
        "Consumer lag — how far behind the consumer is from the latest offset in each partition",
        "Process rate — events processed per second per task"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Key metrics: (1) Consumer lag shows whether the app is keeping up — growing lag means it's falling behind. (2) Process rate identifies throughput bottlenecks per task. (3) State store size reveals memory pressure and checkpoint cost. The producer's compression ratio is irrelevant to the consumer's processing speed. Together, these metrics identify whether the bottleneck is CPU (low process rate), state (large store/slow checkpoint), or throughput capacity (growing lag).",
      "detailedExplanation": "The decision turns on \"team is troubleshooting a Kafka Streams application that's processing slowly\". Validate each option independently; do not select statements that are only partially true. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-066",
      "type": "multi-select",
      "question": "Which factors should influence the choice of Kafka partition count for a new topic? (Select all that apply)",
      "options": [
        "The number of Kafka brokers — partition count should scale with cluster size",
        "Expected key cardinality — too few partitions with high cardinality limits parallelism, too many with low cardinality causes empty partitions",
        "Target consumer throughput — more partitions enable more parallel consumers",
        "Desired maximum consumer group size — consumer count can't exceed partition count"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Partition count depends on: (1) Throughput needs — partitions = max consumers, which determines max throughput. (2) Key distribution — 10 partitions with 5 unique keys means some partitions are empty. (3) Consumer group sizing — if you'll need 20 consumers during peak, you need at least 20 partitions. Broker count is a cluster-level concern (partitions are distributed across brokers) but doesn't directly determine per-topic partition count.",
      "detailedExplanation": "Start from \"factors should influence the choice of Kafka partition count for a new topic? (Select\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-067",
      "type": "numeric-input",
      "question": "A stream processor uses sliding windows of 5 minutes with a 1-minute slide. Each event belongs to multiple overlapping windows. If the stream receives 20,000 events per minute, how many event-window assignments occur per minute?",
      "answer": 100000,
      "unit": "assignments",
      "tolerance": "exact",
      "explanation": "Each event belongs to window_size / slide = 5/1 = 5 windows. With 20,000 events per minute, total assignments: 20,000 × 5 = 100,000 per minute. Each assignment means the event's data is contributed to that window's aggregation state. This is why sliding windows with small slides are expensive: the multiplication factor (5x here) increases state writes proportionally.",
      "detailedExplanation": "The key clue in this question is \"stream processor uses sliding windows of 5 minutes with a 1-minute slide\". Keep every transformation in one unit system and check order of magnitude at the end. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 5 minutes and 1 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-068",
      "type": "numeric-input",
      "question": "A Kafka consumer group has 12 consumers reading from 12 partitions. The consumer processes 8,000 msg/s per instance. A rolling deployment restarts each consumer one at a time with a 30-second gap. During each restart, the remaining consumers absorb the extra partition. What is the peak per-consumer throughput during the deployment?",
      "answer": 8727,
      "unit": "msg/s",
      "tolerance": 0.05,
      "explanation": "Total throughput: 12 × 8,000 = 96,000 msg/s. During a restart, 11 consumers handle 12 partitions: 96,000 / 11 ≈ 8,727 msg/s per consumer. The extra ~727 msg/s (9% increase) must be within each consumer's capacity headroom. If consumers are already at 95% capacity, the extra 9% causes them to fall behind during deployment. This is why N-1 capacity planning is important for Kafka consumer groups.",
      "detailedExplanation": "The core signal here is \"kafka consumer group has 12 consumers reading from 12 partitions\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 12 and 8,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-069",
      "type": "numeric-input",
      "question": "A stream processor with event-time windowing has an allowed lateness of 10 minutes. The watermark is currently at 14:50. A tumbling window for 14:00-15:00 has not yet been emitted (watermark hasn't passed 15:00). An event with event_time 14:35 arrives. Is this event on-time or late?",
      "answer": 0,
      "unit": "(1=late, 0=on-time)",
      "tolerance": "exact",
      "explanation": "An event is late if the watermark has already passed its window's end. The 14:00-15:00 window ends at 15:00. The watermark is at 14:50 — it hasn't passed 15:00 yet. So the window is still open and accepting events. The event at 14:35 is assigned to the 14:00-15:00 window and is on-time. It would be late only if the watermark were past 15:00 (or past 15:10 with the 10-minute allowed lateness grace period).",
      "detailedExplanation": "If you keep \"stream processor with event-time windowing has an allowed lateness of 10 minutes\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 10 minutes and 14 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-070",
      "type": "numeric-input",
      "question": "A Kafka topic has a replication factor of 3 and 6 partitions. The cluster has 3 brokers. How many partition replicas does each broker host on average?",
      "answer": 6,
      "unit": "replicas",
      "tolerance": "exact",
      "explanation": "Total partition replicas: 6 partitions × 3 replicas = 18 replicas. Distributed across 3 brokers: 18 / 3 = 6 replicas per broker. Each broker hosts 6 partition replicas — some as leader (handling reads/writes) and some as followers (replicating from the leader). Kafka's partition assignment algorithm distributes leaders and followers evenly across brokers to balance load.",
      "detailedExplanation": "The key clue in this question is \"kafka topic has a replication factor of 3 and 6 partitions\". Normalize units before computing so conversion mistakes do not propagate. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 3 and 6 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-071",
      "type": "ordering",
      "question": "Rank these Kafka producer acknowledgment settings from fastest (lowest latency) to most durable:",
      "items": [
        "acks=0 — fire and forget, don't wait for any acknowledgment",
        "acks=1 — wait for the partition leader to acknowledge the write",
        "acks=all — wait for all in-sync replicas to acknowledge the write"
      ],
      "correctOrder": [0, 1, 2],
      "explanation": "acks=0: producer doesn't wait — maximum throughput, risk of data loss (leader crash loses the message). acks=1: waits for leader — moderate latency, data survives leader's disk write but not leader crash before replication. acks=all: waits for all ISR replicas — highest latency, data survives any single broker failure. Each level adds a round-trip to more brokers, increasing latency but improving durability.",
      "detailedExplanation": "Start from \"rank these Kafka producer acknowledgment settings from fastest (lowest latency) to most\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 0 and 1 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-072",
      "type": "ordering",
      "question": "Rank these stream processing scaling strategies from first to try to last resort:",
      "items": [
        "Optimize processing logic (reduce per-event CPU/IO cost)",
        "Increase consumer count (up to partition count)",
        "Increase partition count to enable more consumer parallelism",
        "Shard the workload across multiple independent Kafka clusters"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start simple: (1) Optimize — reduce the per-event cost (faster serialization, batched I/O, reduced allocations). Often provides 2-5x improvement. (2) Add consumers — if you have headroom (partitions > consumers), just add instances. (3) Add partitions — enables more consumers but requires careful planning (key redistribution, rebalance). (4) Multi-cluster sharding — the most complex, involving cross-cluster routing and aggregation. Each step increases operational complexity.",
      "detailedExplanation": "The decision turns on \"rank these stream processing scaling strategies from first to try to last resort:\". Build the rank from biggest differences first, then refine with adjacent checks. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-073",
      "type": "ordering",
      "question": "Rank these events in the order they occur during a Flink exactly-once checkpoint:",
      "items": [
        "The checkpoint coordinator injects a checkpoint barrier into each source operator",
        "Barriers flow through the operator graph — each operator snapshots its state when the barrier passes",
        "State snapshots are written asynchronously to durable storage (e.g., S3, HDFS)",
        "All operators report successful snapshot, and the checkpoint coordinator marks the checkpoint complete"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Flink's checkpoint algorithm (based on Chandy-Lamport): (1) Coordinator injects barriers into sources. (2) Barriers propagate through the DAG — each operator aligns barriers from all inputs, then snapshots its state. (3) Snapshots are written to persistent storage asynchronously (processing continues). (4) When all snapshots complete, the checkpoint is finalized. On failure, the job restores from the last completed checkpoint.",
      "detailedExplanation": "Read this as a scenario about \"rank these events in the order they occur during a Flink exactly-once checkpoint:\". Order by relative scale and bottleneck effect, then validate neighboring items. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-074",
      "type": "ordering",
      "question": "Rank these data freshness levels from most delayed to most real-time:",
      "items": [
        "Nightly batch job — data is 0-24 hours stale",
        "Micro-batch processing every 5 minutes — data is 0-5 minutes stale",
        "Continuous stream processing with 10-second checkpoints — data is 0-10 seconds stale",
        "True event-at-a-time processing with sub-second latency"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Data freshness spectrum: nightly batch is the most delayed (hours stale). Micro-batch (Spark Streaming) processes in small intervals (minutes stale). Continuous stream processing (Flink, Kafka Streams) processes events as they arrive (seconds stale, bounded by checkpoint interval for recovery). True event-at-a-time processing achieves sub-second latency for each event. Each step toward real-time increases infrastructure complexity and cost.",
      "detailedExplanation": "Use \"rank these data freshness levels from most delayed to most real-time:\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-075",
      "type": "multiple-choice",
      "question": "A team's Kafka Streams application uses a GlobalKTable for country reference data (200 countries). Every application instance holds a full copy of this table. How does this differ from a regular KTable?",
      "options": [
        "There is no difference — they're aliases",
        "A GlobalKTable is partitioned across instances like a regular KTable",
        "A GlobalKTable is replicated to every instance (every instance has all 200 entries), while a regular KTable is partitioned (each instance has a subset of entries based on the key's partition assignment)",
        "GlobalKTables are stored in Kafka; regular KTables are in memory only"
      ],
      "correct": 2,
      "explanation": "Regular KTable: partitioned by key, each instance holds entries for its assigned partitions. If instance A has partitions 0-3 and country 'US' hashes to partition 5, instance A can't look up 'US.' GlobalKTable: every instance holds the complete table. Any instance can look up any country. This is ideal for small reference data (countries, currencies, config) that every instance needs. Not suitable for large datasets — memory usage = full table × instance count.",
      "detailedExplanation": "This prompt is really about \"team's Kafka Streams application uses a GlobalKTable for country reference data (200\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 200 and 0 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-076",
      "type": "multiple-choice",
      "question": "A team processes payment events and must ensure that each payment is processed exactly once, even across failures. They use Kafka's transactional API. What does a Kafka transaction encompass?",
      "options": [
        "Only the producer's write to the output topic",
        "Only the consumer's offset commit",
        "The consumer's read, the application's processing logic, and the producer's write to output topics — all committed or all rolled back atomically",
        "A distributed lock across all consumers"
      ],
      "correct": 2,
      "explanation": "Kafka transactions bundle: (1) consuming input (advancing offsets), (2) processing, and (3) producing output — into an atomic unit. Either all succeed (offsets committed, outputs written) or all fail (offsets rolled back, outputs discarded). This prevents the gap between 'output written' and 'offset committed' that causes duplicates in at-least-once processing. Note: this only covers Kafka-to-Kafka pipelines. External system calls aren't part of the transaction.",
      "detailedExplanation": "If you keep \"team processes payment events and must ensure that each payment is processed exactly\" in view, the correct answer separates faster. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-077",
      "type": "multiple-choice",
      "question": "A team uses Kafka Streams to compute real-time metrics. They deploy 4 instances. A REST API layer needs to query the aggregated state (e.g., 'total orders for customer X'). But the state is partitioned — customer X's data is on only one instance. How does the API find it?",
      "options": [
        "Query all 4 instances and merge results",
        "Use Kafka Streams' interactive queries feature: it exposes metadata about which instance hosts which key's state, enabling the API to route the query directly to the correct instance",
        "Replicate all state to every instance",
        "Store all state in an external database accessible to the API"
      ],
      "correct": 1,
      "explanation": "Interactive queries: Kafka Streams maintains metadata mapping each key to its hosting instance (based on partition assignment). The API server queries the metadata: 'which instance has customer X?' → instance 3. It then routes the request directly to instance 3's local state store. This avoids querying all instances or maintaining a separate database. If instance 3 is down, standby replicas (if configured) serve the query.",
      "detailedExplanation": "The core signal here is \"team uses Kafka Streams to compute real-time metrics\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Keep quantities like 4 and 3 in aligned units before selecting an answer. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-078",
      "type": "multi-select",
      "question": "Which are disadvantages of micro-batch processing (e.g., Spark Structured Streaming) compared to true continuous stream processing (e.g., Flink)? (Select all that apply)",
      "options": [
        "Higher minimum latency — results are delayed by the batch interval (e.g., minimum 100ms-1s even for simple operations)",
        "Simpler programming model that reuses batch APIs",
        "Less efficient use of resources during low-traffic periods (each micro-batch has fixed overhead)",
        "Poorer support for event-time processing and complex windowing"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Micro-batch trade-offs: (1) Latency floor — even the smallest micro-batch adds the batch interval as minimum latency. (2) Event-time/windowing — micro-batch was designed around processing-time; event-time support was added later and is less natural than in Flink's event-at-a-time model. (3) NOT a disadvantage: the simpler programming model is actually an advantage of micro-batch. (4) Fixed overhead per batch means low-traffic periods waste resources on empty or near-empty batches.",
      "detailedExplanation": "The key clue in this question is \"disadvantages of micro-batch processing (e\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-079",
      "type": "numeric-input",
      "question": "A Flink job checkpoints every 30 seconds. Each checkpoint writes 2 GB of state to S3 at 200 MB/s. How many seconds does each checkpoint take to write?",
      "answer": 10,
      "unit": "seconds",
      "tolerance": "exact",
      "explanation": "Write time: 2,000 MB / 200 MB/s = 10 seconds. With a 30-second checkpoint interval, the job spends 10/30 ≈ 33% of its time writing checkpoints. If the state grows to 6 GB, write time becomes 30 seconds — equal to the interval, causing checkpoint backpressure. Solutions: increase the interval, enable incremental checkpoints (write only changed state), use faster storage, or reduce state size.",
      "detailedExplanation": "Start from \"flink job checkpoints every 30 seconds\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 30 seconds and 2 GB in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-080",
      "type": "numeric-input",
      "question": "A Kafka consumer falls 2 million messages behind. It processes at 15,000 msg/s while the producer writes 10,000 msg/s. How many seconds until the consumer catches up?",
      "answer": 400,
      "unit": "seconds",
      "tolerance": "exact",
      "explanation": "Surplus processing rate: 15,000 - 10,000 = 5,000 msg/s (the consumer processes 5,000 more per second than are produced). Catch-up time: 2,000,000 / 5,000 = 400 seconds ≈ 6.7 minutes. If the consumer could only process 10,000 msg/s (equal to production rate), it would never catch up. If it processed 9,000 msg/s (below production rate), lag would grow forever. Surplus capacity is essential for recovery.",
      "detailedExplanation": "Start from \"kafka consumer falls 2 million messages behind\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 2 and 15,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-081",
      "type": "ordering",
      "question": "Rank these Kafka consumer configurations from most aggressive (fastest consumption) to most conservative (safest processing):",
      "items": [
        "max.poll.records=10000, enable.auto.commit=true, auto.commit.interval.ms=60000",
        "max.poll.records=1000, enable.auto.commit=true, auto.commit.interval.ms=5000",
        "max.poll.records=100, enable.auto.commit=false (manual commit after each batch)",
        "max.poll.records=1, enable.auto.commit=false (manual commit after each message)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Most aggressive: large batches (10K records) with infrequent auto-commit (60s) — maximum throughput but up to 60 seconds of reprocessing on crash. Middle ground: 1K records with 5s auto-commit. Conservative: 100 records with manual commit per batch. Most conservative: single-message processing with per-message commit — lowest throughput but minimal reprocessing risk. The right setting depends on the tolerance for duplicate processing vs. throughput requirements.",
      "detailedExplanation": "The key clue in this question is \"rank these Kafka consumer configurations from most aggressive (fastest consumption) to\". Build the rank from biggest differences first, then refine with adjacent checks. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 10K and 60s should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-082",
      "type": "ordering",
      "question": "Rank these steps in the correct order for investigating why a Kafka consumer group's lag is growing:",
      "items": [
        "Check consumer lag metrics per partition to identify which partitions are falling behind",
        "Determine if the bottleneck is processing speed (CPU/IO bound) or consumption speed (network/deserialization)",
        "If processing-bound: profile the processing logic, optimize hot paths, or add consumers (up to partition count)",
        "If the consumer count equals partition count and still can't keep up: increase partitions or optimize message format"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnosis first: (1) Identify which partitions lag — uniform lag suggests a systemic issue; single-partition lag suggests data skew. (2) Determine the bottleneck — is the consumer spending time in processing logic (CPU) or in consuming/deserializing (IO)? (3) For processing bottlenecks: optimize code or scale out (if partitions available). (4) If already at max consumers: the fundamental limit is partition count or message format efficiency. Each step narrows the cause before acting.",
      "detailedExplanation": "Read this as a scenario about \"rank these steps in the correct order for investigating why a Kafka consumer group's\". Place obvious extremes first, then sort the middle by pairwise comparison. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 1 and 2 in aligned units before selecting an answer. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-083",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team uses Kafka Connect to stream database changes (CDC) into Kafka topics. The CDC connector captures INSERT, UPDATE, and DELETE operations from a PostgreSQL WAL. What does each Kafka message represent?",
          "options": [
            "A SQL query to replay",
            "A complete database snapshot",
            "A single row-level change event — an INSERT, UPDATE, or DELETE on one row, including the before/after state of the row",
            "A batch of changes from the last hour"
          ],
          "correct": 2,
          "explanation": "CDC captures row-level changes from the database's write-ahead log. Each Kafka message represents one row change: INSERT (new row data), UPDATE (before and after row data), or DELETE (deleted row data). This creates a real-time change stream that downstream consumers can use to: replicate to another database, update a search index, maintain a cache, or drive stream processing — all without modifying the source application.",
          "detailedExplanation": "Use \"team uses Kafka Connect to stream database changes (CDC) into Kafka topics\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "The team's Kafka topic receives CDC events keyed by the row's primary key. Over time, a row is inserted, updated 50 times, and then deleted. If the topic uses log compaction, what remains for this key after compaction runs?",
          "options": [
            "The insert and the latest update",
            "Only the latest update event",
            "Only the delete event (tombstone) — and eventually, even the tombstone is removed, leaving nothing for this key",
            "All 52 events (insert + 50 updates + delete)"
          ],
          "correct": 2,
          "explanation": "Compaction retains the latest record per key. After 50 updates, compaction keeps only the most recent update. After the DELETE (published as a tombstone with value=null), compaction eventually removes all records for this key — including the tombstone itself (after a configurable tombstone retention period). The key completely disappears from the compacted topic, correctly representing that the row no longer exists in the source database.",
          "detailedExplanation": "The core signal here is \"team's Kafka topic receives CDC events keyed by the row's primary key\". Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 50 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The decision turns on \"stream Processing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "A stream processing job reads from a Kafka topic, transforms events, and writes to another Kafka topic. The job uses exactly-once semantics (Kafka transactions). During a transaction, the job reads 1,000 events, processes them, and produces 1,000 output events. It then commits the transaction. If the commit fails, what happens?",
          "options": [
            "The broker retries the commit automatically",
            "The transaction is aborted — the 1,000 output events are discarded (marked as aborted), and the consumer offsets are not advanced. On retry, the same 1,000 input events are reprocessed",
            "Half the events are committed, half are lost",
            "The 1,000 output events are visible to downstream consumers"
          ],
          "correct": 1,
          "explanation": "Kafka transactions are atomic: all outputs and offset commits succeed together, or all are rolled back. If the commit fails, the output events are marked as 'aborted' (downstream consumers with read_committed isolation skip them). The input offsets aren't advanced, so the consumer re-reads the same 1,000 events. The retry produces the same outputs (deterministic processing assumed), and the next commit attempt makes them visible.",
          "detailedExplanation": "Start from \"stream processing job reads from a Kafka topic, transforms events, and writes to\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 1,000 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Downstream consumers of the output topic are configured with isolation.level=read_committed. What does this mean for them during an in-progress transaction?",
          "options": [
            "They buffer all events until the topic is empty",
            "They don't see events from in-progress or aborted transactions — only events from committed transactions are visible, ensuring they never process data that might be rolled back",
            "They see all events immediately, including uncommitted ones",
            "They see events but mark them as tentative"
          ],
          "correct": 1,
          "explanation": "read_committed isolation: consumers only see messages from committed transactions. During an in-progress transaction, its output messages are in the topic but invisible to read_committed consumers. If the transaction aborts, those messages become permanently invisible. If it commits, they become visible atomically. This is how Kafka's exactly-once semantics propagate end-to-end: producers use transactions, consumers use read_committed, and the combination ensures no duplicates or phantom reads.",
          "detailedExplanation": "The decision turns on \"downstream consumers of the output topic are configured with isolation\". Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"stream Processing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-085",
      "type": "multi-select",
      "question": "Which are signs that a Kafka topic needs more partitions? (Select all that apply)",
      "options": [
        "The topic stores less than 1 GB of data",
        "All consumers in the group are busy, but adding more consumers doesn't help because they sit idle (consumers = partitions already)",
        "Producer throughput is limited because a single partition leader can't handle the write rate",
        "Consumer lag is growing because all consumers are at maximum throughput and there are no idle consumers to add"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Partition scaling signals: (1) Growing lag with all consumers maxed out — the group can't process fast enough. (2) Consumers = partitions and still not enough — adding consumers is impossible without more partitions. (3) Write bottleneck on a single partition leader — more partitions distribute writes across more brokers. Low data volume (1 GB) is NOT a reason for more partitions — partition count is about throughput and parallelism, not storage.",
      "detailedExplanation": "Use \"signs that a Kafka topic needs more partitions? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-086",
      "type": "multiple-choice",
      "question": "A team discovers that their Kafka Streams application's state store has grown to 100 GB across all instances. Checkpoints are slow and failover takes 10+ minutes to replay the changelog. What optimization reduces both checkpoint size and recovery time?",
      "options": [
        "Reduce the number of partitions",
        "Increase the checkpoint interval to reduce frequency",
        "Switch from RocksDB to in-memory state storage",
        "Enable incremental checkpoints — only write changed state since the last checkpoint, and use standby replicas for fast failover"
      ],
      "correct": 3,
      "explanation": "Incremental checkpoints: instead of writing the full 100 GB state each time, only write the delta (changes since last checkpoint). If 1% of state changes per checkpoint interval, the write drops from 100 GB to 1 GB — dramatically faster. For recovery, standby replicas maintain a near-current copy of the state, reducing failover from 10 minutes (full replay) to seconds (small delta). In-memory storage is impractical for 100 GB and loses data on restart.",
      "detailedExplanation": "The core signal here is \"team discovers that their Kafka Streams application's state store has grown to 100 GB\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 100 GB and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-087",
      "type": "multiple-choice",
      "question": "A team's stream processing pipeline produces results that are consumed by both a real-time dashboard and a nightly batch report. The dashboard needs low latency; the batch report needs completeness (all events processed). What pattern serves both use cases from the same stream?",
      "options": [
        "Only serve the batch report — the dashboard can wait 24 hours",
        "The lambda architecture approach: serve early approximate results to the dashboard from the speed layer, and periodically correct them with the batch layer's complete results. Or use a single stream pipeline with retractions: emit early results, then update them as late events arrive",
        "Only serve the real-time dashboard — the batch report can use approximate data",
        "Run two separate pipelines — one optimized for speed, one for completeness"
      ],
      "correct": 1,
      "explanation": "Two approaches: (1) Lambda architecture: a real-time 'speed layer' provides approximate results quickly, while a batch layer periodically recomputes complete results. The dashboard reads the speed layer; the report reads the batch layer. (2) Kappa architecture (single stream with retractions): emit early results immediately, then update/retract them as late events arrive. The dashboard shows the latest result; the report waits for the window to fully close. Modern stream processing (Flink) supports retractions natively.",
      "detailedExplanation": "If you keep \"team's stream processing pipeline produces results that are consumed by both a\" in view, the correct answer separates faster. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-088",
      "type": "ordering",
      "question": "Rank these Kafka cluster scaling actions from least disruptive to most disruptive:",
      "items": [
        "Add a new broker to the cluster (new partitions can be assigned to it, existing partitions stay put)",
        "Reassign existing partition replicas to the new broker for load balancing (data migration while serving traffic)",
        "Increase the partition count for a topic (changes key-to-partition mapping, may break consumer state)",
        "Decrease the replication factor (removes replicas, reduces fault tolerance during the change)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Adding a broker is minimally disruptive — it joins the cluster and can host new partitions or replicas. Reassigning partitions involves data migration (network I/O) but existing reads/writes continue. Increasing partition count changes key routing — consumers may see state disruption. Decreasing replication factor is the most disruptive: it reduces fault tolerance and involves coordinated replica removal. Each action affects more of the system's guarantees.",
      "detailedExplanation": "Start from \"rank these Kafka cluster scaling actions from least disruptive to most disruptive:\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-089",
      "type": "numeric-input",
      "question": "A stream processor uses tumbling windows of 15 minutes with an allowed lateness of 5 minutes. A window covering 10:00-10:15 closes at the watermark passing 10:15. Late events are accepted until the watermark reaches 10:20 (10:15 + 5 minutes allowed lateness). If the watermark advances at roughly real-time speed, how many minutes after 10:00 does the window's state finally get purged?",
      "answer": 20,
      "unit": "minutes",
      "tolerance": "exact",
      "explanation": "The window 10:00-10:15 is first eligible for emission when the watermark passes 10:15. But the 5-minute allowed lateness keeps the window state until the watermark passes 10:20 (10:15 + 5 = 10:20). At that point, the window state is purged and no more late events for this window are accepted. Total: 20 minutes after the window opened (10:00). During those 20 minutes, the window's aggregation state must be kept in memory/state store.",
      "detailedExplanation": "The key clue in this question is \"stream processor uses tumbling windows of 15 minutes with an allowed lateness of 5\". Keep every transformation in one unit system and check order of magnitude at the end. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 15 minutes and 5 minutes in aligned units before selecting an answer. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-090",
      "type": "numeric-input",
      "question": "A Kafka cluster has 5 brokers. A topic has 10 partitions with replication factor 3. The partition leader for partition 0 goes down. Kafka elects a new leader from the in-sync replicas. If the leader election takes 200ms and the consumer detects the new leader after 500ms (metadata refresh), what is the total unavailability for partition 0?",
      "answer": 700,
      "unit": "ms",
      "tolerance": 0.1,
      "explanation": "Total downtime: leader election (200ms) + consumer metadata refresh (500ms) = 700ms. During this window, partition 0 can't serve reads or writes. The other 9 partitions are unaffected — their leaders are on healthy brokers. This sub-second failover is why Kafka tolerates single-broker failures well. Reducing metadata refresh interval (metadata.max.age.ms) shortens the consumer-side detection, but at the cost of more frequent metadata requests.",
      "detailedExplanation": "The decision turns on \"kafka cluster has 5 brokers\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 5 and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-091",
      "type": "ordering",
      "question": "Rank these stream processing state management approaches from simplest to most fault-tolerant:",
      "items": [
        "In-memory state with no durability (lost on crash)",
        "In-memory state with periodic snapshots to external storage",
        "Local state store (RocksDB) with changelog topic backup",
        "Local state store with changelog backup and standby replicas for instant failover"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Simplest: in-memory with no durability — fast but all state lost on crash. Periodic snapshots add recovery capability but lose state between snapshots. Local store + changelog (Kafka Streams default) provides full state recovery by replaying the changelog. Standby replicas add instant failover — the standby has a near-current state copy, eliminating changelog replay delay. Each level adds operational complexity but improves recovery time and data safety.",
      "detailedExplanation": "Read this as a scenario about \"rank these stream processing state management approaches from simplest to most\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-092",
      "type": "multiple-choice",
      "question": "A team processes clickstream data. Their Kafka topic has 100 partitions to handle peak traffic of 1 million events/second. During off-peak hours (2am-6am), traffic drops to 10,000 events/second but the 100 consumers remain running. What is the cost implication?",
      "options": [
        "Kafka charges per-consumer fees",
        "The consumers automatically scale down",
        "90+ consumers are nearly idle (10,000/100 = 100 events/s each), wasting compute resources. Autoscaling the consumer count based on lag or throughput would reduce costs during off-peak",
        "No cost implication — idle consumers use no resources"
      ],
      "correct": 2,
      "explanation": "Static consumer allocation wastes resources during low-traffic periods. At 100 events/s per consumer, most of the compute capacity sits idle. Autoscaling: scale consumers from 100 (peak) to 10 (off-peak) based on consumer lag or traffic metrics. Kubernetes HPA (Horizontal Pod Autoscaler) with custom Kafka lag metrics can automate this. The savings can be 90% of compute costs during off-peak hours.",
      "detailedExplanation": "The key clue in this question is \"team processes clickstream data\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 100 and 1 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-093",
      "type": "multi-select",
      "question": "Which are valid approaches for handling exactly-once semantics when the stream processor writes to an external database (not Kafka)? (Select all that apply)",
      "options": [
        "Use idempotent writes keyed by a unique event identifier",
        "Use a two-phase commit between Kafka and the database",
        "Store Kafka offsets in the same database transaction as the processing result",
        "Rely on Kafka transactions alone (without any database-side logic)"
      ],
      "correctIndices": [0, 2],
      "explanation": "For external databases: (1) Store offsets in the DB — the offset commit and result write are in the same ACID transaction. On restart, the consumer reads the offset from the DB and resumes from the correct position. (2) Idempotent writes — process events at-least-once, but make writes harmless to repeat (upsert by event ID). Kafka transactions alone can't span external systems. Two-phase commit is theoretically possible but rarely practical — it requires the external DB to participate in a distributed transaction protocol.",
      "detailedExplanation": "Start from \"valid approaches for handling exactly-once semantics when the stream processor writes\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-094",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team's Kafka Streams application performs a stream-stream join between 'page_views' and 'ad_clicks' with a 15-minute join window. During peak traffic, the join state grows to 40 GB. They need to reduce state size. Which approach helps?",
          "options": [
            "Increase the join window to 30 minutes",
            "Add more partitions",
            "Use acks=0 on the producer",
            "Reduce the join window to 5 minutes — fewer events need to be buffered from each stream, proportionally reducing state"
          ],
          "correct": 3,
          "explanation": "Stream-stream join state is proportional to: throughput × window size × both streams. Reducing the window from 15 to 5 minutes reduces state by ~3x (from 40 GB to ~13 GB). The trade-off: clicks that arrive more than 5 minutes after the impression won't be matched. If 95% of clicks occur within 5 minutes, this accepts a 5% miss rate for a 3x state reduction. Window size tuning is the primary lever for join state management.",
          "detailedExplanation": "The key clue in this question is \"team's Kafka Streams application performs a stream-stream join between 'page_views' and\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 15 and 40 GB appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic."
        },
        {
          "question": "Even with a 5-minute window, the state is still 13 GB. The team considers filtering events before the join to reduce volume. They notice 60% of page views never receive a click. Can they pre-filter these 'unclickable' page views?",
          "options": [
            "Use sampling to reduce volume by 60%",
            "No — in a stream-stream join, you can't know at the time a page view arrives whether a click will follow. Filtering would require knowing the future. Pre-filtering is only possible for events with attributes that definitively exclude them (e.g., bot traffic identified by user agent)",
            "Yes — filter out page views that won't receive clicks",
            "Filter after the join completes"
          ],
          "correct": 1,
          "explanation": "Stream-stream join challenge: when a page_view arrives, you don't know if a click will follow. Pre-filtering 'unclickable' views requires predicting the future. However, you CAN filter events that are definitively unmatchable based on their attributes: bot traffic (no human clicks), internal test events, or events for deactivated ads. This reduces state without sacrificing join accuracy. The 60% miss rate is only known in hindsight — it can't be used for prediction.",
          "detailedExplanation": "Read this as a scenario about \"even with a 5-minute window, the state is still 13 GB\". Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 5 and 13 GB should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "If you keep \"stream Processing\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    },
    {
      "id": "msg-str-095",
      "type": "multiple-choice",
      "question": "A team migrates a batch ETL job (runs nightly, processes 24 hours of data) to a stream processing pipeline. The batch job produced deterministic, complete results. The stream pipeline produces incremental, eventually consistent results. What behavior change must downstream consumers handle?",
      "options": [
        "Stream results are always more accurate than batch results",
        "No behavior change — stream results are identical to batch results",
        "The stream pipeline produces results in a different format",
        "Results may be emitted multiple times as they are refined: an early result when the window first closes, then updates as late events arrive. Downstream consumers must handle retractions/updates instead of receiving a single final answer"
      ],
      "correct": 3,
      "explanation": "Batch produces one result per window (after all data is available). Streams produce evolving results: an initial estimate when the window closes, then corrections as late data arrives. Downstream systems must handle: (1) idempotent writes (an updated result overwrites the previous one), (2) retractions (subtract the old result, add the new one), or (3) versioned results (keep all versions, query the latest). This 'result evolution' is the fundamental mental model shift from batch to stream processing.",
      "detailedExplanation": "The core signal here is \"team migrates a batch ETL job (runs nightly, processes 24 hours of data) to a stream\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 24 hours and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        }
      ],
      "tags": ["messaging-async", "stream-processing"],
      "difficulty": "senior"
    }
  ]
}
