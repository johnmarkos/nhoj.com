{
  "unit": 2,
  "unitTitle": "Data Modeling",
  "chapter": 5,
  "chapterTitle": "Denormalization",
  "chapterDescription": "When to break the rules — read vs write tradeoffs, precomputed data, and pragmatic schema design.",
  "problems": [
    {
      "id": "denorm-001",
      "type": "multiple-choice",
      "question": "What is denormalization?",
      "options": ["Removing all indexes from a database", "Intentionally introducing redundancy to optimize read performance", "Converting a relational database to NoSQL", "Deleting unused columns"],
      "correct": 1,
      "explanation": "Denormalization deliberately adds redundant data (duplicated columns, precomputed values, merged tables) to avoid expensive joins and speed up reads — at the cost of more complex writes."
    },
    {
      "id": "denorm-002",
      "type": "multiple-choice",
      "question": "What is the primary tradeoff of denormalization?",
      "options": ["More disk space for less CPU", "Faster reads at the cost of slower/more complex writes", "Simpler schema at the cost of fewer features", "Better security at the cost of performance"],
      "correct": 1,
      "explanation": "Denormalization speeds up reads (fewer joins, precomputed data) but makes writes harder — every update must maintain consistency across all copies of the duplicated data."
    },
    {
      "id": "denorm-003",
      "type": "multi-select",
      "question": "Which are valid reasons to denormalize?",
      "options": ["Reads vastly outnumber writes", "You want to avoid learning SQL joins", "Query latency requirements can't be met with normalized schema + indexes", "Reporting queries scan millions of rows across many tables"],
      "correctIndices": [0, 2, 3],
      "explanation": "Denormalize when read performance is critical, joins are too expensive at scale, or analytical queries need pre-aggregated data. Avoiding joins because you don't understand them is not a valid reason."
    },
    {
      "id": "denorm-004",
      "type": "multiple-choice",
      "question": "An e-commerce product page shows: product name, price, average rating, and review count. The normalized schema requires joining Products, Reviews, and computing AVG(rating) and COUNT(*). What's the standard denormalization?",
      "options": ["Cache the entire page as HTML", "Store avg_rating and review_count directly on the Products table", "Create a separate Analytics database", "Remove the reviews feature"],
      "correct": 1,
      "explanation": "Store precomputed avg_rating and review_count on the Products row. Update them when reviews are added/modified. This avoids a join + aggregation on every product page view."
    },
    {
      "id": "denorm-005",
      "type": "two-stage",
      "stages": [
        {
          "question": "A social media app shows a feed with posts and author names. The normalized query joins Posts and Users on every feed load. With 10M daily active users loading feeds, what's the denormalization approach?",
          "options": ["Store author_name directly on the Posts table", "Cache user names in Redis only", "Use a graph database", "Shard the Users table"],
          "correct": 0,
          "explanation": "Duplicating author_name onto Posts avoids the join entirely. Each feed query reads from a single table."
        },
        {
          "question": "A user changes their display name. What must happen to maintain consistency?",
          "options": ["Nothing — the old name is fine", "Update all their Posts rows with the new name", "Delete and recreate all their posts", "The name change is blocked until all posts are updated synchronously"],
          "correct": 1,
          "explanation": "Every post by that user has a stale copy of the name. You must update all their posts — typically via an async background job, since updating thousands of rows synchronously would be too slow."
        }
      ]
    },
    {
      "id": "denorm-006",
      "type": "ordering",
      "question": "Rank these strategies from LEAST to MOST redundancy introduced:",
      "items": ["Add an index on the normalized table", "Create a materialized view", "Duplicate columns across tables", "Maintain a fully separate denormalized copy of the data"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Index: no data duplication, just a lookup structure. Materialized view: a precomputed snapshot, refreshed periodically. Column duplication: specific fields copied to another table. Full copy: entire dataset duplicated in a different shape."
    },
    {
      "id": "denorm-007",
      "type": "multiple-choice",
      "question": "What is a materialized view?",
      "options": ["A virtual table that runs a query on every access", "A precomputed query result stored as a physical table and refreshed periodically", "A backup copy of a table", "An index on a view"],
      "correct": 1,
      "explanation": "A materialized view stores the result of a query physically on disk. Unlike a regular view (which re-runs the query each time), it's precomputed. The tradeoff: data can be stale between refreshes."
    },
    {
      "id": "denorm-008",
      "type": "multi-select",
      "question": "Which are downsides of materialized views?",
      "options": ["Data can be stale between refreshes", "Refresh operations can be expensive", "They use additional storage", "They prevent you from querying the underlying tables"],
      "correctIndices": [0, 1, 2],
      "explanation": "Materialized views can go stale, cost storage, and refreshing them (especially incrementally) can be expensive. They don't prevent access to underlying tables — you can still query both."
    },
    {
      "id": "denorm-009",
      "type": "multiple-choice",
      "question": "A dashboard shows daily revenue broken down by product category. The query scans the entire Orders table and joins Products. What denormalization reduces this from minutes to milliseconds?",
      "options": ["Add more indexes", "Precompute daily_revenue_by_category in a summary table, updated nightly", "Switch to a columnar database", "Use a larger server"],
      "correct": 1,
      "explanation": "A summary/aggregate table precomputes the result. The dashboard reads a few rows instead of scanning millions. Update it via a nightly batch job or incrementally on each order."
    },
    {
      "id": "denorm-010",
      "type": "numeric-input",
      "question": "A normalized query joins 4 tables and takes 200ms. After denormalizing into a single table, the query takes 5ms. What is the speedup factor?",
      "answer": 40,
      "unit": "x",
      "tolerance": "exact",
      "explanation": "200ms / 5ms = 40x speedup. This is typical for denormalization of multi-join queries into single-table lookups."
    },
    {
      "id": "denorm-011",
      "type": "multiple-choice",
      "question": "What is a 'counter cache' in denormalization?",
      "options": ["A Redis cache for counting queries", "A column that stores a precomputed count to avoid COUNT(*) queries", "An index optimized for counting", "A log of all count operations"],
      "correct": 1,
      "explanation": "A counter cache (e.g., posts_count on the Users table) avoids running COUNT(*) on the Posts table. Increment/decrement it when posts are created/deleted."
    },
    {
      "id": "denorm-012",
      "type": "two-stage",
      "stages": [
        {
          "question": "A forum shows thread listings with reply_count for each thread. Currently, reply_count is computed via COUNT(*) on the Replies table. With 1M threads, each page load queries: SELECT thread.*, COUNT(r.id) FROM threads JOIN replies r ... GROUP BY thread.id. What should you do?",
          "options": ["Add reply_count column to the Threads table", "Cache the result in memcached", "Use a faster database", "Limit threads to 100 replies"],
          "correct": 0,
          "explanation": "Add a reply_count column to Threads. This eliminates the join and GROUP BY entirely. Each thread listing becomes a simple SELECT from one table."
        },
        {
          "question": "How do you keep reply_count accurate?",
          "options": ["Recalculate it on every page load", "Increment on reply insert, decrement on reply delete, via application code or triggers", "Run a daily batch job to recount", "Store it in a separate counter service"],
          "correct": 1,
          "explanation": "Increment/decrement atomically (UPDATE threads SET reply_count = reply_count + 1 WHERE id = ?). This is cheap per-write and always accurate. A daily batch job would leave counts stale."
        }
      ]
    },
    {
      "id": "denorm-013",
      "type": "ordering",
      "question": "Rank these denormalization maintenance strategies from SIMPLEST to MOST COMPLEX:",
      "items": ["Application code updates both tables", "Database triggers", "Change Data Capture (CDC) pipeline", "Event-driven async workers"],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Application code: simplest, just add an extra UPDATE. Triggers: automatic but harder to debug/test. Async workers: decoupled but need message infrastructure. CDC pipeline: most complex, requires dedicated infrastructure (Debezium, Kafka, etc.)."
    },
    {
      "id": "denorm-014",
      "type": "multiple-choice",
      "question": "In a star schema, what is a 'fact table'?",
      "options": ["A table of verified, true data", "The central table containing measurable events (sales, clicks, etc.) with foreign keys to dimension tables", "A lookup table for reference data", "A table that stores database metadata"],
      "correct": 1,
      "explanation": "A fact table holds quantitative, measurable events — each row is a business event (a sale, a page view) with foreign keys pointing to dimension tables (who, what, when, where)."
    },
    {
      "id": "denorm-015",
      "type": "multiple-choice",
      "question": "In a star schema, what is a 'dimension table'?",
      "options": ["A table storing aggregated metrics", "A descriptive lookup table (customers, products, dates) that provides context to fact table rows", "A table for storing spatial data", "A junction table for M:N relationships"],
      "correct": 1,
      "explanation": "Dimension tables provide the 'who, what, when, where' context. They're typically wide (many columns) and relatively small compared to fact tables. Examples: dim_customer, dim_product, dim_date."
    },
    {
      "id": "denorm-016",
      "type": "multi-select",
      "question": "Which are characteristics of star schema design?",
      "options": ["Dimension tables are typically denormalized (wide, flat)", "Optimized for analytical queries (OLAP)", "Requires fewer joins than fully normalized schemas", "Best suited for OLTP transaction processing"],
      "correctIndices": [0, 1, 2],
      "explanation": "Star schemas denormalize dimension tables to be wide and flat, optimizing for analytical reads with fewer joins. They're designed for OLAP (analytics), not OLTP (transactional processing)."
    },
    {
      "id": "denorm-017",
      "type": "multiple-choice",
      "question": "What distinguishes a snowflake schema from a star schema?",
      "options": ["Snowflake has no fact table", "Snowflake normalizes dimension tables into sub-dimensions", "Snowflake uses NoSQL storage", "Snowflake has multiple fact tables"],
      "correct": 1,
      "explanation": "A snowflake schema normalizes dimensions — e.g., instead of a flat dim_product with category_name, you'd have dim_product → dim_category. This saves space but adds joins, partially undoing the denormalization benefit."
    },
    {
      "id": "denorm-018",
      "type": "two-stage",
      "stages": [
        {
          "question": "An analytics team needs to query: 'total revenue by product category by month.' The OLTP schema has Orders, OrderItems, Products, and Categories — 4 tables, heavily normalized. What schema design fits the analytics use case?",
          "options": ["Star schema with a sales fact table and category/date dimensions", "Keep the OLTP schema and add indexes", "Create a single flat CSV export", "Use a graph database"],
          "correct": 0,
          "explanation": "A star schema with fact_sales (revenue, quantity, category_key, date_key) and dimensions (dim_category, dim_date) is purpose-built for this. The query becomes a simple GROUP BY on one or two joins."
        },
        {
          "question": "Should this analytics star schema live in the same database as the OLTP system?",
          "options": ["Yes — easier to keep in sync", "No — separate data warehouse to avoid analytics queries impacting production", "It doesn't matter", "Only if using PostgreSQL"],
          "correct": 1,
          "explanation": "Analytics queries (full table scans, heavy aggregations) would compete with production transactions for resources. Use a separate data warehouse (or at minimum a read replica) populated via ETL or CDC."
        }
      ]
    },
    {
      "id": "denorm-019",
      "type": "multiple-choice",
      "question": "What is 'embedding' in the context of document databases (e.g., MongoDB)?",
      "options": ["Storing vector embeddings for ML", "Nesting related data inside a parent document instead of using a separate collection with references", "Encrypting data within the document", "Linking documents via URL"],
      "correct": 1,
      "explanation": "Embedding means storing related data (e.g., addresses) directly inside a parent document (e.g., user) as a nested object or array. This is denormalization — it avoids 'joins' (lookups) at the cost of potential data duplication."
    },
    {
      "id": "denorm-020",
      "type": "multi-select",
      "question": "When is embedding (nesting) data in a document a good choice?",
      "options": ["The nested data is always read together with the parent", "The nested data is shared across many parents", "The nested data has a bounded, small size", "The nested data changes frequently and independently"],
      "correctIndices": [0, 2],
      "explanation": "Embed when data is always read together (one query) and bounded in size (won't grow unbounded). Don't embed if data is shared across documents (duplication nightmare) or changes independently and frequently."
    },
    {
      "id": "denorm-021",
      "type": "ordering",
      "question": "Rank these data relationships from BEST to WORST candidates for embedding in a document DB:",
      "items": ["User → shipping addresses (max 5)", "BlogPost → comments (unbounded)", "Order → line items (bounded per order)", "Product → shared categories"],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Addresses: bounded, always read with user — ideal. Line items: bounded per order, always read together — good. Comments: unbounded, can grow to thousands — risky. Categories: shared across products — embedding means duplication nightmares."
    },
    {
      "id": "denorm-022",
      "type": "multiple-choice",
      "question": "A blog platform stores posts and tags. Each post has 1-5 tags, and each tag is used by thousands of posts (M:N). Should you embed tags in each post document?",
      "options": ["Yes — embed the full tag objects in every post", "Embed only tag IDs/names (lightweight duplication), keep a separate Tags collection for metadata", "Never embed — always use references", "Store tags as a comma-separated string"],
      "correct": 1,
      "explanation": "Embed lightweight tag data (name, slug) in posts for read performance, but keep a canonical Tags collection for tag metadata (description, creation date). If a tag name changes, update all posts — acceptable for rare tag renames."
    },
    {
      "id": "denorm-023",
      "type": "multiple-choice",
      "question": "What is the 'write amplification' problem in denormalized schemas?",
      "options": ["Writes use more disk I/O due to larger rows", "A single logical change requires updating multiple locations, multiplying write operations", "Write queries are more syntactically complex", "The database writes data twice for durability"],
      "correct": 1,
      "explanation": "If author_name is stored in 10,000 posts, changing the name requires 10,000 updates instead of 1. This is write amplification — one logical change fans out to many physical writes."
    },
    {
      "id": "denorm-024",
      "type": "numeric-input",
      "question": "A user has authored 5,000 posts. Their display_name is denormalized onto each post. If they change their name, how many rows need updating (including the Users row)?",
      "answer": 5001,
      "unit": "rows",
      "tolerance": "exact",
      "explanation": "1 Users row + 5,000 Posts rows = 5,001 total row updates. This is the write amplification cost of denormalization."
    },
    {
      "id": "denorm-025",
      "type": "two-stage",
      "stages": [
        {
          "question": "An online store has 50M products. Each product page shows the seller's name and rating. Currently this requires joining Products and Sellers. The team proposes denormalizing seller_name and seller_rating onto the Products table. What's the concern?",
          "options": ["Read performance will degrade", "When a seller updates their name or rating, up to millions of product rows need updating", "The Products table will have too many columns", "Joins would actually be faster"],
          "correct": 1,
          "explanation": "A seller with 100K products means updating 100K rows on a name change. With 50M total products, these cascading updates can cause significant write load."
        },
        {
          "question": "What's a practical mitigation for this write amplification?",
          "options": ["Don't denormalize — use the join", "Denormalize but update asynchronously via a background job, accepting brief staleness", "Only denormalize for sellers with few products", "Use a different database"],
          "correct": 1,
          "explanation": "Accept eventual consistency — display the (briefly) stale name, and let a background worker propagate the change. Seller name changes are rare; brief staleness is acceptable."
        }
      ]
    },
    {
      "id": "denorm-026",
      "type": "multi-select",
      "question": "Which strategies help manage consistency in denormalized schemas?",
      "options": ["Database triggers that auto-update denormalized columns", "Application-level code that writes to all copies in the same transaction", "Async event-driven updates with eventual consistency", "Periodic reconciliation batch jobs that detect and fix drift"],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four are valid strategies with different tradeoffs. Triggers: automatic but hidden logic. App-level: explicit but couples writes. Async: decoupled but eventually consistent. Batch reconciliation: safety net for catching drift."
    },
    {
      "id": "denorm-027",
      "type": "multiple-choice",
      "question": "What is the main risk of using database triggers to maintain denormalized data?",
      "options": ["Triggers can't update other tables", "Hidden side effects — developers may not realize writes cascade, making debugging and testing harder", "Triggers are too slow", "Triggers don't work in production"],
      "correct": 1,
      "explanation": "Triggers execute invisibly on every qualifying write. Developers may not know they exist, leading to surprising performance issues, unexpected data changes, and difficulty debugging. The logic is hidden from application code."
    },
    {
      "id": "denorm-028",
      "type": "multiple-choice",
      "question": "A normalized schema has User(id, name) and Post(id, user_id, title). Every post listing requires a join. The site does 100K reads/sec and 100 writes/sec. Should you denormalize by adding user_name to Post?",
      "options": ["No — the write overhead isn't worth it", "Yes — 1000:1 read/write ratio makes the write amplification negligible", "Only if using NoSQL", "It depends on the database vendor"],
      "correct": 1,
      "explanation": "With a 1000:1 read-to-write ratio, each denormalized write saves 1000 joins. The math overwhelmingly favors denormalization. The rare extra write cost is insignificant compared to eliminating joins on every read."
    },
    {
      "id": "denorm-029",
      "type": "numeric-input",
      "question": "A system does 50K reads/sec and 500 writes/sec. What is the read-to-write ratio?",
      "answer": 100,
      "unit": ":1",
      "tolerance": "exact",
      "explanation": "50,000 / 500 = 100:1. This is a read-heavy workload where denormalization is likely worthwhile."
    },
    {
      "id": "denorm-030",
      "type": "ordering",
      "question": "Rank these read-to-write ratios from LEAST to MOST likely to benefit from denormalization:",
      "items": ["1:1 (equal reads and writes)", "10:1", "100:1", "1000:1"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Higher read-to-write ratios benefit more from denormalization. At 1:1, write amplification may not be worth the read savings. At 1000:1, each redundant write saves 1000 joins — an easy win."
    },
    {
      "id": "denorm-031",
      "type": "multiple-choice",
      "question": "What is a 'summary table' (also called an 'aggregate table')?",
      "options": ["A table that summarizes the schema", "A precomputed table storing aggregated results (totals, averages, counts) for fast retrieval", "A table with fewer columns than the original", "A log table for auditing"],
      "correct": 1,
      "explanation": "Summary tables store precomputed aggregations — daily sales totals, monthly user counts, etc. Instead of scanning millions of rows, you read one row from the summary table."
    },
    {
      "id": "denorm-032",
      "type": "two-stage",
      "stages": [
        {
          "question": "A SaaS dashboard shows each customer their API usage: total calls this month, average response time, error rate. The raw Events table has 500M rows/month. How should you serve the dashboard?",
          "options": ["Query the Events table with appropriate indexes", "Maintain a customer_monthly_usage summary table with precomputed metrics", "Export to a spreadsheet", "Sample 1% of events"],
          "correct": 1,
          "explanation": "A summary table with one row per customer per month (total_calls, avg_response_ms, error_count) makes the dashboard query trivial. Update it incrementally as events arrive."
        },
        {
          "question": "How should the summary table be updated?",
          "options": ["Full recompute every hour from raw events", "Incremental update on each new event (atomic increment/running average)", "Only update once per day", "Let the dashboard compute it client-side"],
          "correct": 1,
          "explanation": "Incremental updates are efficient: on each event, increment total_calls, update running average, etc. Full recompute is a fallback for reconciliation, not the primary update path."
        }
      ]
    },
    {
      "id": "denorm-033",
      "type": "multi-select",
      "question": "Which are examples of precomputed/denormalized columns?",
      "options": ["orders.total_amount (sum of line items)", "users.full_name (first + last name concatenated)", "products.review_count (count of reviews)", "posts.created_at (timestamp of creation)"],
      "correctIndices": [0, 1, 2],
      "explanation": "total_amount, full_name, and review_count are all derivable from other data — they're redundant for convenience. created_at is an original value, not derived from other data."
    },
    {
      "id": "denorm-034",
      "type": "multiple-choice",
      "question": "An Order has line items: [{product: A, qty: 2, price: $10}, {product: B, qty: 1, price: $25}]. The normalized approach computes total on read via SUM(qty * price). The denormalized approach stores order.total = $45. When is the denormalized total dangerous?",
      "options": ["When orders are read frequently", "When line items can be modified after order creation without recalculating the total", "When the total exceeds $1000", "When using floating-point arithmetic"],
      "correct": 1,
      "explanation": "If line items change (quantity updated, item removed) but the denormalized total isn't recalculated, you have an inconsistency — the total doesn't match reality. This is a classic denormalization bug."
    },
    {
      "id": "denorm-035",
      "type": "multiple-choice",
      "question": "What is 'table splitting' (vertical partitioning) as a denormalization technique?",
      "options": ["Splitting a table across multiple databases", "Splitting a wide table into two: one with frequently-accessed columns and one with rarely-accessed columns", "Splitting rows by date range", "Splitting a table into separate files"],
      "correct": 1,
      "explanation": "Vertical partitioning separates hot columns (read often) from cold columns (read rarely). Example: User(id, name, email) vs UserProfile(user_id, bio, avatar_url). Reduces row size for hot-path queries."
    },
    {
      "id": "denorm-036",
      "type": "ordering",
      "question": "Rank these denormalization techniques from MOST COMMON to LEAST COMMON in typical web applications:",
      "items": ["Counter caches (e.g., posts_count)", "Precomputed columns (e.g., full_name)", "Materialized views", "Star schema with fact/dimension tables"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Counter caches and precomputed columns are everyday web app patterns. Materialized views are common in reporting. Star schemas are typically reserved for dedicated data warehouses, not application databases."
    },
    {
      "id": "denorm-037",
      "type": "multiple-choice",
      "question": "A normalized Users table has first_name and last_name. The app always displays them together. Is adding a full_name column worthwhile?",
      "options": ["Yes — avoids string concatenation on every read", "No — concatenation is trivial; the consistency overhead of maintaining full_name outweighs the negligible compute savings", "Only for databases that don't support computed columns", "Only if names are longer than 50 characters"],
      "correct": 1,
      "explanation": "String concatenation is negligible CPU work. Adding full_name creates a consistency risk (what if first_name changes but full_name doesn't?) for almost zero performance gain. This is over-denormalization."
    },
    {
      "id": "denorm-038",
      "type": "multi-select",
      "question": "Which are signs you've OVER-denormalized?",
      "options": ["Frequent data inconsistencies between denormalized copies", "Simple updates require modifying 5+ tables", "Read performance improved by less than 10%", "Write throughput decreased by 5-10%"],
      "correctIndices": [0, 1, 2],
      "explanation": "Frequent inconsistencies mean maintenance isn't keeping up. Multi-table updates indicate too much duplication. Marginal read improvement means the join wasn't the bottleneck. A 5-10% write throughput decrease is a normal, expected tradeoff of denormalization — not a sign of over-denormalization."
    },
    {
      "id": "denorm-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team denormalized user_email onto Orders, Comments, and Notifications tables for display purposes. A user requests an email change (GDPR compliance). What's the problem?",
          "options": ["No problem — just update the Users table", "The old email persists in 3 other tables, creating a privacy violation and inconsistency", "Email changes aren't allowed under GDPR", "The database will reject the change"],
          "correct": 1,
          "explanation": "The old email lives in Orders, Comments, and Notifications. Updating just Users leaves stale personal data scattered across the database — a GDPR/privacy issue and a consistency bug."
        },
        {
          "question": "What design would have avoided this?",
          "options": ["Don't denormalize personal data that users can request changes/deletion of", "Use a single Users table reference (user_id) and join when displaying email", "Store emails in a separate encrypted table", "Use anonymized email hashes everywhere"],
          "correct": 0,
          "explanation": "Don't duplicate PII (personally identifiable information) across tables. Keep it in one authoritative location. Join when needed. This is a case where the consistency requirement outweighs the read performance benefit."
        }
      ]
    },
    {
      "id": "denorm-040",
      "type": "multiple-choice",
      "question": "What is a 'computed column' (also called 'generated column' or 'virtual column')?",
      "options": ["A column maintained by application code", "A column whose value is automatically derived from other columns by the database engine", "A column that stores computed hash values", "A column that auto-increments"],
      "correct": 1,
      "explanation": "Computed columns (supported by PostgreSQL, MySQL, SQL Server) let the database auto-calculate values like full_name = first_name || ' ' || last_name. Virtual ones compute on read; stored ones persist to disk."
    },
    {
      "id": "denorm-041",
      "type": "multi-select",
      "question": "Which benefits do stored computed columns have over manual denormalization?",
      "options": ["The database guarantees consistency — no stale data", "No application code needed to maintain the derived value", "They can be indexed", "They always outperform application-level caching"],
      "correctIndices": [0, 1, 2],
      "explanation": "Stored computed columns are automatically kept in sync by the database (no stale data), need no app-level maintenance code, and can be indexed. They don't always outperform app caching (Redis, etc.) — that depends on the workload."
    },
    {
      "id": "denorm-042",
      "type": "multiple-choice",
      "question": "What is 'eventual consistency' in the context of denormalized data?",
      "options": ["Data that is always consistent after every write", "A state where denormalized copies may temporarily diverge from the source but will converge over time", "A database that eventually deletes old data", "Consistency that requires manual intervention"],
      "correct": 1,
      "explanation": "When using async updates for denormalized data, there's a window where the source and copies disagree. The system is 'eventually consistent' — the copies will catch up, but not instantaneously."
    },
    {
      "id": "denorm-043",
      "type": "ordering",
      "question": "Rank these consistency guarantees from STRONGEST to WEAKEST when maintaining denormalized data:",
      "items": ["Synchronous update in the same database transaction", "Synchronous update across services via distributed transaction", "Async update via message queue (at-least-once delivery)", "Periodic batch reconciliation (hourly)"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Same-transaction: immediate, ACID-guaranteed. Distributed transaction: strong but fragile. Async queue: eventual but reliable. Batch reconciliation: weakest — stale for up to an hour between runs."
    },
    {
      "id": "denorm-044",
      "type": "multiple-choice",
      "question": "When is synchronous denormalized update (same transaction) appropriate?",
      "options": ["Always — consistency is paramount", "When the denormalized data is in the same database AND the extra write latency is acceptable", "Never — always use async", "Only for financial data"],
      "correct": 1,
      "explanation": "Same-transaction updates work when both tables are in the same database (no distributed coordination needed) and the added write latency is acceptable. For cross-service or high-throughput writes, async is usually better."
    },
    {
      "id": "denorm-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "A microservices system has a User Service and an Order Service, each with their own database. The Order Service needs to display customer_name on order receipts. What's the typical pattern?",
          "options": ["Synchronous API call to User Service on each order display", "Store a denormalized customer_name in the Order Service's database", "Share the User Service's database directly", "Don't show customer names on orders"],
          "correct": 1,
          "explanation": "In microservices, each service owns its data. The Order Service stores a denormalized copy of customer_name to avoid runtime dependencies on the User Service (which could be down or slow)."
        },
        {
          "question": "How does the Order Service learn about name changes in the User Service?",
          "options": ["It doesn't — names are frozen at order time", "User Service publishes a UserNameChanged event; Order Service subscribes and updates its copy", "Order Service polls User Service every minute", "Shared database view"],
          "correct": 1,
          "explanation": "Event-driven architecture: User Service publishes domain events. Order Service subscribes to UserNameChanged and updates its local copy. This is the standard pattern for cross-service data synchronization."
        }
      ]
    },
    {
      "id": "denorm-046",
      "type": "multi-select",
      "question": "In which scenarios is denormalization almost ALWAYS the right choice?",
      "options": ["Read-only analytics/reporting systems", "OLTP systems with equal read and write loads", "Caching layers (Redis, memcached)", "Displaying aggregated counts on high-traffic pages"],
      "correctIndices": [0, 2, 3],
      "explanation": "Analytics systems are read-only — no write amplification concern. Caches are inherently denormalized copies. High-traffic count displays (likes, views) need precomputation. Equal read/write OLTP may not benefit enough to justify the complexity."
    },
    {
      "id": "denorm-047",
      "type": "multiple-choice",
      "question": "Why do data warehouses like Snowflake, BigQuery, and Redshift favor denormalized schemas?",
      "options": ["They can't perform joins", "Their columnar storage and query engines are optimized for scanning wide, denormalized tables rather than many joins", "Normalization is incompatible with SQL", "Legal requirements mandate denormalization"],
      "correct": 1,
      "explanation": "Columnar data warehouses scan columns efficiently. Wide, flat tables minimize joins (which are expensive at warehouse scale). Their architecture is built for bulk analytical reads of denormalized data."
    },
    {
      "id": "denorm-048",
      "type": "numeric-input",
      "question": "A summary table has one row per day per product (365 days × 10,000 products). How many rows does it contain?",
      "answer": 3650000,
      "unit": "rows",
      "tolerance": "exact",
      "explanation": "365 × 10,000 = 3,650,000 rows. This is small — easily handled by any database. Compare this to scanning the raw events table which might have billions of rows."
    },
    {
      "id": "denorm-049",
      "type": "multiple-choice",
      "question": "What pattern does CQRS (Command Query Responsibility Segregation) formalize?",
      "options": ["Using different programming languages for reads and writes", "Maintaining separate optimized models for reads (queries) and writes (commands)", "Splitting the database across multiple regions", "Logging all queries for auditing"],
      "correct": 1,
      "explanation": "CQRS separates the write model (normalized, optimized for consistency) from the read model (denormalized, optimized for queries). This is formalized denormalization — the read model is a denormalized projection of the write model."
    },
    {
      "id": "denorm-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team implements CQRS: writes go to a normalized PostgreSQL database, reads come from a denormalized Elasticsearch index. What keeps them in sync?",
          "options": ["Elasticsearch reads from PostgreSQL directly", "An event/change stream propagates writes from PostgreSQL to Elasticsearch", "They share the same storage", "Manual sync by developers"],
          "correct": 1,
          "explanation": "A change stream (CDC via Debezium, or application-level events) captures writes from PostgreSQL and updates the Elasticsearch read model. This is the standard CQRS sync pattern."
        },
        {
          "question": "What consistency guarantee does this CQRS setup provide?",
          "options": ["Strong consistency — reads always reflect the latest write", "Eventual consistency — reads may lag behind writes briefly", "No consistency guarantee", "Read-your-writes consistency only"],
          "correct": 1,
          "explanation": "There's propagation delay between the write to PostgreSQL and the update to Elasticsearch. During this window, reads may return stale data. The system is eventually consistent."
        }
      ]
    },
    {
      "id": "denorm-051",
      "type": "ordering",
      "question": "When deciding whether to denormalize, rank these steps in the correct order:",
      "items": ["Measure the actual query performance problem", "Try indexing and query optimization first", "Denormalize if indexes aren't sufficient", "Add consistency maintenance logic for the denormalized data"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Always measure first — don't denormalize based on gut feeling. Try indexes (cheaper solution). Only denormalize if proven necessary. Then implement the consistency logic to keep denormalized data accurate."
    },
    {
      "id": "denorm-052",
      "type": "multiple-choice",
      "question": "A team wants to denormalize because 'joins are slow.' Their largest table has 50,000 rows and the join query takes 15ms. What should you advise?",
      "options": ["Denormalize — joins are always bad", "Don't denormalize — 50K rows with proper indexes should join in milliseconds; 15ms is fine for most use cases", "Switch to a NoSQL database", "Add more RAM"],
      "correct": 1,
      "explanation": "50K rows is tiny. A 15ms join is fast for most applications. Denormalizing here adds complexity for negligible gain. Save denormalization for when you've exhausted indexing and the performance problem is measured and real."
    },
    {
      "id": "denorm-053",
      "type": "multi-select",
      "question": "Which types of data are POOR candidates for denormalization?",
      "options": ["Frequently changing data (e.g., real-time stock prices)", "Data subject to regulatory compliance (PII, financial records)", "Data with complex update semantics (cascading changes)", "Relatively static reference data (country codes, currencies)"],
      "correctIndices": [0, 1, 2],
      "explanation": "Frequently changing data has high write amplification cost. Compliance-sensitive data shouldn't be scattered across tables. Complex update chains are error-prone. Static reference data is actually a great denormalization candidate — it rarely changes."
    },
    {
      "id": "denorm-054",
      "type": "multiple-choice",
      "question": "In a social media app, a user's follower_count is denormalized onto their profile. Two users follow them simultaneously. Without proper handling, what bug occurs?",
      "options": ["Both follows are rejected", "Lost update — both reads count=100, both write count=101, losing one increment", "The count becomes negative", "The database crashes"],
      "correct": 1,
      "explanation": "Classic race condition: both transactions read follower_count=100, both increment to 101, both write 101. One follow is lost. Fix: use atomic increment (SET count = count + 1) instead of read-modify-write."
    },
    {
      "id": "denorm-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Which SQL statement safely increments a denormalized counter without race conditions?",
          "options": ["SELECT count FROM users WHERE id = 1; UPDATE users SET follower_count = {count + 1}", "UPDATE users SET follower_count = follower_count + 1 WHERE id = 1", "INSERT INTO users (follower_count) VALUES (follower_count + 1)", "LOCK TABLE users; UPDATE users SET follower_count = follower_count + 1"],
          "correct": 1,
          "explanation": "SET follower_count = follower_count + 1 is atomic — the database handles the read-increment-write in a single operation. No application-level race condition possible."
        },
        {
          "question": "For extremely high-contention counters (millions of increments/sec, e.g., a viral video's view count), what additional technique helps?",
          "options": ["Use a bigger server", "Buffer increments in memory and flush periodically in batches", "Lock the row for each increment", "Use a 64-bit integer instead of 32-bit"],
          "correct": 1,
          "explanation": "At extreme scale, even atomic increments create row-level lock contention. Buffer increments in application memory (or Redis) and flush them to the database in batches (e.g., every 5 seconds). Accept slight display inaccuracy."
        }
      ]
    },
    {
      "id": "denorm-056",
      "type": "multiple-choice",
      "question": "What is a 'lookup table' pattern in denormalization?",
      "options": ["A table used for DNS lookups", "A small, precomputed table mapping keys to denormalized data for O(1) retrieval", "A table that stores function pointers", "A temporary table created during query execution"],
      "correct": 1,
      "explanation": "Lookup tables store precomputed mappings — e.g., a user_id → {name, email, avatar_url} table for fast retrieval without joining multiple tables. Common in caching and materialized view patterns."
    },
    {
      "id": "denorm-057",
      "type": "ordering",
      "question": "Rank these approaches from FASTEST READ to SLOWEST READ for displaying a product page (name, price, category, avg rating, review count):",
      "items": ["Single denormalized Products table with all fields", "Products + Categories join (rating/count precomputed on Products)", "Products + Categories + Reviews (compute AVG/COUNT at query time)", "Products + Categories + Reviews + separate Ratings aggregation subquery"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Single table: one index lookup. Two-table join: still fast. Three-table with aggregation: moderate. Four-table with subquery: slowest. Each additional join/aggregation adds latency."
    },
    {
      "id": "denorm-058",
      "type": "multiple-choice",
      "question": "A chat application shows conversation lists with the last message preview and unread count. What denormalization is standard?",
      "options": ["Store all messages in a single flat table", "Maintain last_message_text, last_message_at, and unread_count on the Conversations table", "Cache the entire conversation in Redis", "Don't show previews"],
      "correct": 1,
      "explanation": "Store last_message_text, last_message_at, and unread_count directly on Conversations. This makes the conversation list query trivial — one table scan with no joins or aggregations."
    },
    {
      "id": "denorm-059",
      "type": "multi-select",
      "question": "Which fields on a Conversations table are denormalized?",
      "options": ["conversation_id (primary key)", "last_message_text (copy of latest message)", "unread_count (precomputed count)", "created_at (when conversation started)"],
      "correctIndices": [1, 2],
      "explanation": "last_message_text is duplicated from the Messages table. unread_count is a precomputed aggregate. conversation_id and created_at are original data, not derived from elsewhere."
    },
    {
      "id": "denorm-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "A leaderboard shows the top 100 users by score. Computing this requires scanning all users and sorting — too slow for real-time display. What denormalization helps?",
          "options": ["Maintain a separate Leaderboard table with the top N users, updated on score changes", "Cache the query result for 24 hours", "Use a sorted set in Redis", "Limit users to improve sort performance"],
          "correct": 0,
          "explanation": "A Leaderboard table (or sorted set in Redis) stores precomputed rankings. Update it when scores change — only needs to check if the new score enters the top N."
        },
        {
          "question": "A user's score increases. How do you efficiently update the leaderboard?",
          "options": ["Recompute the entire leaderboard", "Compare the new score against the lowest score in the top 100; if higher, insert and evict the lowest", "Ignore it until the next batch refresh", "Lock the leaderboard during updates"],
          "correct": 1,
          "explanation": "Compare with the minimum score in the top 100. If the new score is higher, insert the user and remove the previous #100. This is O(1) per update instead of O(N log N) for full recomputation."
        }
      ]
    },
    {
      "id": "denorm-061",
      "type": "multiple-choice",
      "question": "What is 'data duplication budget' — a concept for deciding how much denormalization is acceptable?",
      "options": ["The storage cost of duplicated data", "A deliberate limit on how many copies of each piece of data exist, balancing read performance against consistency maintenance burden", "The maximum number of tables allowed", "A financial budget for database licenses"],
      "correct": 1,
      "explanation": "It's a design heuristic: for each piece of data, decide how many copies you're willing to maintain. More copies = faster reads but harder consistency. Set a limit to avoid uncontrollable duplication."
    },
    {
      "id": "denorm-062",
      "type": "numeric-input",
      "question": "A denormalized column (product_name) exists in 3 tables: Products, OrderItems, and CartItems. When product_name changes, how many UPDATE statements are needed (minimum)?",
      "answer": 3,
      "unit": "statements",
      "tolerance": "exact",
      "explanation": "One UPDATE per table: Products, OrderItems (all rows with that product), CartItems (all rows with that product). Three statements minimum, though each may affect multiple rows."
    },
    {
      "id": "denorm-063",
      "type": "multiple-choice",
      "question": "When denormalizing for a search index (e.g., Elasticsearch), what's the typical architecture?",
      "options": ["Replace the database with Elasticsearch", "Write to the normalized database first, then asynchronously update the search index with denormalized documents", "Write to Elasticsearch first, then sync to the database", "Use Elasticsearch as the primary database"],
      "correct": 1,
      "explanation": "The normalized database remains the source of truth. Changes are propagated to Elasticsearch (via CDC, events, or application-level sync) where data is stored in a denormalized, search-optimized format."
    },
    {
      "id": "denorm-064",
      "type": "ordering",
      "question": "Rank these from EASIEST to HARDEST to keep consistent with the source of truth:",
      "items": ["Computed column (database-managed)", "Same-transaction denormalized column", "Async-updated search index (Elasticsearch)", "Separate data warehouse (ETL pipeline)"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Computed columns: database guarantees consistency. Same-transaction: app manages it but it's ACID. Async index: eventually consistent, needs monitoring. Data warehouse: ETL pipelines are complex and can fail silently."
    },
    {
      "id": "denorm-065",
      "type": "multi-select",
      "question": "Which are valid uses of JSON columns as denormalization?",
      "options": ["Storing a snapshot of order data at time of purchase (frozen copy)", "Storing user preferences that vary per user", "Replacing all foreign keys with embedded JSON objects", "Caching API responses for display"],
      "correctIndices": [0, 1, 3],
      "explanation": "JSON is good for: frozen snapshots (order receipt data), flexible/varying structures (preferences), and cached display data. Replacing foreign keys with JSON loses referential integrity and query ability — that's not denormalization, it's schema destruction."
    },
    {
      "id": "denorm-066",
      "type": "two-stage",
      "stages": [
        {
          "question": "An e-commerce system stores shipping_address on Orders as a JSON snapshot: {street, city, state, zip}. The user later updates their address in the Users table. Should the Order's shipping_address be updated?",
          "options": ["Yes — keep it consistent with the user's current address", "No — the order was shipped to the OLD address; the snapshot is intentionally frozen"],
          "correct": 1,
          "explanation": "Order addresses are intentionally frozen at purchase time. The order was fulfilled using that address. Updating it would be historically incorrect. This is a case where 'stale' data is actually correct."
        },
        {
          "question": "What design pattern is this an example of?",
          "options": ["Event sourcing", "Point-in-time snapshot — capturing the state of data at a specific moment for historical accuracy", "Cache invalidation", "Soft delete"],
          "correct": 1,
          "explanation": "Point-in-time snapshots intentionally freeze data as it was when an event occurred. Common in orders, invoices, contracts — anywhere the historical record matters more than current state."
        }
      ]
    },
    {
      "id": "denorm-067",
      "type": "multiple-choice",
      "question": "What is the difference between 'intentional denormalization' and 'accidental denormalization'?",
      "options": ["There is no difference", "Intentional: deliberate, documented, with consistency maintenance. Accidental: organic duplication with no maintenance strategy, leading to data drift.", "Intentional uses triggers, accidental uses application code", "Intentional is in production, accidental is in development"],
      "correct": 1,
      "explanation": "Intentional denormalization is a conscious design decision with documented consistency guarantees. Accidental denormalization happens when data gets duplicated without a plan — leading to silent inconsistencies and data quality issues."
    },
    {
      "id": "denorm-068",
      "type": "numeric-input",
      "question": "A Products table has 1M rows. A nightly materialized view refresh reads the entire table. If each row is 500 bytes, approximately how much data is scanned per refresh?",
      "answer": 500,
      "unit": "MB",
      "tolerance": 0.1,
      "explanation": "1,000,000 rows × 500 bytes = 500,000,000 bytes = ~500 MB per refresh. Not huge, but it grows with the table. Incremental refresh (only changed rows) would be more efficient."
    },
    {
      "id": "denorm-069",
      "type": "multiple-choice",
      "question": "What is an 'incremental materialized view refresh' compared to a 'full refresh'?",
      "options": ["Refresh that runs faster by using more CPU", "Only recomputing the portions of the view affected by changes since the last refresh, rather than rebuilding the entire view", "Refreshing one column at a time", "Refreshing in smaller batches over several hours"],
      "correct": 1,
      "explanation": "Incremental refresh tracks changes (inserts, updates, deletes) since the last refresh and only recomputes affected portions. Full refresh rebuilds from scratch. Incremental is faster but more complex to implement."
    },
    {
      "id": "denorm-070",
      "type": "multi-select",
      "question": "Which databases natively support materialized views?",
      "options": ["PostgreSQL", "MySQL (as of 8.0)", "Oracle", "MongoDB"],
      "correctIndices": [0, 2],
      "explanation": "PostgreSQL and Oracle have native materialized view support (CREATE MATERIALIZED VIEW). MySQL 8.0 does not have native materialized views — you simulate them with tables + triggers/events. MongoDB has $merge for a similar pattern but not traditional materialized views."
    },
    {
      "id": "denorm-071",
      "type": "ordering",
      "question": "Rank these denormalization approaches from MOST to LEAST database-managed (least application code needed):",
      "items": ["Stored generated/computed columns", "Materialized views with auto-refresh", "Database triggers maintaining denormalized tables", "Application-code maintaining denormalized copies"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Generated columns: fully database-managed, zero app code. Materialized views: database handles refresh (you set the schedule). Triggers: database-managed but you write the trigger logic. Application code: fully app-managed, most flexible but most work."
    },
    {
      "id": "denorm-072",
      "type": "multiple-choice",
      "question": "A reporting query aggregates data across 12 months: SELECT month, SUM(revenue) FROM orders WHERE year = 2024 GROUP BY month. It scans 50M rows and takes 30 seconds. What's the most effective denormalization?",
      "options": ["Add more indexes", "Create a monthly_revenue summary table with one row per month, updated incrementally", "Partition the table by month", "Use a faster server"],
      "correct": 1,
      "explanation": "A monthly_revenue summary table turns a 50M-row scan into a 12-row lookup. Update it incrementally as orders come in. Partitioning helps but still requires aggregation; a summary table eliminates it."
    },
    {
      "id": "denorm-073",
      "type": "two-stage",
      "stages": [
        {
          "question": "A news site has Articles and Tags (M:N via ArticleTags junction). The homepage shows 'Trending tags' — the top 10 tags by article count this week. Computing this requires joining 3 tables and counting. What denormalization is appropriate?",
          "options": ["Maintain a trending_tags summary table updated hourly", "Embed all tag names in each article", "Remove the Tags feature", "Use full-text search instead of tags"],
          "correct": 0,
          "explanation": "A trending_tags table with (tag_id, tag_name, article_count, period) precomputes the result. Update it hourly (or on each article publish). The homepage reads 10 rows from one table."
        },
        {
          "question": "Is hourly refresh acceptable for 'trending tags'?",
          "options": ["No — must be real-time", "Yes — trending content doesn't need second-level precision; hourly is fine for this use case", "Only if you use Redis", "Depends on the database"],
          "correct": 1,
          "explanation": "Trending tags are inherently approximate — a 1-hour lag is imperceptible to users. This is a case where eventual consistency perfectly fits the use case. Real-time would add complexity for no user-visible benefit."
        }
      ]
    },
    {
      "id": "denorm-074",
      "type": "multiple-choice",
      "question": "In a microservices architecture, why is denormalization (local data copies) often necessary rather than optional?",
      "options": ["Microservices can't do joins", "Each service owns its database; cross-service joins are impossible, so services must store local copies of data they need from other services", "Denormalization is required by the microservices specification", "It improves code readability"],
      "correct": 1,
      "explanation": "Microservices enforce database-per-service. You can't JOIN across service boundaries. If the Order Service needs customer names, it must store a local copy — denormalization is architecturally mandatory, not optional."
    },
    {
      "id": "denorm-075",
      "type": "multi-select",
      "question": "Which are common patterns for cross-service data synchronization in microservices?",
      "options": ["Event-driven updates (publish/subscribe)", "API calls at query time (no local copy)", "Change Data Capture (CDC) streams", "Shared database (anti-pattern but used)"],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are used in practice. Events: standard async sync. API calls: simpler but creates runtime coupling. CDC: captures database changes as a stream. Shared database: violates microservice principles but pragmatically exists."
    },
    {
      "id": "denorm-076",
      "type": "ordering",
      "question": "Rank these cross-service data strategies from MOST COUPLED to LEAST COUPLED:",
      "items": ["Shared database", "Synchronous API call on every read", "Async event-driven local copy", "Periodic batch sync"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Shared database: maximum coupling (shared schema). Sync API call: runtime dependency (service must be up). Async events: decoupled, handles service downtime. Batch sync: most decoupled but most stale."
    },
    {
      "id": "denorm-077",
      "type": "multiple-choice",
      "question": "What does 'fan-out on write' mean?",
      "options": ["Writing data to a fan-shaped partition scheme", "Precomputing and distributing data to recipients at write time, so reads are fast and simple", "Spreading writes across multiple servers", "Writing to multiple file formats"],
      "correct": 1,
      "explanation": "Fan-out on write: when a user posts, immediately push the post to all followers' feeds (precomputed). Reads are trivial — just fetch the precomputed feed. Twitter's original fanout model. Opposite of fan-out on read (compute feed at read time)."
    },
    {
      "id": "denorm-078",
      "type": "two-stage",
      "stages": [
        {
          "question": "Twitter-style feed: User A has 10,000 followers and posts a tweet. In a 'fan-out on write' system, what happens?",
          "options": ["The tweet is written once; followers query for it", "The tweet is copied into 10,000 individual feed timelines at write time", "The tweet is cached in CDN", "Nothing until followers refresh"],
          "correct": 1,
          "explanation": "Fan-out on write: the tweet is pushed into each follower's precomputed feed (10,000 inserts). This makes reading feeds fast (just read your precomputed timeline) but writes are expensive."
        },
        {
          "question": "A celebrity has 50 million followers. Why is pure fan-out on write problematic for them?",
          "options": ["Their tweets take up too much storage", "Writing to 50M timelines per tweet creates massive write amplification and latency", "Their followers can't read that fast", "The tweet content is too long"],
          "correct": 1,
          "explanation": "50M timeline inserts per tweet is extreme write amplification. The solution: hybrid approach — fan-out on write for normal users, fan-out on read (merge at query time) for celebrities."
        }
      ]
    },
    {
      "id": "denorm-079",
      "type": "multiple-choice",
      "question": "What is 'fan-out on read'?",
      "options": ["Reading from multiple database replicas", "Computing the result at read time by querying source data — no precomputation, always fresh but potentially slow", "Caching read results in a CDN", "Reading data in parallel threads"],
      "correct": 1,
      "explanation": "Fan-out on read: compute results at query time. For a feed: query all followed users' posts and merge. Always fresh, but expensive — requires joins/unions across many users' posts on every feed load."
    },
    {
      "id": "denorm-080",
      "type": "ordering",
      "question": "Rank these from MOST WRITE WORK to MOST READ WORK:",
      "items": ["Fan-out on write (fully precomputed feeds)", "Hybrid (precompute for most, compute on read for celebrities)", "Fan-out on read (compute feed at query time)", "No feed — just show latest posts globally"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Fan-out on write: max write work (push to all followers). Hybrid: moderate write, moderate read. Fan-out on read: no write work, max read work. No feed: minimal work on both sides."
    },
    {
      "id": "denorm-081",
      "type": "multi-select",
      "question": "Which are examples of denormalization in NoSQL/document databases?",
      "options": ["Embedding subdocuments instead of referencing", "Storing computed fields (e.g., total_price) in the document", "Using a secondary index", "Duplicating a user's name in every document they authored"],
      "correctIndices": [0, 1, 3],
      "explanation": "Embedding, computed fields, and duplicated attributes are all denormalization — trading write complexity for read performance. Secondary indexes are an access optimization, not data duplication."
    },
    {
      "id": "denorm-082",
      "type": "multiple-choice",
      "question": "In DynamoDB, why is denormalization the STANDARD approach rather than an optimization?",
      "options": ["DynamoDB doesn't support normalization", "DynamoDB has no JOIN operation — queries access a single table, so data must be structured for single-table access patterns", "DynamoDB is a key-value store only", "Amazon requires it"],
      "correct": 1,
      "explanation": "DynamoDB (and most key-value/wide-column stores) cannot JOIN. Every query accesses one table (or index). Data must be modeled for your access patterns, which inherently means denormalization — storing related data together."
    },
    {
      "id": "denorm-083",
      "type": "two-stage",
      "stages": [
        {
          "question": "In DynamoDB single-table design, an e-commerce app stores Users, Orders, and OrderItems all in one table using composite keys. What's the partition key / sort key pattern for accessing 'all orders for user 123'?",
          "options": ["PK=USER#123, SK begins_with ORDER#", "PK=ORDER#*, SK=USER#123", "Scan the entire table with a filter", "Use a separate Orders table"],
          "correct": 0,
          "explanation": "PK=USER#123 groups all of user 123's data. SK begins_with ORDER# retrieves just their orders. This single-table pattern is inherently denormalized — user, order, and item data coexist in one table."
        },
        {
          "question": "What's the main drawback of DynamoDB single-table design?",
          "options": ["Poor read performance", "High storage cost", "Extreme complexity — the data model is tightly coupled to access patterns and hard to evolve", "It only works for small datasets"],
          "correct": 2,
          "explanation": "Single-table design is powerful but brittle. Adding a new access pattern may require restructuring the entire table. The schema is driven by queries, not data relationships — changing requirements can mean expensive migrations."
        }
      ]
    },
    {
      "id": "denorm-084",
      "type": "numeric-input",
      "question": "A Redis sorted set maintains a leaderboard. Adding a score is O(log N). With 1 million users in the set, approximately how many comparison operations does an insert require?",
      "answer": 20,
      "unit": "operations",
      "tolerance": 0.1,
      "explanation": "log2(1,000,000) ≈ 20. Redis sorted sets use skip lists, which provide O(log N) insert/lookup. Even at 1M entries, updates are ~20 comparisons — microseconds."
    },
    {
      "id": "denorm-085",
      "type": "multiple-choice",
      "question": "What is a 'write-behind cache' pattern?",
      "options": ["A cache that's always behind the database", "Writes go to the cache first, then asynchronously flush to the database — the cache is the write buffer", "A cache that stores previous values", "Writing to backup storage"],
      "correct": 1,
      "explanation": "Write-behind (write-back): the application writes to cache, and the cache asynchronously persists to the database. This absorbs write spikes but risks data loss if the cache crashes before flushing."
    },
    {
      "id": "denorm-086",
      "type": "multi-select",
      "question": "Which are risks of write-behind caching?",
      "options": ["Data loss if cache crashes before flushing to database", "Database and cache can be temporarily inconsistent", "Reads may see data that isn't yet durable", "The cache can never be invalidated"],
      "correctIndices": [0, 1, 2],
      "explanation": "Write-behind risks: data loss (unflushed writes lost on crash), inconsistency (cache is ahead of database), durability concerns (reads see not-yet-persisted data). The cache can absolutely be invalidated — that's a separate concern."
    },
    {
      "id": "denorm-087",
      "type": "ordering",
      "question": "Rank these caching strategies from SIMPLEST to MOST COMPLEX:",
      "items": ["Cache-aside (lazy loading)", "Read-through cache", "Write-through cache", "Write-behind (write-back) cache"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Cache-aside: app manages cache manually (check cache, miss → query DB, populate cache). Read-through: cache handles misses automatically. Write-through: writes go to cache + DB synchronously. Write-behind: writes buffer in cache and async-flush — most complex."
    },
    {
      "id": "denorm-088",
      "type": "multiple-choice",
      "question": "A system uses cache-aside with Redis. The flow is: read from Redis → cache miss → read from PostgreSQL → write to Redis. What's the cache invalidation strategy when data is updated?",
      "options": ["Delete the Redis key on write, so the next read repopulates it", "Update Redis synchronously with every database write", "Let the cache expire via TTL only", "Never invalidate — cache is permanent"],
      "correct": 0,
      "explanation": "Delete-on-write is the standard cache-aside invalidation: delete the stale cache entry when the underlying data changes. The next read will miss, fetch fresh data from the database, and repopulate the cache."
    },
    {
      "id": "denorm-089",
      "type": "two-stage",
      "stages": [
        {
          "question": "An application has a race condition: Thread A deletes the cache key after a database update. Thread B (reading) misses the cache, reads the OLD data from a lagging replica, and repopulates the cache with stale data. What pattern fixes this?",
          "options": ["Use a short TTL as a safety net so stale data eventually expires", "Double-delete: delete cache, wait briefly, delete again to catch stale repopulations", "Lock the cache during writes", "Use a stronger database replica"],
          "correct": 0,
          "explanation": "A short TTL (e.g., 30 seconds) ensures stale cache entries expire quickly even if repopulated with old data. This is the simplest mitigation for replica lag + cache repopulation races."
        },
        {
          "question": "What's the fundamental cause of this race condition?",
          "options": ["Redis is too slow", "The database replica hasn't received the latest write when the cache-miss read hits it", "The TTL is too long", "Thread B has a bug"],
          "correct": 1,
          "explanation": "Replication lag: the write goes to the primary, the cache is deleted, but Thread B reads from a replica that hasn't received the write yet. The replica returns stale data, which gets cached. This is inherent in async replication + caching."
        }
      ]
    },
    {
      "id": "denorm-090",
      "type": "numeric-input",
      "question": "A cache has a 95% hit rate. The system receives 10,000 requests/sec. How many requests per second hit the database (cache misses)?",
      "answer": 500,
      "unit": "req/sec",
      "tolerance": "exact",
      "explanation": "10,000 × (1 - 0.95) = 10,000 × 0.05 = 500 database hits per second. A 95% hit rate reduces database load by 20x."
    },
    {
      "id": "denorm-091",
      "type": "multiple-choice",
      "question": "What is 'cache warming' (or 'cache priming')?",
      "options": ["Heating the cache server hardware", "Pre-loading frequently accessed data into the cache before it's needed, to avoid cold-start cache misses", "Gradually increasing cache TTL", "Testing the cache in a staging environment"],
      "correct": 1,
      "explanation": "Cache warming preloads the cache with expected hot data — at deploy time, after a cache flush, or during startup. Without it, the first requests after a cold start all hit the database (thundering herd)."
    },
    {
      "id": "denorm-092",
      "type": "multi-select",
      "question": "Which situations require cache warming?",
      "options": ["After deploying a new version that flushes the cache", "After a Redis restart/failover", "During normal steady-state operation with high hit rates", "Before a known traffic spike (product launch, sale event)"],
      "correctIndices": [0, 1, 3],
      "explanation": "Warm the cache after events that empty it (deployments, restarts) and before expected traffic spikes. During normal steady state with high hit rates, the cache is already warm — no action needed."
    },
    {
      "id": "denorm-093",
      "type": "two-stage",
      "stages": [
        {
          "question": "A product catalog has 10M products. Only 50K are viewed daily. Should you denormalize all 10M products into the cache?",
          "options": ["Yes — cache everything for maximum performance", "No — only cache the hot 50K; caching 10M wastes memory for 99.5% of rarely-accessed data", "Cache 10M but with short TTL", "Don't use a cache at all"],
          "correct": 1,
          "explanation": "Follow the Pareto principle: a small subset drives most traffic. Cache the hot set (~50K products). The remaining 9.95M are accessed rarely and can tolerate a database hit."
        },
        {
          "question": "How do you ensure the hot 50K stay cached while cold products don't consume cache memory?",
          "options": ["Manually manage which products are cached", "Use an LRU (Least Recently Used) eviction policy — hot products are accessed often and stay; cold products get evicted", "Set TTL to exactly 24 hours", "Use a larger cache"],
          "correct": 1,
          "explanation": "LRU eviction naturally keeps hot data in cache. Frequently accessed products are always 'recently used' and won't be evicted. Rarely accessed products fall off. No manual management needed."
        }
      ]
    },
    {
      "id": "denorm-094",
      "type": "numeric-input",
      "question": "A cache hit takes 1ms. A database query takes 50ms. With a 90% cache hit rate, what is the average response time per request?",
      "answer": 5.9,
      "unit": "ms",
      "tolerance": 0.1,
      "explanation": "Average = (0.90 × 1ms) + (0.10 × 50ms) = 0.9 + 5.0 = 5.9ms. Even with 10% misses, the average is much closer to the cache latency."
    },
    {
      "id": "denorm-095",
      "type": "ordering",
      "question": "Rank these scenarios from MOST APPROPRIATE to LEAST APPROPRIATE for denormalization:",
      "items": ["Read-heavy dashboard displaying aggregated metrics", "Financial transaction ledger requiring audit trail", "Product catalog displayed on high-traffic homepage", "User table used for login authentication"],
      "correctOrder": [0, 2, 3, 1],
      "explanation": "Dashboard aggregations: classic denormalization case. Product catalog: high read traffic justifies it. User authentication: moderate case (login is a single-row lookup anyway). Financial ledger: accuracy and auditability are paramount — denormalization adds risk."
    },
    {
      "id": "denorm-096",
      "type": "multiple-choice",
      "question": "What is the 'single source of truth' principle in denormalized systems?",
      "options": ["Only one database server exists", "One authoritative version of each data item exists; all denormalized copies are derived from it and can be rebuilt", "Only one application can write data", "Data is stored in one format only"],
      "correct": 1,
      "explanation": "Even in denormalized systems, one version is canonical. Denormalized copies are derived projections. If they drift, the source of truth wins. This principle lets you rebuild denormalized data from scratch if needed."
    },
    {
      "id": "denorm-097",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team discovers that their denormalized review_count on Products is wrong for 5% of products — it doesn't match COUNT(*) from the Reviews table. What's the recovery strategy?",
          "options": ["Delete all reviews and start over", "Run a reconciliation query: UPDATE products SET review_count = (SELECT COUNT(*) FROM reviews WHERE product_id = products.id)", "Accept the inaccuracy", "Remove the denormalized column"],
          "correct": 1,
          "explanation": "Reconcile by recomputing from the source of truth (the Reviews table). This is why maintaining a single source of truth matters — you can always rebuild denormalized data from the canonical version."
        },
        {
          "question": "How do you prevent this drift from recurring?",
          "options": ["Run reconciliation more frequently", "Identify the root cause — likely a code path that modifies reviews without updating the counter — and fix it, plus add periodic reconciliation as a safety net", "Stop using the denormalized counter", "Use a different database"],
          "correct": 1,
          "explanation": "Fix the root cause (find the write path that skips the counter update). Add periodic reconciliation as a safety net (e.g., nightly batch that compares counts and fixes drift). Both are needed — fix + verify."
        }
      ]
    },
    {
      "id": "denorm-098",
      "type": "multi-select",
      "question": "Which are good practices when introducing denormalization?",
      "options": ["Document what is denormalized, where the copies live, and how consistency is maintained", "Measure the performance improvement to validate it was worthwhile", "Add monitoring/alerts for data drift between source and copies", "Denormalize everything at once for maximum performance"],
      "correctIndices": [0, 1, 2],
      "explanation": "Document your denormalization strategy. Measure the gains (was it worth the complexity?). Monitor for drift. Don't denormalize everything at once — be surgical, target specific bottlenecks."
    },
    {
      "id": "denorm-099",
      "type": "ordering",
      "question": "Rank the lifecycle of a denormalization decision from FIRST to LAST step:",
      "items": ["Identify a specific read performance bottleneck", "Verify indexes and query optimization are insufficient", "Design the denormalization and consistency strategy", "Implement, measure improvement, and add drift monitoring"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Don't denormalize speculatively. Start with a real bottleneck, exhaust simpler solutions, design carefully (including consistency maintenance), then implement with measurement and monitoring."
    },
    {
      "id": "denorm-100",
      "type": "two-stage",
      "stages": [
        {
          "question": "A startup is building their v1 product. A developer proposes denormalizing the schema 'for performance.' The app has 100 users. What should you do?",
          "options": ["Denormalize now to avoid problems later", "Start normalized — premature denormalization adds complexity you don't need yet; optimize when you have real performance data", "Use NoSQL to avoid the question entirely", "Build both and A/B test"],
          "correct": 1,
          "explanation": "Start normalized. At 100 users, joins are trivial. Denormalization adds write complexity and consistency risk for zero measurable benefit. Optimize when you have real performance data from real scale."
        },
        {
          "question": "At what point should this startup reconsider denormalization?",
          "options": ["Never — normalized schemas scale infinitely", "When specific queries are measured to be too slow AND indexes aren't sufficient", "When they hire a DBA", "When they reach 1 million users regardless of performance"],
          "correct": 1,
          "explanation": "Denormalize when you have evidence: measured slow queries that indexes can't fix. User count alone isn't the trigger — a well-indexed normalized schema can serve millions of users for many access patterns."
        }
      ]
    }
  ]
}
